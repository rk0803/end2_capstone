{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"document_encoder.ipynb","provenance":[{"file_id":"1oPgiCcOQL9E1zUO_3Ej_BhOMXKerhMOK","timestamp":1631440427794}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1oPgiCcOQL9E1zUO_3Ej_BhOMXKerhMOK","authorship_tag":"ABX9TyPCeWiH4RIp9if4dxt9k0wR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nLj5IAL3N9S2"},"source":["##Document Encoder\n","This notebook takes the documents(Z part) from our dataset and passes them through the encoder to create the embeddings for the documents.\n","The encoder used here is BertModel with BertTokenizer as the tokenizer.\n","\n","The Model is used as a pre-trained model and is not trained on the current data specifically.\n","This is done for two reasons:\n","1) We index the documents. If train the document encoder, everytime the index needs to be changed, which is compute intensive.\n","2) If we want to add more documents, we need to re-train the complete model, again another compute intensive task.\n","\n","## Challenges faced here\n","1)Inspite of having colab pro version and High-RAM availability, CUDA runs out of memory after some 60% of the training data, which is about 14000 samples.\n","So I divided the training data in two parts. Encoded the first part of training data. Saved it. Killed the kernel and restarted and encoded the second part of training data, saved it and reapeated to encode the test data.\n","2) About 50% of the documents were lengthier than the number of tokens bert model could actually take (which is 512 tokens)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yK04nqgmn4ZA","executionInfo":{"status":"ok","timestamp":1631421359953,"user_tz":-330,"elapsed":3443,"user":{"displayName":"Ritambhra Korpal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVTs8WqR_O80rsXO6tVh5uGAtlP3cdOy0OgQoq-g=s64","userId":"16099834797387286691"}},"outputId":"aec74f4a-42f5-4d94-b9bf-e26431b8f625"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"id":"BLcW0Mm7oUq-"},"source":["import pandas as pd\n","import numpy as np\n","\n","# torch imports\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#from transformers import BertTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hH8e8l9qeIo"},"source":["train_data=pd.read_csv(\"/content/drive/MyDrive/train_qa.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"vnP0QX9OqjTf","executionInfo":{"status":"ok","timestamp":1631421379617,"user_tz":-330,"elapsed":817,"user":{"displayName":"Ritambhra Korpal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVTs8WqR_O80rsXO6tVh5uGAtlP3cdOy0OgQoq-g=s64","userId":"16099834797387286691"}},"outputId":"e79cd4d5-128b-436e-eeca-2c72192c1cbf"},"source":["train_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>Z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How to use torch.atan, give an example?</td>\n","      <td>&gt;&gt;&gt; a = torch.randn(4)\\n&gt;&gt;&gt; a\\ntensor([ 0.2341...</td>\n","      <td>&gt;&gt;&gt; a = torch.randn(4)\\n&gt;&gt;&gt; a\\ntensor([ 0.2341...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How can a handle be used to remove the added h...</td>\n","      <td>callinghandle.remove()</td>\n","      <td>Hooks will be called in order of registration....</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>What tensor of sizewin_length can a window be?</td>\n","      <td>1-D</td>\n","      <td>windowcan be a 1-D tensor of sizewin_length, e...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What did aScriptModuleorScriptFunction previou...</td>\n","      <td>withtorch.jit.save</td>\n","      <td>Functionally equivalent to aScriptModule, but ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>What is used as an entry point into aScriptMod...</td>\n","      <td>annn.Module</td>\n","      <td>Warning The@torch.jit.ignoreannotation’s behav...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   X  ...                                                  Z\n","0            How to use torch.atan, give an example?  ...  >>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.2341...\n","1  How can a handle be used to remove the added h...  ...  Hooks will be called in order of registration....\n","2     What tensor of sizewin_length can a window be?  ...  windowcan be a 1-D tensor of sizewin_length, e...\n","3  What did aScriptModuleorScriptFunction previou...  ...  Functionally equivalent to aScriptModule, but ...\n","4  What is used as an entry point into aScriptMod...  ...  Warning The@torch.jit.ignoreannotation’s behav...\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"RFFdDU2b66oj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hB_1fRR09w02"},"source":["batch_size=8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"-6CqSUQcRut6","executionInfo":{"status":"ok","timestamp":1631421141403,"user_tz":-330,"elapsed":546,"user":{"displayName":"Ritambhra Korpal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVTs8WqR_O80rsXO6tVh5uGAtlP3cdOy0OgQoq-g=s64","userId":"16099834797387286691"}},"outputId":"8149a19f-0373-4fb1-de0c-f0d99f875483"},"source":["train_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>Z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10008</th>\n","      <td>How to use torch.atan, give an example?</td>\n","      <td>&gt;&gt;&gt; a = torch.randn(4)\\n&gt;&gt;&gt; a\\ntensor([ 0.2341...</td>\n","      <td>&gt;&gt;&gt; a = torch.randn(4)\\n&gt;&gt;&gt; a\\ntensor([ 0.2341...</td>\n","    </tr>\n","    <tr>\n","      <th>6408</th>\n","      <td>How can a handle be used to remove the added h...</td>\n","      <td>callinghandle.remove()</td>\n","      <td>Hooks will be called in order of registration....</td>\n","    </tr>\n","    <tr>\n","      <th>17395</th>\n","      <td>What tensor of sizewin_length can a window be?</td>\n","      <td>1-D</td>\n","      <td>windowcan be a 1-D tensor of sizewin_length, e...</td>\n","    </tr>\n","    <tr>\n","      <th>15488</th>\n","      <td>What did aScriptModuleorScriptFunction previou...</td>\n","      <td>withtorch.jit.save</td>\n","      <td>Functionally equivalent to aScriptModule, but ...</td>\n","    </tr>\n","    <tr>\n","      <th>11847</th>\n","      <td>What is used as an entry point into aScriptMod...</td>\n","      <td>annn.Module</td>\n","      <td>Warning The@torch.jit.ignoreannotation’s behav...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                       X  ...                                                  Z\n","10008            How to use torch.atan, give an example?  ...  >>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.2341...\n","6408   How can a handle be used to remove the added h...  ...  Hooks will be called in order of registration....\n","17395     What tensor of sizewin_length can a window be?  ...  windowcan be a 1-D tensor of sizewin_length, e...\n","15488  What did aScriptModuleorScriptFunction previou...  ...  Functionally equivalent to aScriptModule, but ...\n","11847  What is used as an entry point into aScriptMod...  ...  Warning The@torch.jit.ignoreannotation’s behav...\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"v23WYQN3GcpX"},"source":["train_data1=train_data[0:7200]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tX2RnBYhGkBq"},"source":["train_data2=train_data[7200:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDxPNhzRd3sz"},"source":["#doctrain_data=[t for t in train_data['Z']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_p2SV5poISN"},"source":["#BERT imports\n","from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Lh2BAp12JOR4","executionInfo":{"status":"ok","timestamp":1631421535034,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ritambhra Korpal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVTs8WqR_O80rsXO6tVh5uGAtlP3cdOy0OgQoq-g=s64","userId":"16099834797387286691"}},"outputId":"937db379-dff1-4f24-f8d8-347f9d2d3d1d"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla T4'"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8b2LZ_e-gD5","executionInfo":{"status":"ok","timestamp":1631420064248,"user_tz":-330,"elapsed":3447,"user":{"displayName":"Ritambhra Korpal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVTs8WqR_O80rsXO6tVh5uGAtlP3cdOy0OgQoq-g=s64","userId":"16099834797387286691"}},"outputId":"5021e95e-6d6f-412a-a78a-5f72b2f139c4"},"source":["#BertModel to encode context (Z)\n","bert_z=BertModel.from_pretrained(\"bert-base-uncased\")\n","for param in bert_z.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","metadata":{"id":"JqpZOMRf3Y2C"},"source":["## get document embeddings for training\n","Train embeddings were obtained in parts as CUDA runs out of memory\n"]},{"cell_type":"code","metadata":{"id":"MQIAiIJQsnDO"},"source":["tokenized_train1_z=tokenizer([t for t in train_data1['Z']],truncation=True,padding=True, return_tensors='pt')\n","tokenized_train2_z=tokenizer([t for t in train_data2['Z']],truncation=True,padding=True, return_tensors='pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxE_IZO212uW"},"source":["train1_dataset_z=TensorDataset(tokenized_train1_z['input_ids'],tokenized_train1_z['attention_mask'],tokenized_train1_z['token_type_ids'])\n","train2_dataset_z=TensorDataset(tokenized_train2_z['input_ids'],tokenized_train2_z['attention_mask'],tokenized_train2_z['token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_W42IAA12uX"},"source":["z1_sampler=SequentialSampler(train1_dataset_z)\n","z2_sampler=SequentialSampler(train2_dataset_z)\n","\n","z_train1_dataloader=DataLoader(train1_dataset_z, sampler=z1_sampler, batch_size=batch_size, shuffle=False)\n","z_train2_dataloader=DataLoader(train2_dataset_z, sampler=z2_sampler, batch_size=batch_size, shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZGrhAm-QO3Y"},"source":["len(train1_dataset_z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIlyi7YsKFc3"},"source":["from tqdm import tqdm, trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCbSuCYdJ6Kz"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8yCpZBXKFFk"},"source":["bert_z.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQAAn9aMJDQl"},"source":["num_train_epochs = 1\n","encoded_z=[]\n","z_train_iterator = trange(num_train_epochs,desc=\"epochs\")\n","for _ in z_train_iterator:\n","    epoch_iterator = tqdm(z_train1_dataloader, desc=\"Iteration\")\n","    for step, batch in enumerate(epoch_iterator):\n","      batch = tuple(t.to(device) for t in batch)\n","      inputs = {'input_ids':       batch[0],\n","                'attention_mask':  batch[1], \n","                'token_type_ids':  batch[2]}\n","      outputs = bert_z(**inputs)\n","      cls_tok=outputs[0][:,0,:]\n","      encoded_z.append(cls_tok)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZpHkZbNQ3Ed"},"source":["len(encoded_z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpaowqK2eRxz"},"source":["new_z=torch.stack(encoded_z)\n","new_z1=new_z.reshape(-1,768)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RjJ8zUHem_Q"},"source":["torch.save(new_z1,\"encoded1_doc.pt\")\n","from google.colab import files\n","files.download('encoded1_doc.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBD-EzTvMugQ"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaDNqAytH-Jx"},"source":["num_train_epochs = 1\n","encoded_z=[]\n","z_train_iterator = trange(num_train_epochs,desc=\"epochs\")\n","for _ in z_train_iterator:\n","    epoch_iterator = tqdm(z_train2_dataloader, desc=\"Iteration\")\n","    for step, batch in enumerate(epoch_iterator):\n","      batch = tuple(t.to(device) for t in batch)\n","      inputs = {'input_ids':       batch[0],\n","                'attention_mask':  batch[1], \n","                'token_type_ids':  batch[2]}\n","      outputs = bert_z(**inputs)\n","      cls_tok=outputs[0][:,0,:]\n","      encoded_z.append(cls_tok)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tznDtbGe0Bu"},"source":["new_z=torch.stack(encoded_z)\n","new_z1=new_z.reshape(-1,768)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Btnr7M-ue0Bv"},"source":["torch.save(new_z1,\"encoded2_doc.pt\")\n","from google.colab import files\n","files.download('encoded2_doc.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqfMOi9IfOau"},"source":["Now embeddings for test data"]},{"cell_type":"code","metadata":{"id":"7GeQU6Effca7"},"source":["test_data=pd.read_csv(\"/content/drive/MyDrive/test_qa.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_j060Slfca8"},"source":["test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnJ6sVhMfv0N"},"source":["tokenized_test_z=tokenizer([t for t in test_data['Z']],truncation=True,padding=True, return_tensors='pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFmIcN-Rfv0O"},"source":["test_dataset_z=TensorDataset(tokenized_test_z['input_ids'],tokenized_test_z['attention_mask'],tokenized_test_z['token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-OWu0Oxfv0O"},"source":["z_sampler=SequentialSampler(test_dataset_z)\n","\n","z_test_dataloader=DataLoader(test_dataset_z, sampler=z_sampler, batch_size=batch_size, shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyvZv5mhgHpx"},"source":["num_test_epochs = 1\n","encoded_z=[]\n","z_test_iterator = trange(num_test_epochs,desc=\"epochs\")\n","for _ in z_train_iterator:\n","    epoch_iterator = tqdm(z_test_dataloader, desc=\"Iteration\")\n","    for step, batch in enumerate(epoch_iterator):\n","      batch = tuple(t.to(device) for t in batch)\n","      inputs = {'input_ids':       batch[0],\n","                'attention_mask':  batch[1], \n","                'token_type_ids':  batch[2]}\n","      outputs = bert_z(**inputs)\n","      cls_tok=outputs[0][:,0,:]\n","      encoded_z.append(cls_tok)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J42MFDSSgQQJ"},"source":["new_z=torch.stack(encoded_z)\n","new_z1=new_z.reshape(-1,768)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuWuHwfzgQQK"},"source":["torch.save(new_z1,\"encoded3_doc.pt\")\n","from google.colab import files\n","files.download('encoded3_doc.pt')"],"execution_count":null,"outputs":[]}]}