[
    {
        "in_text": "Where can you view a profile created using emit_nvtx?torch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "out_text": "Nvidia Visual Profiler"
    },
    {
        "in_text": "What are the invariants of a sparse COO tensor?Tensor.matmul Seetorch.matmul()Tensor.isreal Seetorch.isreal()Tensor.frexp Seetorch.frexp()",
        "out_text": "s.sparse_dim(), K = s.dense_dim()"
    },
    {
        "in_text": "Patterns can be either module names (\"foo.bar\") or what?Boolean torch.bool torch.*.BoolTensorScriptModule A wrapper around C++torch::jit::Module.row_stack Alias of torch.vstack().",
        "out_text": "globs"
    },
    {
        "in_text": "What is nn.SELU?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.atan Seetorch.atan()",
        "out_text": "Applied element-wise"
    },
    {
        "in_text": "What are the bare minimum arguments to build a CUDA/C++ extension?softshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()Tensor.i0_ In-place version ofi0()",
        "out_text": "CUDA include path, library path and runtime library"
    },
    {
        "in_text": "What can be created from provided tensors?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismExample Compute capabilities:",
        "out_text": "a block diagonal matrix"
    },
    {
        "in_text": "What is the tensor based on?\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Ln-norm"
    },
    {
        "in_text": "What should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and summupsample Upsamples the input to either the givensizeor the givenscale_factorTensor.addbmm_ In-place version ofaddbmm()Tensor.transpose_ In-place version oftranspose()",
        "out_text": "crashed and resumed experiments"
    },
    {
        "in_text": "If the output tensor is of the same size as input, what is it?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.isreal Seetorch.isreal()Tensor.kthvalue Seetorch.kthvalue()",
        "out_text": "IfkeepdimisTrue"
    },
    {
        "in_text": "What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?IfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "a tensor"
    },
    {
        "in_text": "Returns a 3-dimensional view of each input tensor with what dimensions?round Returns a new tensor with each of the elements of inputrounded to the closest integer.Returns the random number generator state as atorch.ByteTensor. Returns the initial seed for generating random numbers as afloor Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "out_text": "zero"
    },
    {
        "in_text": "What is the value of the scales used to convert a float tensor to a per-channel quantized tensorReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "zero points"
    },
    {
        "in_text": "What is the preferred way to createScriptModules?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "2.torch.jit.script(nn_module_instance)"
    },
    {
        "in_text": "In the symbolic function, if the operator is already standardized in ONNX, we just need to do what to represent the ONNX operatorSeetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, youFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you",
        "out_text": "create a node"
    },
    {
        "in_text": "What type of tensor returns ones on the diagonal and zeros elsewhere?Tensor.moveaxis Seetorch.moveaxis()Tensor.isposinf Seetorch.isposinf()IfupperisFalse, the returned matrixLis lower-triangular, and",
        "out_text": "2-D tensor"
    },
    {
        "in_text": "What is the return value of the low-level function for calling LAPACK's geqrf?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "namedtuple"
    },
    {
        "in_text": "AdaptiveMaxPool3d Applies what type of adaptive max pooling over an input signal composed of several input planes?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismnn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes.",
        "out_text": "3D"
    },
    {
        "in_text": "What is currently not traceable?\u2019fro\u2019 Frobenius norm \u2013Notetorch",
        "out_text": "Tensor constructors"
    },
    {
        "in_text": "What is the term for persistent_workers?Alias fortorch.ne().Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "IfTrue"
    },
    {
        "in_text": "What does the currentStream for a given device return?upsample Upsamples the input to either the givensizeor the givenscale_factorcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()",
        "out_text": "the currently selectedStreamfor a given device"
    },
    {
        "in_text": "What indicates if CUDNN is currently available?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "bool"
    },
    {
        "in_text": "What does current implementation of torch.Tensor introduce?Tensor.ndimension Alias fordim()cosine_embedding_loss SeeCosineEmbeddingLossfor details.soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "memory overhead"
    },
    {
        "in_text": "What rounds the results of the division towards zero?IfupperisFalse, the returned matrixLis lower-triangular, andIfkeepdimisTrue, both thevaluesandindicestensorscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "trunc"
    },
    {
        "in_text": "What is operator_export_type?Tensor.i0_ In-place version ofi0()Tensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()",
        "out_text": "OperatorExportTypes"
    },
    {
        "in_text": "What is an example of a Heaviside step function?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.frexp Seetorch.frexp()",
        "out_text": "Example:"
    },
    {
        "in_text": "What does return a tensor of size endstartstepleftlceil fractextendCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossTensor.negative_ In-place version ofnegative()Tensor.lgamma_ In-place version oflgamma()",
        "out_text": "a 1-D tensor"
    },
    {
        "in_text": "What parsing has limited support for the__import__(...)syntax?Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warningcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.tanh_ In-place version oftanh()",
        "out_text": "AST"
    },
    {
        "in_text": "What is lazy initialization of of theConv1d that is inferred from theinput.size(1)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.This is the second value returned bytorch.max(). See its",
        "out_text": "thein_channelsargument"
    },
    {
        "in_text": "What does Holds parameters in a dictionary?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "Holds parameters in a list"
    },
    {
        "in_text": "What is a warning when the distance between any two singular values is close to zero?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Warning"
    },
    {
        "in_text": "What is the number of M[sparse_csr] at V[strided] -> V[strideTensor.i0_ In-place version ofi0()Tensor.cosh_ In-place version ofcosh()Tensor.tanh_ In-place version oftanh()",
        "out_text": "no"
    },
    {
        "in_text": "What is the indices of specified elements called?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfupperisFalse, the returned matrixLis lower-triangular, andtorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "ndim"
    },
    {
        "in_text": "What is a warning about usinglstsq()?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "Warning"
    },
    {
        "in_text": "inv_ex computes what of a square matrix  if it is invertible?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismTensor.log10 Seetorch.log10()",
        "out_text": "inverse"
    },
    {
        "in_text": "What is a LongTensor Boolean torch.bool?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.special.expit().",
        "out_text": "LongTensor torch.cuda"
    },
    {
        "in_text": "What does memory_format return?Tensor.ndimension Alias fordim()Tensor.i0_ In-place version ofi0()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "the desired memory format of returned tensor"
    },
    {
        "in_text": "What type of control flow is supported by PyTorch?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "static control flow"
    },
    {
        "in_text": "nn.Threshold Thresholds each element of what input?function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.clip_grad_value_ Clips gradient of an iterable of parameters at specified value.function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable.",
        "out_text": "Tensor"
    },
    {
        "in_text": "What is clip that shows the gradient of an iterable of parameters at specified value?upsample Upsamples the input to either the givensizeor the givenscale_factorcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "clip_grad_value"
    },
    {
        "in_text": "If there are multiple maximal values, the indices of the first maximal value are returned.The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). WarningNote The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warningsin Returns a new tensor with the sine of the elements of input.",
        "out_text": "multiple maximal values"
    },
    {
        "in_text": "What are the returned pivots if pivot is False?Tensor.t Seetorch.t()condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifdist Returns the p-norm of (input-other)",
        "out_text": "a tensor filled with zeros of the appropriate size"
    },
    {
        "in_text": "How to use Mixing Tracing and Scripting, give an example?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)"
    },
    {
        "in_text": "What infers data type fromvalues?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsTensor.ndimension Alias fordim()",
        "out_text": "if None"
    },
    {
        "in_text": "What Computes the natural logarithm of the absolute value of the gamma function on input?igammac Computes the regularized upper incomplete gamma function:igammac Computes the regularized upper incomplete gamma function:igamma Computes the regularized lower incomplete gamma function:",
        "out_text": "lgamma"
    },
    {
        "in_text": "What is an example of a non-quantized tensor?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length Tensorscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "e2e"
    },
    {
        "in_text": "it is recommended to use what to validate the timing?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.However, there are some steps you can take to limit the number of sources of",
        "out_text": "separate runs with and without shape recording"
    },
    {
        "in_text": "What is the learning rate policy for each parameter group?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningpositive Returnsinput.positive Returnsinput.",
        "out_text": "1cycle"
    },
    {
        "in_text": "What is the setuptools.Extension for?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteBoolean torch.bool torch.*.BoolTensorenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "CUDA/C++"
    },
    {
        "in_text": "a torch.nn.Conv2dmodule with what initialization of thein_channelsargument of theConv2Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasscatter_add Out-of-place version of torch.Tensor.scatter_add_()Tensor.addbmm_ In-place version ofaddbmm()",
        "out_text": "lazy"
    },
    {
        "in_text": "What is the information that a Graph requires?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "What are the inputs to the method"
    },
    {
        "in_text": "Holds submodules in a dictionary <sep>Tensor.cumsum_ In-place version ofcumsum()Tensor.hypot_ In-place version ofhypot()Tensor.tanh_ In-place version oftanh()",
        "out_text": "Holds submodules in a dictionary"
    },
    {
        "in_text": "What computes the dot product between a vector vand the Hessian of a given scalar function at the point given byTensor.kthvalue Seetorch.kthvalue()Tensor.isclose Seetorch.isclose()Tensor.isclose Seetorch.isclose()",
        "out_text": "functional.vhp Function"
    },
    {
        "in_text": "What is the name of the metric parameter and it's corresponding value in add_hparams method?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.ndim Alias fordim()",
        "out_text": "metric_dict"
    },
    {
        "in_text": "How to use torch.zeros_like, give an example?Tensor.i0_ In-place version ofi0()Tensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": ">>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])"
    },
    {
        "in_text": "What walks through interfacing TorchScript with OpenCV?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "TheExtending TorchScript with Custom C++ Operatorstutorial"
    },
    {
        "in_text": "What is faster and more numerically stable than computing the pseudoinverse explicitly?The published models should be at least in a branch/tag. It can\u2019t be a random commit.Completely reproducible results are not guaranteed across PyTorch releases,Completely reproducible results are not guaranteed across PyTorch releases,",
        "out_text": "usingtorch.linalg.lstsq()"
    },
    {
        "in_text": "What does Alias fortorch.acosh do?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor"
    },
    {
        "in_text": "Blocklist modules whose names match what from the list of modules the package can import?All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:Tensor.fix_ In-place version offix()",
        "out_text": "glob patterns"
    },
    {
        "in_text": "InstanceNorm2d Applies Instance Normalization over what input?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismTensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.",
        "out_text": "4D"
    },
    {
        "in_text": "Prior to PyTorch 1.1.0, when was the learning rate scheduler expected to be called?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "out_text": "before the optimizer\u2019s update"
    },
    {
        "in_text": "Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused, and will tryOf course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. WarningExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "out_text": "Include only what you use"
    },
    {
        "in_text": "What method can be used to create a coalesced copy of a sparse COO tensor?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismTensor.isreal Seetorch.isreal()",
        "out_text": "torch"
    },
    {
        "in_text": "What does liketorch.minimum() compute?Tensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "the element-wise minimum ofinputandother"
    },
    {
        "in_text": "What is paper that describes Instance Normalization over a 4D input?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "Instance Normalization: The Missing Ingredient for Fast Stylization"
    },
    {
        "in_text": "What is the name of the method that can be traced as calls to?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Customizing Tracing with the Tracer class"
    },
    {
        "in_text": "What is Ninja's advantage over setuptools.build_ext?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "greatly speeds up compilation"
    },
    {
        "in_text": "What method returns  true if two tensors have the same size and elements?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "equal"
    },
    {
        "in_text": "What keyword causes the exporter to print out a human-readable representation of the network?You can usetorch.manual_seed()to seed the RNG for all devices (bothupsample Upsamples the input to either the givensizeor the givenscale_factorThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "verbose=True"
    },
    {
        "in_text": "Default:True pad_mode(string,optional) \u2013 controls the padding method used what?>>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')get_rng_state Returns the random number generator state as a torch.ByteTensor.",
        "out_text": "whencenterisTrue"
    },
    {
        "in_text": "What does Returns the indices of the minimum value(s) of the flattened tensor or along a dimension?condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifIfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "the indices of the minimum value(s) of the flattened tensor or along a dimension"
    },
    {
        "in_text": "Howtorch.packagefinds your code's dependencies?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Dependency Management torch.packagesharp edges"
    },
    {
        "in_text": "What does set_rng_state set?Tensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()Alias fortorch.trunc()",
        "out_text": "random number generator state"
    },
    {
        "in_text": "What does torch.linalg.multi_dot() accept instead of multiple arguments?Tensor.take_along_dim Seetorch.take_along_dim()Tensor.tanh_ In-place version oftanh()Tensor.i0_ In-place version ofi0()",
        "out_text": "a list of two or more tensors"
    },
    {
        "in_text": "Returns True if the inputs a single element tensor which is not equal to zero after type conversions?Tensor.is_signed Returns True if the data type ofselfis a signed data type.b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "out_text": "if the inputs a single element tensor"
    },
    {
        "in_text": "in torch.logsumexp If keepdim is True, the output tensor has how many fewer dimension(s)?while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: >>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "1"
    },
    {
        "in_text": "What is supported to a common shape, type promotion, and integer and float inputs?ScriptModule A wrapper around C++torch::jit::Module.Tensor.index_select Seetorch.index_select()enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "broadcasting"
    },
    {
        "in_text": "What are bound to names in Python?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "functions and classes"
    },
    {
        "in_text": "What is Tensor.vdot?Tensor.ndim Alias fordim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Seetorch.vdot"
    },
    {
        "in_text": "What version ofaddbmm() is used?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a conveniencecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "Tensor.addbmm_ In-place"
    },
    {
        "in_text": "What is the minimum number of bins?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "minlength(int)"
    },
    {
        "in_text": "What must be provided when exporting a ScriptModule or TorchScript Function?softshrink Applies the soft shrinkage function element wiseAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "\u2018example_outputs\u2019"
    },
    {
        "in_text": "The returned tensor shares the storage with what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "the input tensor"
    },
    {
        "in_text": "What is a way to test if a package is executing inside a package?The published models should be at least in a branch/tag. It can\u2019t be a random commit.The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "Patch code into a package"
    },
    {
        "in_text": "What is another name for Alias for torch.acos?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.special.expit().",
        "out_text": "Alias for torch.acos"
    },
    {
        "in_text": "What need to be specified as collections that have a deterministic ordering that is consistent between runs?The published models should be at least in a branch/tag. It can\u2019t be a random commit.torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "out_text": "Warning Parameters"
    },
    {
        "in_text": "What computes the QR decomposition of a matrix  or a batch of matricesinput?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Returns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "qr"
    },
    {
        "in_text": "Along what axis are dstack Stack tensors in sequence depthwise?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "third axis"
    },
    {
        "in_text": "Tracing of in-place operations of tensor views (e.g. what is on the left-hand side of an assignment)sort Sorts the elements of the input tensor along a given dimension in ascending order by value.sort Sorts the elements of the input tensor along a given dimension in ascending order by value.embedding A simple lookup table that looks up embeddings in a fixed dictionary and size.",
        "out_text": "indexing"
    },
    {
        "in_text": "What type of sequences does nn.utils.rnn.pack_padded_sequence Pack a TensorTensor.sqrt_ In-place version ofsqrt()Tensor.ldexp_ In-place version ofldexp()Tensor.bitwise_or_ In-place version ofbitwise_or()",
        "out_text": "padded sequences"
    },
    {
        "in_text": "What is the default to export the model to the opset version of the onnx submodule?Tensor.i0_ In-place version ofi0()You can usetorch.manual_seed()to seed the RNG for all devices (bothWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "opset_version"
    },
    {
        "in_text": "What is below the list of supported patterns for LHS indexing?Tensor.ndimension Alias fordim()upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "unsupported patterns"
    },
    {
        "in_text": "What is deprecated and may be removed in a future PyTorch release?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasWarning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. WarningExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "torch.norm"
    },
    {
        "in_text": "What is Seetorch.lcm()?Tensor.take_along_dim Seetorch.take_along_dim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Tensor.lcm"
    },
    {
        "in_text": "What is the sign of a ByteTensor?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "8-bit integer"
    },
    {
        "in_text": "What will dependencies on this module do during package export?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "raise an error"
    },
    {
        "in_text": "Everything in a user definedTorchScript Classis exported what?ScriptModule A wrapper around C++torch::jit::Module.Boolean torch.bool torch.*.BoolTensorWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "by default"
    },
    {
        "in_text": "If root is a GraphModule, what type of object will references to Module-based objects be copied over from the respective place within rootparameters_to_vector Convert parameters to one vectorTensor.ldexp_ In-place version ofldexp()Tensor.addbmm_ In-place version ofaddbmm()",
        "out_text": "Module"
    },
    {
        "in_text": "Who owns The.data/directory?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas>>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "torch.package"
    },
    {
        "in_text": "What are all operators exported as?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "ATen ops"
    },
    {
        "in_text": "Which root can be an nn.Module instance or a Dict mapping strings to any attribute type?All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "out_text": "root"
    },
    {
        "in_text": "What happens if the callback function throws?alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "the future returned bythenwill be marked appropriately with the encountered error"
    },
    {
        "in_text": "How many bits does float32ortorch.float torch have?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "32"
    },
    {
        "in_text": "Quantized Tensors allow for what?IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same sizenn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "out_text": "serialization of data in a quantized format"
    },
    {
        "in_text": "What is the default value for return half of results to avoid redundancy for real inputs?upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wiseThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "False onesided"
    },
    {
        "in_text": "What does nn.EmbeddingBag compute?soft_margin_loss SeeSoftMarginLossfor details.Tensor.take_along_dim Seetorch.take_along_dim()Alias fortorch.trunc()",
        "out_text": "means of \u2018bags\u2019"
    },
    {
        "in_text": "What is increasing(bool,optional)?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceThis is the second value returned bytorch.max(). See itsIf you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...",
        "out_text": "Order of the powers of the columns"
    },
    {
        "in_text": "What is an example of a function that does not need a decorator?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "@torch.jit.exporton a method"
    },
    {
        "in_text": "Tensor.new_full returns a Tensor of what size?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Tensor.arccosh_ acosh_() -> TensorTensor.arccosh_ acosh_() -> Tensor",
        "out_text": "size filled"
    },
    {
        "in_text": "How to use torch.neg, give an example?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": ">>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])"
    },
    {
        "in_text": "How to use torch.squeeze, give an example?Broadcastsinputto the shapeshape.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": ">>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])"
    },
    {
        "in_text": "How to use Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:, give an example?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "out_text": ">>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])"
    },
    {
        "in_text": "How to use For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It\u2019s also easy to move all parameters to a different device or change their precision using\nto():, give an example?Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call.script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "out_text": "# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)"
    },
    {
        "in_text": "What is nn.GaussianNLLLoss Gaussian?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Tensor.ndimension Alias fordim()",
        "out_text": "negative log likelihood loss"
    },
    {
        "in_text": "What does lgamma compute?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "the natural logarithm of the absolute value of the gamma function on input"
    },
    {
        "in_text": "What Seetorch.addbmm() Tensor?Tensor.flip Seetorch.flip()Tensor.tanh_ In-place version oftanh()Tensor.asinh_ In-place version ofasinh()",
        "out_text": "addbmm"
    },
    {
        "in_text": "What is the one dimensional discrete Fourier transform of?Tensor.isreal Seetorch.isreal()Tensor.kthvalue Seetorch.kthvalue()igammac Computes the regularized upper incomplete gamma function:",
        "out_text": "Hermitian symmetricinputsignal"
    },
    {
        "in_text": "What can one construct of a sparse COO tensor using the torch?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "coalesced copy"
    },
    {
        "in_text": "What did load Load aScriptModuleorScriptFunctionpreviously save?Tensor.i0_ In-place version ofi0()soft_margin_loss SeeSoftMarginLossfor details.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "with torch.jit.save"
    },
    {
        "in_text": "What is Tensor.histc?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Seetorch.histc"
    },
    {
        "in_text": "Computes the bitwise what of inputandother?IfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "OR"
    },
    {
        "in_text": "The synchronization of tensors that reside on GPUs should be done separately through a call to what?softshrink Applies the soft shrinkage function element wisecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "wait()"
    },
    {
        "in_text": "A tensor of specific data type can be constructed by passing what to a constructor?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "torch.dtype and/or a torch.device"
    },
    {
        "in_text": "What can be saved to the archive using pickle?You can usetorch.manual_seed()to seed the RNG for all devices (bothsoft_margin_loss SeeSoftMarginLossfor details.devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "python object"
    },
    {
        "in_text": "Tensor.new_zeros Returns a Tensor of size size filled with what?from_numpy Creates a Tensor froma numpy.ndarray.Tensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.",
        "out_text": "0."
    },
    {
        "in_text": "What is an object that represents the data type of atorch.Tensor?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length Tensorstorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "Atorch.dtype"
    },
    {
        "in_text": "What does expected(Any) mean?NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Expected input"
    },
    {
        "in_text": "What must you do for each module that the dependency resolver finds?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "specify an action to take"
    },
    {
        "in_text": "Why do we allow Caffe2 to call directly to Torch implementations of operators?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "to help you smooth over these differences when precision is important"
    },
    {
        "in_text": "What function returns output of dtypebool for all supported dtypes exceptuint8?Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "NumPy"
    },
    {
        "in_text": "If downloaded file is a what, it will be automatically decompressed. If the object is already present inmodel_dir, it\u2019s desenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceIn order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type",
        "out_text": "zip file"
    },
    {
        "in_text": "What does Seetorch.swapdims do?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Tensor.swapdims"
    },
    {
        "in_text": "Where can you save a version of this module for use in a separate process?You can usetorch.manual_seed()to seed the RNG for all devices (bothgeqrf This is a low-level function for calling LAPACK\u2019s geqrf directly.Tensor.i0_ In-place version ofi0()",
        "out_text": "offline"
    },
    {
        "in_text": "What does nn.TripletMarginWithDistanceLoss create?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "criterion"
    },
    {
        "in_text": "What is the second tutorial in a series of three tutorials?Note\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()",
        "out_text": "leanr how to generate names from languages"
    },
    {
        "in_text": "Who will try to handle implicit scalar datatype casting?softshrink Applies the soft shrinkage function element wiseAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "the exporter"
    },
    {
        "in_text": "What corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for aAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')get_rng_state Returns the random number generator state as a torch.ByteTensor.true_divide Alias for torch.div()withrounding_mode=None.",
        "out_text": "Element 0"
    },
    {
        "in_text": "What Applied element-wise, as: nn.CELU Applies the element-wise function: nn.Sigmoid AppTensor.logical_or_ In-place version oflogical_or()Tensor.lgamma_ In-place version oflgamma()Tensor.lgamma_ In-place version oflgamma()",
        "out_text": "nn.SELU"
    },
    {
        "in_text": "What will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes asYou can usetorch.manual_seed()to seed the RNG for all devices (bothTensor.ldexp_ In-place version ofldexp()ScriptModule A wrapper around C++torch::jit::Module.",
        "out_text": "Freezing aScriptModule"
    },
    {
        "in_text": "Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvctorch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "out_text": "nvcc"
    },
    {
        "in_text": "What build extension?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "custom setuptools"
    },
    {
        "in_text": "current_blas_handle Returns cublasHandle_t pointer to what?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "current cuBLAS handle"
    },
    {
        "in_text": "Which bool controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on AmTensor.acosh_ In-place version ofacosh()Tensor.arctanh_ In-place version ofarctanh()Tensor.i0_ In-place version ofi0()",
        "out_text": "SeeTensorFloat-32(TF32)"
    },
    {
        "in_text": "What does current_stream return?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "current_stream Returns the currently selectedStreamfor a given device"
    },
    {
        "in_text": "What is a significant confounding factor when measuring code?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoftshrink Applies the soft shrinkage function element wiseupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "run-to-run variation"
    },
    {
        "in_text": "What library has a bug that causes the LU factorization to be repeated for singular matrices?Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "MAGMA library"
    },
    {
        "in_text": "In what state is the API in?Alias fortorch.trunc()Alias fortorch.trunc()absolute Alias for torch.abs()",
        "out_text": "beta"
    },
    {
        "in_text": "What function is deprecated in favor oftorch.linalg.eigh()?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.conj Seetorch.conj()Tensor.conj Seetorch.conj()",
        "out_text": "torch.symeig()"
    },
    {
        "in_text": "Does the stashing logic have any way to anticipate if the user will move Tensors to a new device within the run_fnupsample Upsamples the input to either the givensizeor the givenscale_factorYou can usetorch.manual_seed()to seed the RNG for all devices (bothcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "no way to anticipate if the user will move Tensors to a new device within the run_fn itself"
    },
    {
        "in_text": "What can be found in the indices of PyTorch sparse COO tensors?Tensor.cummax Seetorch.cummax()Tensor.cummax Seetorch.cummax()Tensor.kthvalue Seetorch.kthvalue()",
        "out_text": "duplicate coordinates"
    },
    {
        "in_text": "What is it recommended to set if you are interested in quantizing a model to run on ARM?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Alias fortorch.trunc()",
        "out_text": "qconfig"
    },
    {
        "in_text": "How to use torch.special.exp2, give an example?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteBoolean torch.bool torch.*.BoolTensorWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": ">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])"
    },
    {
        "in_text": "What is supported for a dense layout?\u2019fro\u2019 Frobenius norm \u2013upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "Onlytorch.strided"
    },
    {
        "in_text": "How to use A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g., give an example?Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "out_text": "exporter.mock(\"**\", exclude=[\"torchvision.**\"])"
    },
    {
        "in_text": "What are two ways to avoid a copy of a tensor?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoftshrink Applies the soft shrinkage function element wise",
        "out_text": "requires_grad_() or detach()"
    },
    {
        "in_text": "In what book has SWA been proposed?\u2019fro\u2019 Frobenius norm \u2013Notetorch",
        "out_text": "Averaging Weights Leads to Wider Optima and Better Generalization"
    },
    {
        "in_text": "What is set if environment variableXDG_CACHE_HOMEis set?Tensor.i0_ In-place version ofi0()Tensor.index_select Seetorch.index_select()Tensor.ldexp_ In-place version ofldexp()",
        "out_text": "$XDG_CACHE_HOME/torch/hub"
    },
    {
        "in_text": "What quantiles of each row of the input tensor along the dimensiondim?upsample Upsamples the input to either the givensizeor the givenscale_factorcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "q-th"
    },
    {
        "in_text": "Package Importerexposes complementary methods called what?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossTensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "load_pickle"
    },
    {
        "in_text": "What type of tensor does Alias fortorch.linalg.matrix_power() return?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Tensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.Tensor.bitwise_and Seetorch.bitwise_and()",
        "out_text": "2-D"
    },
    {
        "in_text": "What is the replacement for torch.solve()?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notepinverse Alias for torch.linalg.pinv()pinverse Alias for torch.linalg.pinv()",
        "out_text": "torch.linalg.solve()"
    },
    {
        "in_text": "What are the names to assign to the output nodes of the graph?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "output_names"
    },
    {
        "in_text": "What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoft_margin_loss SeeSoftMarginLossfor details.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "8-bit"
    },
    {
        "in_text": "If a module is intern-ed, it will be what?\u2019fro\u2019 Frobenius norm \u2013upsample Upsamples the input to either the givensizeor the givenscale_factorcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "placed into the package"
    },
    {
        "in_text": "What are the elements of polar Construct a complex tensor?\u2019fro\u2019 Frobenius norm \u2013callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "Cartesian coordinates"
    },
    {
        "in_text": "What is the input tensor using replication of the input boundary?The published models should be at least in a branch/tag. It can\u2019t be a random commit.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "Pads"
    },
    {
        "in_text": "What is version oferf()?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "Tensor.erf_ In-place"
    },
    {
        "in_text": "Who returns a new tensor with the arcsine of the elements ofinput?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsTensor.triangular_solve Seetorch.triangular_solve()",
        "out_text": "Alias fortorch.asin()"
    },
    {
        "in_text": "What can you specify to exlcude?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "patterns"
    },
    {
        "in_text": "What is the parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and correspondingdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to forkdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to forkdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "main_tag"
    },
    {
        "in_text": "What two classes does nn.BCEWithLogitsLoss combine?IfkeepdimisTrue, both thevaluesandindicestensorscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notenn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "aSigmoidlayer and theBCELossin"
    },
    {
        "in_text": "What is currently available?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningThis module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Warning",
        "out_text": "CUDA"
    },
    {
        "in_text": "How is the cosine similarity between x1 and x2 computed?The published models should be at least in a branch/tag. It can\u2019t be a random commit.The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "computed along dim"
    },
    {
        "in_text": "What is the name of the logarithmic scale used to create a one-dimensional tensor of sizesteps?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "basebase"
    },
    {
        "in_text": "What is an example of a sequence of tensors to concatenate out(Tensor,optional) \u2013nn.Transformer A transformer model.torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "Example"
    },
    {
        "in_text": "Sets the gradients of all optimizedtorch.Tensors to what?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "zero"
    },
    {
        "in_text": "What is the term for add_1?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "linear"
    },
    {
        "in_text": "What is index_copy() called on?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "a CPU or CUDA tensor"
    },
    {
        "in_text": "Iftrackersetsbvars = True, the iteration process will be hard-stopped.If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedAll subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "out_text": "Iftrackersetsbvars"
    },
    {
        "in_text": "What can you depend on?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "third-party libraries"
    },
    {
        "in_text": "What is atan avg_pool1d avg_pool2d avg_pool3d as_sTensor.i0_ In-place version ofi0()avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp ltTensor.ldexp_ In-place version ofldexp()",
        "out_text": "argmin"
    },
    {
        "in_text": "What must be _onnx_main_opset or in _onnx_stable_opsets?soft_margin_loss SeeSoftMarginLossfor details.Tensor.i0_ In-place version ofi0()Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "opset_version"
    },
    {
        "in_text": "When are subclasses particularly useful?NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "when data come from a stream"
    },
    {
        "in_text": "Extra_cflags - optional list of what to forward to the build?torch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "out_text": "compiler flags"
    },
    {
        "in_text": "What module has lazy initialization of thein_channelsargument of theConv2d?upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "nn.LazyConv2d a torch.nn.Conv2dmodule"
    },
    {
        "in_text": "What does this no-op do for storages already in shared memory and for CUDA storages?softshrink Applies the soft shrinkage function element wisecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "Moves the storage to shared memory"
    },
    {
        "in_text": "What does atorch.ByteTensor mean?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Sets the seed for generating random numbers"
    },
    {
        "in_text": "Are the indices of specified tensor elements unique or unique?Tensor.kthvalue Seetorch.kthvalue()prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random.Tensor.nonzero Seetorch.nonzero()",
        "out_text": "unique"
    },
    {
        "in_text": "Computes the one dimensional discrete Fourier transform of what?Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,Returns the product of each row of theinputtensor in the given",
        "out_text": "Hermitian symmetricinputsignal"
    },
    {
        "in_text": "Where are the indices specified?Tensor.ndimension Alias fordim()Tensor.istft Seetorch.istft()Tensor.istft Seetorch.istft()",
        "out_text": "theindextensor"
    },
    {
        "in_text": "What returns the other elements of the result?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningreduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "tensoroutare"
    },
    {
        "in_text": "What is the value of the inverse of MaxUnpool3d?Tensor.ndimension Alias fordim()softshrink Applies the soft shrinkage function element wisecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "nn"
    },
    {
        "in_text": "What does Tensor.tril Seetorch.tril do?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.matmul Seetorch.matmul()",
        "out_text": "Tensor.tril Seetorch.tril()"
    },
    {
        "in_text": "How to use Methods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy:, give an example?Tensor.lgamma_ In-place version oflgamma()Tensor.lgamma_ In-place version oflgamma()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()"
    },
    {
        "in_text": "What is the Tensor that is detached from the graph that created it?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "a leaf"
    },
    {
        "in_text": "What is loss caused by cosineEmbeddingLoss?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "cosine_embedding_loss"
    },
    {
        "in_text": "What is the term for a seetorch?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "Tensor"
    },
    {
        "in_text": "Name denotes the name of this GraphModule for what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.det Seetorch.det()Tensor.ndimension Alias fordim()",
        "out_text": "debugging purposes"
    },
    {
        "in_text": "Where does functional.vjp compute the dot product between a vector v and the Jacobian of a given function?Tensor.requires_grad Is True if gradients need to be computed for this Tensor,False otherwise.Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p).Tensor.bitwise_and Seetorch.bitwise_and()",
        "out_text": "the point given by the inputs"
    },
    {
        "in_text": "What does function._ContextMethodMixin.save_for_backward save?Tensor.i0_ In-place version ofi0()Tensor.ndim Alias fordim()Tensor.ndimension Alias fordim()",
        "out_text": "given tensors"
    },
    {
        "in_text": "What is an example of a function that should know how to handle the inputs passed as the tuple?The published models should be at least in a branch/tag. It can\u2019t be a random commit.The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "LSTM"
    },
    {
        "in_text": "What does not mean PyTorch is built with CUDA support?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "CUDA"
    },
    {
        "in_text": "What implements the lazy version of Adam algorithm suitable for sparse tensors?Returns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "Adam algorithm"
    },
    {
        "in_text": "What type of pooling does adaptive_max_pool2d apply?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoftshrink Applies the soft shrinkage function element wiseRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "2D"
    },
    {
        "in_text": "What does Checkpointing currently only support?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "torch.autograd.backward()"
    },
    {
        "in_text": "What language does setuptools.Extension work for?Tensor.ndimension Alias fordim()Tensor.ndim Alias fordim()Tensor.det Seetorch.det()",
        "out_text": "C++"
    },
    {
        "in_text": "What is the name of the object that raises TypeError if no implementation is found?Tensor.i0_ In-place version ofi0()Tensor.ndimension Alias fordim()Tensor.cosh_ In-place version ofcosh()",
        "out_text": "Example"
    },
    {
        "in_text": "a torch.nn.BatchNorm2dmodule with lazy initialization of what?Boolean torch.bool torch.*.BoolTensorScriptModule A wrapper around C++torch::jit::Module.torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "out_text": "thenum_featuresargument of theBatchNorm2d"
    },
    {
        "in_text": "What type of datasets can users alternatively specifybatch_sampler?Tensor.ndimension Alias fordim()Tensor.i0_ In-place version ofi0()Tensor.tanh_ In-place version oftanh()",
        "out_text": "map-style datasets"
    },
    {
        "in_text": "What type of Quantization includes both weight and activations?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "Static Quantization"
    },
    {
        "in_text": "What is the default value of the upper-triangular system of equations?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIf you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.This is the second value returned bytorch.max(). See its",
        "out_text": "True"
    },
    {
        "in_text": "How many bits are floating point1?Tensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "16"
    },
    {
        "in_text": "What does inference_mode Context-manager enable or disable?Tensor.ndimension Alias fordim()Tensor.ndim Alias fordim()Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "inference mode"
    },
    {
        "in_text": "What is an example of a setuptools.Extension constructor?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.tanh_ In-place version oftanh()Tensor.ndimension Alias fordim()",
        "out_text": "Compute capabilities"
    },
    {
        "in_text": "torch.all matches the behavior of what function in returning output of dtype bool?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Boolean torch.bool torch.*.BoolTensorTensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.",
        "out_text": "NumPy"
    },
    {
        "in_text": "What will need to be present in theimporterlist for this to be possible to save objects that have previously been packaged?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.i0_ In-place version ofi0()Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "importer\u2019simport_modulemethod"
    },
    {
        "in_text": "What is the COO tensor?\u2019fro\u2019 Frobenius norm \u2013upsample Upsamples the input to either the givensizeor the givenscale_factorTensor.ndimension Alias fordim()",
        "out_text": "sparse"
    },
    {
        "in_text": "What is the name of the tensor wherevaluesis the k th smallest element of each row of the input tensor in theIfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same sizeIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "named tuple"
    },
    {
        "in_text": "What parameter should be set to match the backend?The published models should be at least in a branch/tag. It can\u2019t be a random commit.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "torch.backends.quantized.engine"
    },
    {
        "in_text": "What is the axis along which to index index(LongTensor)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "dim(int)"
    },
    {
        "in_text": "What does the Kullback-Leibler divergence Loss Function use for details?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningPlease see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "out_text": "MultiLabelSoftMarginLoss"
    },
    {
        "in_text": "What can the function to be called be?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "PyTorch operator, Python function, or member of the builtins or operator namespaces"
    },
    {
        "in_text": "Function that computes what of a given function. functional.hessian Function that computes the Hessian of a given scalfunctional.vhp Function that computes the dot product between a vector vand the Hessian of a given scalar function at the point given by the inputs.smooth_l1_loss Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "out_text": "Jacobian"
    },
    {
        "in_text": "What is Tensor.nanmedian?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.tril Seetorch.tril()Tensor.tril Seetorch.tril()",
        "out_text": "Seetorch.nanmedian"
    },
    {
        "in_text": "If increasing is what, the first column isx(N1)x(N-1)x(N1)?IfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "False"
    },
    {
        "in_text": "What is the name of all entrypoints available in hub?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "in github hubconf"
    },
    {
        "in_text": "What is the default value of generator(torch.Generator,optional)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.i0_ In-place version ofi0()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "default:None"
    },
    {
        "in_text": "How to use torch.enable_grad, give an example?Tensor.i0_ In-place version ofi0()soft_margin_loss SeeSoftMarginLossfor details.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue"
    },
    {
        "in_text": "What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningTensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "Negative inputs"
    },
    {
        "in_text": "What is nn.LazyConv1d a torch.nn.Conv2dmodule with lazy initialization of theThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedTensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.",
        "out_text": "nn.LazyConvTranspose2d"
    },
    {
        "in_text": "What is included when printing a Measurement?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "Compare"
    },
    {
        "in_text": "How to use torch.bitwise_or, give an example?soft_margin_loss SeeSoftMarginLossfor details.Tensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()",
        "out_text": ">>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])"
    },
    {
        "in_text": "What operation in kHkWkH times kWkHkW regions by step size sHsWsRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingandWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "2D average-pooling"
    },
    {
        "in_text": "What is Tensor.log1p?This is the second value returned bytorch.max(). See itsenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a conveniencetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "Seetorch.log1p"
    },
    {
        "in_text": "Returns a tensor that is what version of input?Returns a tensor of the same size asinputwith each elementReturns a namedtuple(values,indices)wherevaluesis thekthReturns a namedtuple(values,indices)wherevaluesis thekth",
        "out_text": "transposed"
    },
    {
        "in_text": "What is the name of the criterion that optimizes a multi-class classification hinge loss?Tensor.matmul Seetorch.matmul()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "TripletMarginLoss"
    },
    {
        "in_text": "What should you do if your package is intended to be heavily used?Alias fortorch.trunc()Alias fortorch.trunc()However, there are some steps you can take to limit the number of sources of",
        "out_text": "restructuring your code"
    },
    {
        "in_text": "What does TorchScript compiler fail with if the type annotation is not specified?Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningtorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "runtime error"
    },
    {
        "in_text": "What does torch.packageFormat Overview find your code's dependencies?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Dependency Management"
    },
    {
        "in_text": "What is an example of aScriptModule?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.matmul Seetorch.matmul()",
        "out_text": "model like:"
    },
    {
        "in_text": "What does mantissa and exponent tensors do?nn.Transformer A transformer model.torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Returns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "Decomposes input"
    },
    {
        "in_text": "What is the name of the add_1 method?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "linear_1"
    },
    {
        "in_text": "What does nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmoduleWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasBoolean torch.bool torch.*.BoolTensorTensor.i0_ In-place version ofi0()",
        "out_text": "thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1)"
    },
    {
        "in_text": "Why is repeats broadcasted?torchtorchNote",
        "out_text": "to fit the shape of the given axis"
    },
    {
        "in_text": "What is container that holds and manages the originalparameter or buffer of a parametrizedtorch.nn.If you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...Tensor.broadcast_to Seetorch.broadcast_to().function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation.",
        "out_text": "parametrize.ParametrizationList"
    },
    {
        "in_text": "What does IfnormalizedisTrue return?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "normalized STFT results"
    },
    {
        "in_text": "What is the default value for the padding method used when center is True?softshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "reflect"
    },
    {
        "in_text": "Where is Multiprocessing best practiceson more details related to multiprocessing?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningWarning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. WarningPlease see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "out_text": "PyTorch"
    },
    {
        "in_text": "ivars[\u201cistep\u201d]- the current approximation of eigenvectorsE- the current approximupsample Upsamples the input to either the givensizeor the givenscale_factorIfkeepdimisTrue, both thevaluesandindicestensorscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "current iteration stepX"
    },
    {
        "in_text": "What is Seetorch.split() function?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.split Seetorch.split()Tensor.ndimension Alias fordim()",
        "out_text": "Tensor.split"
    },
    {
        "in_text": "What Python Language Reference Comparison Debugging Disable JIT for Debugging?Tensor.asinh_ In-place version ofasinh()Tensor.tanh_ In-place version oftanh()Tensor.cosh_ In-place version ofcosh()",
        "out_text": "Python Functions and Modules"
    },
    {
        "in_text": "Which indices are sorted in lexicographical order?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingandcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "torch"
    },
    {
        "in_text": "In what version of Python did the @torch.jit.ignoreannotation's behavior change?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasTensor.ndimension Alias fordim()Tensor.ndim Alias fordim()",
        "out_text": "PyTorch 1.2"
    },
    {
        "in_text": "Annotations on what are not currently supported?All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules. Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported. For a full listing of supported Python features, seePython Language Reference Coverage.",
        "out_text": "local names"
    },
    {
        "in_text": "What is Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization?Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization"
    },
    {
        "in_text": "Quantization Mode Support Post Training Quantization: Dynamic, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training QuantizationEager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlyEager Mode Quantization FX Graph Mode QuantizationQuant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "out_text": "Static"
    },
    {
        "in_text": "What compiler computes the regularized upper incomplete gamma function?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()igammac Computes the regularized upper incomplete gamma function:",
        "out_text": "igammac"
    },
    {
        "in_text": "Once we verify what, the goal becomes figuring out what went wrong during our GraphModule transformation?The published models should be at least in a branch/tag. It can\u2019t be a random commit.If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "out_text": "tracing is working as expected"
    },
    {
        "in_text": "What is the name of the function that disables gradient calculation?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "Context-manager"
    },
    {
        "in_text": "Computes the what of a matrix or batches of matrices A?Returns the product of each row of theinputtensor in the givenReturns a tensor of the same size asinputwith each elementReturns a namedtuple(values,indices)wherevaluesis thekth",
        "out_text": "LU factorization"
    },
    {
        "in_text": "How to use do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,, give an example?tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)"
    },
    {
        "in_text": "What Returns a named tuple(values,indices)where values is the cumulative maximum of elements ofinput in the dimensiondWarning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin).Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort alongIfdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "out_text": "cummax"
    },
    {
        "in_text": "isnan Returns a new tensor with what elements?Tensor.det Seetorch.det()Tensor.tril Seetorch.tril()Tensor.tril Seetorch.tril()",
        "out_text": "boolean elements"
    },
    {
        "in_text": "Params is an iterable oftorch.Tensors ordicts. Specifies what Tensors shouldSeetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingTensor.scatter_ Writes all values from the tensorsrcintoselfat the indices specified in theindextensor.Tensor.scatter_ Writes all values from the tensorsrcintoselfat the indices specified in theindextensor.",
        "out_text": "iterable"
    },
    {
        "in_text": "What is the version number of Visual Studio 2019?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningcosine_embedding_loss SeeCosineEmbeddingLossfor details.Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "10.2"
    },
    {
        "in_text": "What input does Batch Normalization apply over?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "5D"
    },
    {
        "in_text": "What is the name of the callback that takes in one argument, is the reference to this Future?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningIfbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "which"
    },
    {
        "in_text": "What does extra_include_paths provide to forward to the build?upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "include directories"
    },
    {
        "in_text": "Holds parameters in a dictionary. Holds submodules in a dictionary. Holds parameters in a dictionary <sep>A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1dgithub(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Examplegithub(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "out_text": "Holds parameters in a dictionary"
    },
    {
        "in_text": "What does M =?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()",
        "out_text": "s.sparse_dim()"
    },
    {
        "in_text": "What does a Tensor Example Fill the input Tensor with?Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "out_text": "scalar value0"
    },
    {
        "in_text": "What computes a partial inverse ofMaxPool3d?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "nn.AdaptiveMaxPool3d"
    },
    {
        "in_text": "A,B,iK- input what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.take_along_dim Seetorch.take_along_dim()Alias fortorch.trunc()",
        "out_text": "Tensor arguments"
    },
    {
        "in_text": "How to use torch.triu_indices, give an example?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": ">>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])"
    },
    {
        "in_text": "What always returns the diagonal of its input?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "torch.diagonal()"
    },
    {
        "in_text": "What method is used to prune tensors corresponding to all parameters inparameters?Tensor.isreal Seetorch.isreal()Tensor.kthvalue Seetorch.kthvalue()Tensor.frexp Seetorch.frexp()",
        "out_text": "specifiedpruning_method"
    },
    {
        "in_text": "What is In-place version ofhypot()?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.tanh_ In-place version oftanh()Tensor.cumsum_ In-place version ofcumsum()",
        "out_text": "Tensor.hypot"
    },
    {
        "in_text": "Name target args kwargs placeholder what ()  get_attr linear_weight linear.weight ()Tensor.i0_ In-place version ofi0()Tensor.index_select Seetorch.index_select()Tensor.eq_ In-place version ofeq()",
        "out_text": "x x"
    },
    {
        "in_text": "What is the default padding method used oninputwhencenterisTrue?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasTensor.i0_ In-place version ofi0()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "reflect"
    },
    {
        "in_text": "What does cast this storage to complex float type return if it's not already on the CPU?You can usetorch.manual_seed()to seed the RNG for all devices (bothTensor.i0_ In-place version ofi0()upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "a CPU copy"
    },
    {
        "in_text": "What does aCallable that takes in one argument, is the reference to this Future?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsnn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "Note"
    },
    {
        "in_text": "Pytorch/vision repo dependencies variable is alistof what required for training a model?Tensor.ndimension Alias fordim()Tensor.lu_solve Seetorch.lu_solve()Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "dependencies"
    },
    {
        "in_text": "What should this op be disambiguated?Note\u2019fro\u2019 Frobenius norm \u2013torch",
        "out_text": "with torch.logsumexp()"
    },
    {
        "in_text": "What dimensions does the Alias fortorch.transpose() transpose?Tensor.ndimension Alias fordim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "0 and 1"
    },
    {
        "in_text": "What is the result of the 2D max pooling over an input signal composed of several input planes?softshrink Applies the soft shrinkage function element wisecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "Computes a partial inverse of MaxPool1d"
    },
    {
        "in_text": "nn.ReLU Applies the rectified linear unit function what?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.nn.Transformer A transformer model.The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "out_text": "element-wise"
    },
    {
        "in_text": "What is used to calculate the pairwise distance between vectorsv1v_1v1,v2v_2v2?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Returns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "p-norm"
    },
    {
        "in_text": "What does the tensor return when selfis a sparse CSR tensor of layoutsparse_csTensor.lgamma_ In-place version oflgamma()Tensor.lgamma_ In-place version oflgamma()Tensor.ldexp_ In-place version ofldexp()",
        "out_text": "column indices of theselftensor"
    },
    {
        "in_text": "What is version ofsgn()?Tensor.det Seetorch.det()Tensor.frexp Seetorch.frexp()Tensor.matmul Seetorch.matmul()",
        "out_text": "Tensor.sgn_ In-place"
    },
    {
        "in_text": "What does thatPackage Importerinstancepatches the returned module to?IfkeepdimisTrue, both thevaluesandindicestensorsnn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "use self"
    },
    {
        "in_text": "What does torch.package find your code's dependencies?soft_margin_loss SeeSoftMarginLossfor details.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteYou can usetorch.manual_seed()to seed the RNG for all devices (both",
        "out_text": "Dependency Management"
    },
    {
        "in_text": "What does RuntimeError stand for?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "Examples"
    },
    {
        "in_text": "What is the name of the function that computesinputother text inputleq textotherinAlias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "Alias for torch.le()"
    },
    {
        "in_text": "What prunes tensors corresponding to all parameters inparameters by applying the specifiedpruning_method?upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()",
        "out_text": "global_unstructured"
    },
    {
        "in_text": "What is the element-wise value of tanhshrink?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "x"
    },
    {
        "in_text": "What is the name of the cuFFT plans?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "cufft_plan_cachecaches"
    },
    {
        "in_text": "Python code generated from what underlying GraphModule?ScriptModule A wrapper around C++torch::jit::Module.All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "out_text": "Graph"
    },
    {
        "in_text": "What is an EmbeddingBag operator?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "torch.Tensoris"
    },
    {
        "in_text": "What does f =?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "fx.symbolic_trace"
    },
    {
        "in_text": "What type of string matches any string, including the empty string?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "out_text": "wildcard"
    },
    {
        "in_text": "What is the input 2-D tensor tol(float,optional) \u2013 the tolerance value?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "input(Tensor)"
    },
    {
        "in_text": "What is the default value for the default value?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningcosine_embedding_loss SeeCosineEmbeddingLossfor details.",
        "out_text": "False"
    },
    {
        "in_text": "What produces the output of TorchScript's compilation of the code for theforwardmethod?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningBroadcastsinputto the shapeshape.",
        "out_text": "The example above"
    },
    {
        "in_text": "Supports what to a common shape, type promotion, and integer and float inputs?softshrink Applies the soft shrinkage function element wiseTensor.t_ In-place version oft()Tensor.asinh_ In-place version ofasinh()",
        "out_text": "broadcasting"
    },
    {
        "in_text": "What is recommended to define for anything beyond the simplest use cases?\u2019fro\u2019 Frobenius norm \u2013callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "a custom module"
    },
    {
        "in_text": "What is the term for 'fro'?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Frobenius norm"
    },
    {
        "in_text": "What is Tensor.lu Seetorch.lu?Tensor.tanh Seetorch.tanh()torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Tensor.arccosh_ acosh_() -> Tensor",
        "out_text": "Tensor.lu Seetorch.lu"
    },
    {
        "in_text": "What is the purpose of *argsand**kwargsintorch.hub.load()?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "toinstantiatea model"
    },
    {
        "in_text": "What module with lazy initialization of thein_channelsargument of theConvTranspose1d is inferred from theinIn order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.Tensor.addbmm_ In-place version ofaddbmm()Tensor.baddbmm_ In-place version ofbaddbmm()",
        "out_text": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d"
    },
    {
        "in_text": "What is an example of a control flow that is dependent on inputs?Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "tensor shapes"
    },
    {
        "in_text": "How to use torch.trace, give an example?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": ">>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)"
    },
    {
        "in_text": "When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum dMoreover, as forgather(), the values ofindexmust benn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band a Tensorlabelyyywith values 1 or -1.",
        "out_text": "arithmetic"
    },
    {
        "in_text": "What does Alias for torch.clamp() compute?row_stack Alias of torch.vstack().row_stack Alias of torch.vstack().true_divide Alias for torch.div()withrounding_mode=None.",
        "out_text": "the element-wise conjugate of the giveninput tensor"
    },
    {
        "in_text": "What are dictionaries of integer, float, and boolean valued input parameters?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length TensorsTensor.ndimension Alias fordim()",
        "out_text": "iparams,fparams,bparams"
    },
    {
        "in_text": "What device may produce nondeterministic gradients when given tensors?upsample Upsamples the input to either the givensizeor the givenscale_factorsoftshrink Applies the soft shrinkage function element wisecallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "CUDA device"
    },
    {
        "in_text": "What compiler does setuptools.build_ext support?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "CUDA"
    },
    {
        "in_text": "What do you learn how to augment your network using?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "visual attention mechanism"
    },
    {
        "in_text": "This module exposes what for the__torch_function__protocol?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "out_text": "various helper functions"
    },
    {
        "in_text": "Description is included when printing a what?You can usetorch.manual_seed()to seed the RNG for all devices (bothdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to forkdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "Measurement"
    },
    {
        "in_text": "What is manual?Alias fortorch.trunc()Alias fortorch.trunc()absolute Alias for torch.abs()",
        "out_text": "Operator Fusion Manual Automatic"
    },
    {
        "in_text": "What provides the concept of buffers?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "PyTorch"
    },
    {
        "in_text": "What types of inputs are supported by the LOBPCG method?upsample Upsamples the input to either the givensizeor the givenscale_factordevices(iterable of CUDA IDs) \u2013 CUDA devices for which to forkdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "dense, sparse, and batches of dense matrices"
    },
    {
        "in_text": "What is the default matrix system of equations?\u2019fro\u2019 Frobenius norm \u2013Tensor.ndimension Alias fordim()Alias fortorch.trunc()",
        "out_text": "upper triangular"
    },
    {
        "in_text": "What does Seetorch.slogdet do?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Tensor.slogdet"
    },
    {
        "in_text": "InstanceNorm3d Applies Instance Normalization over what input?Tensor.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True.Tensor.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True.Tensor.as_subclass Makes aclsinstance with the same data pointer asself.",
        "out_text": "5D"
    },
    {
        "in_text": "What are computed if the argumenteigenvectors is False?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "eigenvalues"
    },
    {
        "in_text": "What is the last window in a window?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "last"
    },
    {
        "in_text": "What does fix Alias for torch.trunc() do?absolute Alias for torch.abs()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "fix Alias for torch.trunc()"
    },
    {
        "in_text": "When specifying actions, you can pass what patterns?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warningenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "multiple"
    },
    {
        "in_text": "n_fft (int) \u2013 the input tensor n_fft (int) \u2013 sizeupsample Upsamples the input to either the givensizeor the givenscale_factor\u2019fro\u2019 Frobenius norm \u2013devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "Fourier transform"
    },
    {
        "in_text": "What is the default if source='local'?Tensor.nelement Alias fornumel()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.logical_or_ In-place version oflogical_or()",
        "out_text": "True"
    },
    {
        "in_text": "What is another name for Tensor?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "Tensor"
    },
    {
        "in_text": "What type of storage does Casts this storage to bool type Casts this storage to char type Returns?Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to testTensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.get_rng_state Returns the random number generator state as a torch.ByteTensor.",
        "out_text": "a copy"
    },
    {
        "in_text": "If actual and expected are  complex-valued, they are considered close what?The published models should be at least in a branch/tag. It can\u2019t be a random commit.Moreover, as forgather(), the values ofindexmust beIfbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "if both their real and imaginary components are considered close according to the definition above"
    },
    {
        "in_text": "Below is the list of what for RHS indexing?soft_margin_loss SeeSoftMarginLossfor details.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwascallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "supported patterns"
    },
    {
        "in_text": "What does this method do if wait on any Future throws?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "throw an error"
    },
    {
        "in_text": "What is the value of the M[sparse_coo]@V[strided]->V[stridedtorch.matmul() no M[sparse_csr]@M[strided]->M[strided]Tensor.i0_ In-place version ofi0()Tensor.ldexp_ In-place version ofldexp()",
        "out_text": "no"
    },
    {
        "in_text": "What is In-place version ofacosh()?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "Tensor.acosh"
    },
    {
        "in_text": "What is the default value for return normalized STFT results?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.det Seetorch.det()",
        "out_text": "False"
    },
    {
        "in_text": "Profile_memory(bool) \u2013 what?torchtorchNote",
        "out_text": "track tensor memory allocation/deallocation"
    },
    {
        "in_text": "Installs what where none exist yet if they are subpaths of target?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "empty Modules"
    },
    {
        "in_text": "If actual and expected are  close, they are considered close if and they have the same device(if check_device is True), samecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifThe published models should be at least in a branch/tag. It can\u2019t be a random commit.When indices are not unique, the behavior is non-deterministic (one of the",
        "out_text": "real-valued and finite"
    },
    {
        "in_text": "What does tracing treat the numpy values as?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoftshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()",
        "out_text": "the constant node"
    },
    {
        "in_text": "GraphModule This function can be called at what level to register fn_or_name as a \"leaf function\"You can usetorch.manual_seed()to seed the RNG for all devices (bothThis is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf.Tensor.addbmm_ In-place version ofaddbmm()",
        "out_text": "module-level scope"
    },
    {
        "in_text": "What can empty container types do?Example Compute capabilities:Tensor.acos Seetorch.acos()Tensor.acos Seetorch.acos()",
        "out_text": "annotate their types"
    },
    {
        "in_text": "What does not exist at M[strided] -> M[hybrid sparse_coo] torch?torch.matmul() no M[sparse_csr]@M[strided]->M[strided]callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "M[sparse_coo]"
    },
    {
        "in_text": ":type relevant_args: iterable True: if any of the elements of relevant_args have __torchcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteYou can usetorch.manual_seed()to seed the RNG for all devices (bothWhen this flag is False (default) then some PyTorch warnings may only",
        "out_text": "if any of the elements of relevant_args have __torch_function__ implementations"
    },
    {
        "in_text": "What intentionally only supports computing the dot product of two 1D tensors with the same number of elements?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length TensorsTensor.isreal Seetorch.isreal()",
        "out_text": "torch.vdot"
    },
    {
        "in_text": "What is the name of a sparse COO tensor?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "out_text": "torch"
    },
    {
        "in_text": "What is tensor.half self.half() equivalent to?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.transpose Seetorch.transpose()condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "out_text": "self.to(torch.float16)"
    },
    {
        "in_text": "Tol(float,optional) \u2013 what for stopping criterion?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfupperisTrue, andAAAis a batch of symmetric positive-definiteIfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "out_text": "residual tolerance"
    },
    {
        "in_text": "If s is a sparse COO tensor and M = s.sparse_dim(), whatIfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same sizediagflat Ifinputis a vector (1-D tensor), then returns a 2-D square tensor",
        "out_text": "K"
    },
    {
        "in_text": "What does cProfile include when profiling CUDA code?cosine_embedding_loss SeeCosineEmbeddingLossfor details.softshrink Applies the soft shrinkage function element wisesoft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "CUDA startup time"
    },
    {
        "in_text": "A tensor can be constructed from a Python list or sequence using what constructor?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.If you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...from_numpy Creates a Tensor froma numpy.ndarray.",
        "out_text": "torch.tensor()"
    },
    {
        "in_text": "Which function returns a new tensor with the inverse hyperbolic sine of the elements of input?IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same sizelog2 Returns a new tensor with the logarithm to the base 2 of the elements of input.",
        "out_text": "Alias for torch.asin()"
    },
    {
        "in_text": "What does torch.as_tensor() do?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Warning"
    },
    {
        "in_text": "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy performed?The published models should be at least in a branch/tag. It can\u2019t be a random commit.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "asynchronously"
    },
    {
        "in_text": "in torch.mean What do you do ifdimis a list of dimensions over all of them?IfkeepdimisTrue, both thevaluesandindicestensorsBroadcastsinputto the shapeshape.nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "reduce"
    },
    {
        "in_text": "What controls the padding method used when center is True?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteEquation:The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "True pad_mode"
    },
    {
        "in_text": "What mode is used to export all operators as ATen ops?upsample Upsamples the input to either the givensizeor the givenscale_factorTensor.i0_ In-place version ofi0()Tensor.ndimension Alias fordim()",
        "out_text": "operator_export_type mode"
    },
    {
        "in_text": "What is the loss given an input tensorxxand a labels tensoryyy?IfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, andIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "MultiLabelMarginLoss"
    },
    {
        "in_text": "In what language are pickle_load_args used?Tensor.take_along_dim Seetorch.take_along_dim()Tensor.ndimension Alias fordim()Tensor.ndim Alias fordim()",
        "out_text": "Python 3"
    },
    {
        "in_text": "Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note\u2019fro\u2019 Frobenius norm \u2013upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "debug the transformation itself"
    },
    {
        "in_text": "What is the name of everything else inside the a torch.packagefile?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "User files"
    },
    {
        "in_text": "What does nansum treat Not a Numbers as?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "zero"
    },
    {
        "in_text": "How does PyTorch integrate with its autogradsystem?softshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "out_text": "Tightly integrated"
    },
    {
        "in_text": "What are selected  if atol is omitted?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "default values based on the dtype"
    },
    {
        "in_text": "What type of version is supported by most optimizers?softshrink Applies the soft shrinkage function element wiseSupport for Customization Limited Support Fully SupportedFeatures described in this documentation are classified by release status",
        "out_text": "simplified"
    },
    {
        "in_text": "What are ortho_fparams and ortho_bparams?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d"
    },
    {
        "in_text": "Sets the seed for generating random numbers. Returns the random number generator state as atorch.ByteTensor.Returns the random number generator state as a torch.ByteTensor.get_rng_state Returns the random number generator state as a torch.ByteTensor.Returns the random number generator state as atorch.ByteTensor. Returns the initial seed for generating random numbers as a",
        "out_text": "random number generator state"
    },
    {
        "in_text": "What does if None use when both start and end are real?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "global default dtype"
    },
    {
        "in_text": "What is the operator?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "aten"
    },
    {
        "in_text": "Can be passed as what in which case it will be called with the mismatching tensors and a namespace of diagnostic info?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "callable"
    },
    {
        "in_text": "What is the Futuretype primarily used by?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "theDistributed RPC Framework"
    },
    {
        "in_text": "What is linear_1 linear (add_1)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "call_module"
    },
    {
        "in_text": "What is returned from the Graph underlying this GraphModule?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningTensor.matmul Seetorch.matmul()",
        "out_text": "Python code"
    },
    {
        "in_text": "What type of example would be helpful to include?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "minimal working"
    },
    {
        "in_text": "If a mock is added with allow_empty=False, andclose()is called and the mock has not been matched toTensor.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True.Tensor.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True.Tensor.is_signed Returns True if the data type ofselfis a signed data type.",
        "out_text": "If allow_empty=True"
    },
    {
        "in_text": "What is the value of the function PReLU(x)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifIfupperisFalse, the returned matrixLis lower-triangular, and",
        "out_text": "0,x"
    },
    {
        "in_text": "How is the desired layout of returned Tensor with torch.ones?soft_margin_loss SeeSoftMarginLossfor details.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "with torch.layout"
    },
    {
        "in_text": "A new tensor is returned with what of the elements of input?log2 Returns a new tensor with the logarithm to the base 2 of the elements of input.IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "arcsine"
    },
    {
        "in_text": "What can a function equivalently be used as?nn.Transformer A transformer model.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "decorator"
    },
    {
        "in_text": "What is your real dependency?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "foo.bar"
    },
    {
        "in_text": "Performs what product of the matrix mat and the vector vec?Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,Computes the Cholesky decomposition of a symmetric positive-definite",
        "out_text": "matrix-vector product"
    },
    {
        "in_text": "What does a pattern contain?\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "one or more segments"
    },
    {
        "in_text": "What is the lowest L1-norm?\u2019fro\u2019 Frobenius norm \u2013callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "L1-norm"
    },
    {
        "in_text": "What does the sorted_sequence right returned index satisfies?Tensor.index_select Seetorch.index_select()Tensor.asinh_ In-place version ofasinh()Tensor.logical_or_ In-place version oflogical_or()",
        "out_text": "1-D False"
    },
    {
        "in_text": "Where is the ATen operator/function defined?Tensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()Tensor.vsplit Seetorch.vsplit()",
        "out_text": "VariableType.h"
    },
    {
        "in_text": "What is a 16-bit floating point1 torch?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "half torch"
    },
    {
        "in_text": "What is the name of the nn.Module?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "bool"
    },
    {
        "in_text": "What interacts with Multi-process data loading?Example Compute capabilities:Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "howIterableDataset"
    },
    {
        "in_text": "How to use torch.utils.data.distributed.DistributedSampler, give an example?Tensor.ndimension Alias fordim()cosine_embedding_loss SeeCosineEmbeddingLossfor details.Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": ">>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)"
    },
    {
        "in_text": "What library does PyTorch use to build the dataset and classify text?Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "torchtext library"
    },
    {
        "in_text": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes what?igammac Computes the regularized upper incomplete gamma function:igammac Computes the regularized upper incomplete gamma function:igamma Computes the regularized lower incomplete gamma function:",
        "out_text": "inverse ofrfftn()"
    },
    {
        "in_text": "What type of indices are selected?\u2019fro\u2019 Frobenius norm \u2013Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "1-dimensional indices"
    },
    {
        "in_text": "What does ignoringNaNvalues mean?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "the median of the values ininput"
    },
    {
        "in_text": "What does Checkpointing trade?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "compute for memory"
    },
    {
        "in_text": "When using anIterableDatasetwith what?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "multi-process data loading"
    },
    {
        "in_text": "Params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torTensor.i0_ In-place version ofi0()Tensor.asinh_ In-place version ofasinh()Tensor.cosh_ In-place version ofcosh()",
        "out_text": "local optimizer"
    },
    {
        "in_text": "What function sets whether to materialize output grad tensors?The published models should be at least in a branch/tag. It can\u2019t be a random commit.IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "function._ContextMethodMixin"
    },
    {
        "in_text": "Learning rate scheduling should be applied when?alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "after optimizer\u2019s update"
    },
    {
        "in_text": "Inputs are always first, then what else?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsEquation:",
        "out_text": "tensors"
    },
    {
        "in_text": "What is the given sequence of tensors?Tensor.isreal Seetorch.isreal()Tensor.frexp Seetorch.frexp()Tensor.kthvalue Seetorch.kthvalue()",
        "out_text": "Do cartesian product"
    },
    {
        "in_text": "What can TensorFloat-32 tensor cores be used in?softshrink Applies the soft shrinkage function element wiseTensor.ndimension Alias fordim()Tensor.acosh_ In-place version ofacosh()",
        "out_text": "matrix multiplications"
    },
    {
        "in_text": "What language can you define your models in?\u2019fro\u2019 Frobenius norm \u2013callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()",
        "out_text": "Python"
    },
    {
        "in_text": "What language has a range builtin?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()",
        "out_text": "Python"
    },
    {
        "in_text": "What Return the number of sparse dimensions in a sparse tensor self?condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifdist Returns the p-norm of (input-other)IfupperisTrue, the returned matrixUis upper-triangular, and",
        "out_text": "Tensor.sparse_dim"
    },
    {
        "in_text": "What is the name of the function that adds the scalar other to each element of the input?Tensor.isreal Seetorch.isreal()Tensor.ndimension Alias fordim()Tensor.frexp Seetorch.frexp()",
        "out_text": "Alias for torch.acosh()"
    },
    {
        "in_text": "Returns what of inputin the dimension dim?alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfinalphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "the cumulative product of elements"
    },
    {
        "in_text": "Which algorithm implements the resilient backpropagation algorithm?Tensor.det Seetorch.det()Tensor.ndimension Alias fordim()softshrink Applies the soft shrinkage function element wise",
        "out_text": "RMSprop algorithm"
    },
    {
        "in_text": "What does matrix_power return for a 2-D tensor?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "numerical rank"
    },
    {
        "in_text": "What does the network do?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "computes a loss"
    },
    {
        "in_text": "Resetting all.grads to None before each accumulation phase is a valid alternative to what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "model.zero_grad() or optimizer.zero_grad()"
    },
    {
        "in_text": "What does a call_function node represent?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.i0_ In-place version ofi0()",
        "out_text": "a call to a Python callable"
    },
    {
        "in_text": "Who executes graphs?IfkeepdimisTrue, both thevaluesandindicestensorsThe tensor autograd APIs and thetorchThe tensor autograd APIs and thetorch",
        "out_text": "backends/runtimes"
    },
    {
        "in_text": "What is another name for quantization aware training?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingandcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "static quantization"
    },
    {
        "in_text": "What does the code pretty-printer give an interpretation of the script method\u2019s code as valid?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Python syntax"
    },
    {
        "in_text": "What is the in-place version of abs() Tensor.absolute?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.logical_or_ In-place version oflogical_or()Tensor.ndimension Alias fordim()",
        "out_text": "abs() Tensor.abs"
    },
    {
        "in_text": "If map_location is a what, it will be used to remap location tags appearing in the file?torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.torch.use_deterministic_algorithms()lets you configure PyTorch to usetorch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "out_text": "dict"
    },
    {
        "in_text": "What does set_device set?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "current device"
    },
    {
        "in_text": "What is the dimension to do the operation over out(tuple,optional)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.std Seetorch.std()Tensor.log10 Seetorch.log10()",
        "out_text": "dim(int)"
    },
    {
        "in_text": "profile_memory (bool, optional) \u2013 what?Tensor.take_along_dim Seetorch.take_along_dim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "track tensor memory allocation/deallocation"
    },
    {
        "in_text": "What is sources a list of?Note\u2019fro\u2019 Frobenius norm \u2013torch",
        "out_text": "relative or absolute paths to C++ source files"
    },
    {
        "in_text": "By  torch.max, What is returned If there are multiple maximal values in a reduced row ?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.This is the second value returned bytorch.max(). See itscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "the indices of the first maximal value"
    },
    {
        "in_text": "Which context manager disables gradient calculation?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "Context-manager"
    },
    {
        "in_text": "For more information, see what?soft_margin_loss SeeSoftMarginLossfor details.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()",
        "out_text": "PyTorch Profiler TensorBoard"
    },
    {
        "in_text": "What type of dimensions are broadcasted?\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "non-matrix"
    },
    {
        "in_text": "What are the elements of input to  ake?Alias fortorch.trunc()Alias fortorch.trunc()IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "the given indices"
    },
    {
        "in_text": "Where are the unpruned units in the prune.RandomUnstructured Prune located?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "a tensor at random"
    },
    {
        "in_text": "What is the name of the constructor that describes PyTorch?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "sub_label"
    },
    {
        "in_text": "What does the add_image method require?Alias fortorch.trunc()Alias fortorch.trunc()softshrink Applies the soft shrinkage function element wise",
        "out_text": "pillow package"
    },
    {
        "in_text": "What do we use to find what goes wrong?Alias fortorch.trunc()Alias fortorch.trunc()However, there are some steps you can take to limit the number of sources of",
        "out_text": "a debugger"
    },
    {
        "in_text": "Returns a tensor filled with the scalar value 0, with what size as input?Writes all values from the tensorsrcintoselfat the indicesTensor.log10 Seetorch.log10()Tensor.sort Seetorch.sort()",
        "out_text": "same size"
    },
    {
        "in_text": "If the input is complex and neitherdtypenoroutis specified, what will be the corresponding floating point type?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Ifbetais 0, theninputwill be ignored, andnanandinfinIfbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "the result\u2019s data type"
    },
    {
        "in_text": "What is used to instantiate a model?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.matmul Seetorch.matmul()",
        "out_text": "*argsand**kwargsintorch.hub.load()"
    },
    {
        "in_text": "What is the output if return_complex is True?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "input.dim() + 2 dimensional real tensor"
    },
    {
        "in_text": "What is a tent?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Tensor.ceil Seetorch.ceil"
    },
    {
        "in_text": "What is a slightly overestimated rank of A?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "q"
    },
    {
        "in_text": "What does PyTorch not support with the layout signatureM[strided]@M[sparse_coo]?Tensor.cosh_ In-place version ofcosh()Tensor.i0_ In-place version ofi0()Tensor.tanh_ In-place version oftanh()",
        "out_text": "PyTorch does not support matrix multiplication"
    },
    {
        "in_text": "What is the function that converts parameters to one vector?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "Convert parameters to one vector"
    },
    {
        "in_text": "Fills the 3, 4, 5-dimensional inputTensorwith what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notereduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713Broadcastsinputto the shapeshape.",
        "out_text": "Dirac delta function"
    },
    {
        "in_text": "Python methods are implemented via what?softshrink Applies the soft shrinkage function element wiseTensor.cosh_ In-place version ofcosh()Tensor.i0_ In-place version ofi0()",
        "out_text": "C++-Python bindings"
    },
    {
        "in_text": "What does a Hermitian symmetricinputsignal do?Tensor.isreal Seetorch.isreal()igammac Computes the regularized upper incomplete gamma function:igammac Computes the regularized upper incomplete gamma function:",
        "out_text": "one dimensional discrete Fourier transform"
    },
    {
        "in_text": "What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "out_text": "IfkeepdimisTrue"
    },
    {
        "in_text": "What will be automatically regenerated when a graph is reassigned?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingandcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "code and forward"
    },
    {
        "in_text": "What do we do to each segment of a sequential model?Alias fortorch.trunc()Alias fortorch.trunc()IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "checkpoint"
    },
    {
        "in_text": "y = xAT + by= xAT+b. linear Applies a linear transformation to the incoming data:ylinear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+bsigmoid Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b",
        "out_text": "xAT+b"
    },
    {
        "in_text": "What module has lazy initialization?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d"
    },
    {
        "in_text": "if sourceis'local',repo_or_diris expected to be what?IfkeepdimisTrue, both thevaluesandindicestensorscallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notenn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "path to a local directory"
    },
    {
        "in_text": "What is the result object of this Future?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "object"
    },
    {
        "in_text": "What does torch.autograd.profiler.emit_nvtx() use?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience>>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)",
        "out_text": "nvprof"
    },
    {
        "in_text": "What computes the Jacobian of a given function?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorTensor.ndimension Alias fordim()",
        "out_text": "functional.jacobian Function"
    },
    {
        "in_text": "What type of structure does prune use?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "random"
    },
    {
        "in_text": "What is Tensor.sgn?Tensor.tril Seetorch.tril()Tensor.tril Seetorch.tril()Tensor.tril Seetorch.tril()",
        "out_text": "Seetorch.sgn"
    },
    {
        "in_text": "What is the latest version of Visual Studio?Alias fortorch.trunc()Alias fortorch.trunc()Alias for torch.acos().",
        "out_text": "Visual Studio 2019"
    },
    {
        "in_text": "Support for what type of support?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "Customization Limited Support Fully Supported"
    },
    {
        "in_text": "What is a list of modules that are consideredextern:class:Package Importer.externmodules?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length TensorsScriptModule A wrapper around C++torch::jit::Module.",
        "out_text": "module"
    },
    {
        "in_text": "The published models should be at least in a branch/tag. It can\u2019t be a what?The published models should be at least in a branch/tag. It can\u2019t be a random commit.callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "random commit"
    },
    {
        "in_text": "What can happen between CPU and GPU executions?A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "out_text": "results may not be reproducible"
    },
    {
        "in_text": "How can you use the forwardmethod?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "to ensure TorchScript (tracing or scripting) has captured your model code correctly"
    },
    {
        "in_text": "What is the criterion that measures the triplet loss given an input tensor?igammac Computes the regularized upper incomplete gamma function:igammac Computes the regularized upper incomplete gamma function:Returns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "nn.TripletMarginWithDistanceLoss"
    },
    {
        "in_text": "What types of inputs does this function support?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "float, double, cfloat and cdouble dtypes"
    },
    {
        "in_text": "What does a CUDA tensor require when the input dimension is larger than one?softshrink Applies the soft shrinkage function element wiseRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "grad"
    },
    {
        "in_text": "What are the input Tensor arguments?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.matmul Seetorch.matmul()",
        "out_text": "A,B,iK"
    },
    {
        "in_text": "What is a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional)Tensor.i0_ In-place version ofi0()Tensor.ldexp_ In-place version ofldexp()Tensor.baddbmm_ In-place version ofbaddbmm()",
        "out_text": "model(string)"
    },
    {
        "in_text": "What is used to group and display results for comparison?Completely reproducible results are not guaranteed across PyTorch releases,Completely reproducible results are not guaranteed across PyTorch releases,Completely reproducible results are not guaranteed across PyTorch releases,",
        "out_text": "theCompareclass"
    },
    {
        "in_text": "nn.MaxPool2d Applies a what kind of max pooling over an input signal?nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.nn.Tanh Applies the element-wise function:nn.ReLU6 Applies the element-wise function:",
        "out_text": "2D"
    },
    {
        "in_text": "If eigenvectors=False, it's an empty tensor. Otherwise, this tensor contains what?Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201dReturns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201dIfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse.",
        "out_text": "orthonormal eigenvectors of theinput"
    },
    {
        "in_text": "What is file that contains the name Alias for torch.linalg.pinv?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience>>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "Alias for torch.linalg.pinv"
    },
    {
        "in_text": "Relevant_args: Iterable or what to check for __torch_function__ methods?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteYou can usetorch.manual_seed()to seed the RNG for all devices (bothWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "aguments"
    },
    {
        "in_text": "When patterns are checked in the order that they were defined, what action will be taken?The published models should be at least in a branch/tag. It can\u2019t be a random commit.Completely reproducible results are not guaranteed across PyTorch releases,Completely reproducible results are not guaranteed across PyTorch releases,",
        "out_text": "first"
    },
    {
        "in_text": "What does Tensor.new_empty return a Tensor of size size filled with?Tensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()Tensor.i0_ In-place version ofi0()",
        "out_text": "uninitialized data"
    },
    {
        "in_text": "What does params(iterable) specify to be optimized?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notesoftshrink Applies the soft shrinkage function element wise",
        "out_text": "Tensors"
    },
    {
        "in_text": "What name denotes the name of this GraphModule for debugging purposes?Tensor.ndimension Alias fordim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "class_name"
    },
    {
        "in_text": "What operator supports Brain Floating Point?\u2019fro\u2019 Frobenius norm \u2013callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()",
        "out_text": "EmbeddingBag operator"
    },
    {
        "in_text": "GraphModule.to_folder() can be used to examine modules and parameters using what method?Tensor.i0_ In-place version ofi0()Tensor.cosh_ In-place version ofcosh()Tensor.ndimension Alias fordim()",
        "out_text": "to_folder"
    },
    {
        "in_text": "What will you be familiar with after completing this tutorial?Notetorchtorch",
        "out_text": "basic API"
    },
    {
        "in_text": "What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "same shape"
    },
    {
        "in_text": "What is target?Notetorchtorch",
        "out_text": "The fully-qualified string name of the new submodule"
    },
    {
        "in_text": "How much resources does the Ninja backend use on some systems?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "too many resources"
    },
    {
        "in_text": "What returns a new tensor with each of the elements of inputrounded to the closest integer?Tensor.ndimension Alias fordim()upsample Upsamples the input to either the givensizeor the givenscale_factorTensor.ndim Alias fordim()",
        "out_text": "round"
    },
    {
        "in_text": "What does Seetorch.nn.functional.hardshrink() do?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Boolean torch.bool torch.*.BoolTensor",
        "out_text": "Tensor.hardshrink"
    },
    {
        "in_text": "Random integers generated uniformly between low (inclusive) and high (exclusive) Returns a tensor with the same shape as inputfloor Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.round Returns a new tensor with each of the elements of inputrounded to the closest integer.ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.",
        "out_text": "exclusive"
    },
    {
        "in_text": "What could happen if a call to value() fails?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "Future may not yet hold a value"
    },
    {
        "in_text": "What is the data type of input if it is a complex data type?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "one of torch.complex64, and torch.complex128"
    },
    {
        "in_text": "What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.Tensor.negative_ In-place version ofnegative()Tensor.lgamma_ In-place version oflgamma()",
        "out_text": "Default:True"
    },
    {
        "in_text": "input(Tensor) \u2013 the input tensor n_fft(int) \u2013 the input tenupsample Upsamples the input to either the givensizeor the givenscale_factorTensor.negative_ In-place version ofnegative()Number \u2013 sum(abs(x)**ord)**(1./ord)",
        "out_text": "Fourier transform"
    },
    {
        "in_text": "Convert a tensor to compressed row storage format?64-bit complex torch.complex6464-bit complex torch.complex64split Splits the tensor into chunks.",
        "out_text": "Tensor"
    },
    {
        "in_text": "What is the name of the source of the github repo?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "source"
    },
    {
        "in_text": "Why are public functions that are publicly available in the torch API but cannot be overridden by__torch_function__?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningExample: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningWarning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "out_text": "none of the arguments of these functions are tensors or tensor-likes"
    },
    {
        "in_text": "What is cast to the LongTensor internally?Alias fortorch.trunc()Alias fortorch.trunc()IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "torch"
    },
    {
        "in_text": "A return value of what indicates that the target was not a valid reference to a submodule?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "False"
    },
    {
        "in_text": "What does i-1 mean?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "True boundaries"
    },
    {
        "in_text": "What is the default dimension along which to split the tensor?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, youFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you",
        "out_text": "0"
    },
    {
        "in_text": "What is the default value of the q(int,optional)?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.nn.ReLU6 Applies the element-wise function:nn.ReLU6 Applies the element-wise function:",
        "out_text": "By default,q=min(6,m,n)"
    },
    {
        "in_text": "How many non-zero 32-bit floating point numbers does a 10 000 x 10 000 tensor have?Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, youFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, youNote This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "out_text": "100 000"
    },
    {
        "in_text": "What is the name of all events in profiler.profile?soft_margin_loss SeeSoftMarginLossfor details.pinverse Alias for torch.linalg.pinv()pinverse Alias for torch.linalg.pinv()",
        "out_text": "total_average"
    },
    {
        "in_text": "Checkpointing currently only supports what function?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceYou can usetorch.manual_seed()to seed the RNG for all devices (bothsoftshrink Applies the soft shrinkage function element wise",
        "out_text": "torch.autograd.backward()"
    },
    {
        "in_text": "To make FX Graph Mode Quantization work, users might need to familiarize themselves with what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteIfkeepdimisTrue, both thevaluesandindicestensorsThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "torch.fx"
    },
    {
        "in_text": "What are the pivots of?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "size"
    },
    {
        "in_text": "How to use Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present., give an example?Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:Returns a new tensor with the hyperbolic sine of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninput is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": ">>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n      col_indices=tensor([0, 1, 0, 1]),\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n      dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)"
    },
    {
        "in_text": "What Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently AskedTensor.i0_ In-place version ofi0()Tensor.asinh_ In-place version ofasinh()Tensor.tanh_ In-place version oftanh()",
        "out_text": "Python Functions and Modules"
    },
    {
        "in_text": "What returns a tensor with the same size asinput?IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same sizeReturns a tensor of the same size asinputwith each element",
        "out_text": "random permutation of integers from0ton-1"
    },
    {
        "in_text": "What are scaling factors on matrix-vector product betweenmat1 andmat2?Tensor.kthvalue Seetorch.kthvalue()Tensor.triangular_solve Seetorch.triangular_solve()Tensor.isreal Seetorch.isreal()",
        "out_text": "alphaandbetaare scaling factors"
    },
    {
        "in_text": "What is the mutually exclusive feature of batch_sampler?Alias fortorch.trunc()Alias fortorch.trunc()absolute Alias for torch.abs()",
        "out_text": "withbatch_size,shuffle,sampler, anddrop_last"
    },
    {
        "in_text": "How many consolidated state_dict lists are updated per rank?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingandcosine_embedding_loss SeeCosineEmbeddingLossfor details.",
        "out_text": "one per rank"
    },
    {
        "in_text": "What do you want to specify when you only want to vary a single option, while keeping all others consistent between parameter groups?However, there are some steps you can take to limit the number of sources ofHowever, there are some steps you can take to limit the number of sources ofcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "per-layer learning rates"
    },
    {
        "in_text": "What is a module or function to be traced and converted into?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "Graph representation"
    },
    {
        "in_text": "What Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensReturns a tensor of the same size asinputwith each elementTensor.dense_dim Return the number of dense dimensions in asparse tensorself.floor Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "out_text": "dim"
    },
    {
        "in_text": "In the symbolic function, if the operator is already standardized in what, we only need to create a node to represent the ONNXSeetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingIf you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "out_text": "ONNX"
    },
    {
        "in_text": "What does Tensor.resize_ do?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Tensor.resize_ Resizesselftensor to the specified size"
    },
    {
        "in_text": "If input is complex and neitherdtypenoroutis specified, the result's data type will be what?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingis_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.",
        "out_text": "corresponding floating point type"
    },
    {
        "in_text": "Is the Tensor a meta tensor?Tensor.isreal Seetorch.isreal()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.det Seetorch.det()",
        "out_text": "True"
    },
    {
        "in_text": "What is the index location of each element found?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndimension Alias fordim()",
        "out_text": "Andindices"
    },
    {
        "in_text": "What is a tensor to specified GPU devices?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Tensor.vdot Seetorch.vdot()",
        "out_text": "comm.broadcast Broadcasts"
    },
    {
        "in_text": "Why is the Variable API deprecated?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. WarningIf the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warningtorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "Variables are no longer necessary to use autograd with tensors"
    },
    {
        "in_text": "What computes the zeroth order modified Bessel function of the first kind for each element of input?igammac Computes the regularized upper incomplete gamma function:igammac Computes the regularized upper incomplete gamma function:igamma Computes the regularized lower incomplete gamma function:",
        "out_text": "i0"
    },
    {
        "in_text": "What is the name of the device where the Tensor is stored?Alias fortorch.trunc()Alias fortorch.trunc()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "torch.device"
    },
    {
        "in_text": "Where are specified values in the a sparse tensor?Tensor.log10 Seetorch.log10()Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "givencrow_indicesandcol_indices"
    },
    {
        "in_text": "What are the values specified by nan_to_num?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "bynan,posinf, andneginf"
    },
    {
        "in_text": "What is glu?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.ne().",
        "out_text": "gated linear unit"
    },
    {
        "in_text": "What is the N dimensional inverse discrete Fourier transform ofinput?Tensor.ndimension Alias fordim()softshrink Applies the soft shrinkage function element wiseTensor.ndim Alias fordim()",
        "out_text": "Computes the N dimensional discrete Fourier transform"
    },
    {
        "in_text": "Registers what?Alias fortorch.trunc()Alias fortorch.trunc()IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "an extern hook on the exporter"
    },
    {
        "in_text": "What does Aboolthat return?IfkeepdimisTrue, both thevaluesandindicestensorsBroadcastsinputto the shapeshape.soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "whether PyTorch is built with MKL support"
    },
    {
        "in_text": "What does inputin the dimensiondim return?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "the cumulative product of elements"
    },
    {
        "in_text": "PyTorch implements what format for sparse tensors?The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminismnn.utils.rnn.pack_sequence Packs a list of variable length Tensors",
        "out_text": "Coordinate format"
    },
    {
        "in_text": "What is an example of a list of tuples of inputs that will be used to re-trace the computation and verify theReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "example"
    },
    {
        "in_text": "What kind of view does the tensor class provide?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "multi-dimensional"
    },
    {
        "in_text": "What does the Tensor.new_empty return a Tensor of size size filled with?nn.utils.rnn.pack_sequence Packs a list of variable length Tensorsnn.utils.rnn.pack_sequence Packs a list of variable length Tensorsrepeat_interleave Repeat elements of a tensor.",
        "out_text": "uninitialized data"
    },
    {
        "in_text": "Where are most modules fromtorch.nn supported?soft_margin_loss SeeSoftMarginLossfor details.Alias fortorch.special.expit().Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "TorchScript"
    },
    {
        "in_text": "How can data-dependent control flow be captured?Example Compute capabilities:softshrink Applies the soft shrinkage function element wiseAlias fortorch.trunc()",
        "out_text": "usingtorch.jit.script()"
    },
    {
        "in_text": "What can a scripted function call?\u2019fro\u2019 Frobenius norm \u2013Notetorch",
        "out_text": "an encoder module generated using tracing"
    },
    {
        "in_text": "What does the __init__ method create?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "SummaryWriter"
    },
    {
        "in_text": "What is it called to have code that behaves differently depending on whether it's packaged or not?Alias fortorch.trunc()Alias fortorch.trunc()Random sampling creation ops are listed underRandom samplingand",
        "out_text": "it\u2019s bad practice"
    },
    {
        "in_text": "When should you set the environment variableCUBLAS_WORKSPACE_CONFIG?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.ndim Alias fordim()",
        "out_text": "if you are using CUDA tensors"
    },
    {
        "in_text": "What is the case when the inputs are promoted to the default scalar type?The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningreduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "out_text": "if bothinputandotherare integer types"
    },
    {
        "in_text": "Is True if the Tensor is a what?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "out_text": "meta tensor"
    },
    {
        "in_text": "What do you need to distinguish between?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "packaged code and non-packaged code"
    },
    {
        "in_text": "What is the term for multi_margin_loss(input, target, p=1, margin=1, weight=None,Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')",
        "out_text": "multi_margin_loss"
    },
    {
        "in_text": "What does Tensor.log10_ In-place version oflog10() do?Tensor.log10_ In-place version oflog10()Tensor.ldexp_ In-place version ofldexp()Tensor.baddbmm_ In-place version ofbaddbmm()",
        "out_text": "Tensor.log10_ In-place version oflog10()"
    },
    {
        "in_text": "What is the tensor?\u2019fro\u2019 Frobenius norm \u2013NoteAlias fortorch.trunc()",
        "out_text": "3-D"
    },
    {
        "in_text": "Enabling shape and stack tracing results in what?reduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "out_text": "additional overhead"
    },
    {
        "in_text": "How to use torch.utils.cpp_extension.load, give an example?Tensor.addbmm_ In-place version ofaddbmm()Tensor.ldexp_ In-place version ofldexp()scatter_add Out-of-place version of torch.Tensor.scatter_add_()",
        "out_text": ">>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)"
    },
    {
        "in_text": "What is the input with the following cases?Alias fortorch.trunc()Alias fortorch.trunc()\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "* log1p(other)"
    },
    {
        "in_text": "What is the name of the file to save stacks file to?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "path(str)"
    },
    {
        "in_text": "What is a wrapper around?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "C++torch::jit::Module"
    },
    {
        "in_text": "If split_size_or_sections is a list, tensor will be split into what chunks?torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.Tensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True.repeat_interleave Repeat elements of a tensor.",
        "out_text": "len"
    },
    {
        "in_text": "What is the result of the element-wise multiplication of tensor1 by tensor2?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.IfkeepdimisTrue, the output tensor is of the same sizeIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "scalar value"
    },
    {
        "in_text": "What does the element-wise division oftensor1bytensor2 perform?torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.The published models should be at least in a branch/tag. It can\u2019t be a random commit.If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "out_text": "the element-wise multiplication"
    },
    {
        "in_text": "What is Seetorch.conj?Broadcastsinputto the shapeshape.The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warningnn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "out_text": "Tensor.conj"
    },
    {
        "in_text": "What returns the tensor as a (nested) list?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()upsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "Tensor.tolist"
    },
    {
        "in_text": "repo_or_dir(string) \u2013 what does repo_owner/repo_name[:tag_name] mean?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.i0_ In-place version ofi0()Tensor.cumsum_ In-place version ofcumsum()",
        "out_text": "repo name"
    },
    {
        "in_text": "Who executes the graphs?\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "backends/runtimes"
    },
    {
        "in_text": "What fills each location ofselfwith an independent sample fromBernoulli(p)textBernoulli(callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.i0_ In-place version ofi0()IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "Tensor.bernoulli"
    },
    {
        "in_text": "What is the default value for sorted_sequence[m][n]?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThis is the second value returned bytorch.max(). See itsTensor.ndimension Alias fordim()",
        "out_text": "False"
    },
    {
        "in_text": "If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels thatWarning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwasYou can usetorch.manual_seed()to seed the RNG for all devices (bothThe published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "out_text": "tensors"
    },
    {
        "in_text": "What should you do with your imports?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "Qualify"
    },
    {
        "in_text": "If True(default) asserts that which tensors have the same stride?condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (iftorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.t Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.",
        "out_text": "check_stride asserts corresponding tensors have the same stride"
    },
    {
        "in_text": "What is the name of the error message that can be used if the values of corresponding tensors mismatch?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThe published models should be at least in a branch/tag. It can\u2019t be a random commit.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "See below for details"
    },
    {
        "in_text": "How can we disable JIT?Alias fortorch.trunc()Alias fortorch.trunc()Note",
        "out_text": "globally disable JIT"
    },
    {
        "in_text": "How are tensors modified in function._ContextMethodMixin.mark_dirty?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.i0_ In-place version ofi0()scatter_add Out-of-place version of torch.Tensor.scatter_add_()",
        "out_text": "in-place operation"
    },
    {
        "in_text": "What does each element of the input Tensor have?Tensor.ndimension Alias fordim()Alias fortorch.trunc()Alias fortorch.trunc()",
        "out_text": "Thresholds"
    },
    {
        "in_text": "What is the safest option for finding the CUDA install directory?soft_margin_loss SeeSoftMarginLossfor details.Tensor.ndimension Alias fordim()Alias fortorch.trunc()",
        "out_text": "setting the CUDA_HOME environment variable"
    },
    {
        "in_text": "What is the name of the given tensor?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndimension Alias fordim()Tensor.take_along_dim Seetorch.take_along_dim()",
        "out_text": "Compute combinations of length rrr"
    },
    {
        "in_text": "How is each element of the tensor other multiplied?Returns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute theReturns the numerical rank of a 2-D tensor. The method to compute the",
        "out_text": "multiplied by the scalar alpha and added to each element of the tensor input"
    },
    {
        "in_text": "What returns the Python code generated from the Graph underlying this GraphModule?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteThis is the second value returned bytorch.max(). See itsIf you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "out_text": "Return the Python code generated from the Graph underlying this GraphModule"
    },
    {
        "in_text": "What sets the random number generator state of the specified GPU?Alias fortorch.trunc()Alias fortorch.trunc()Tensor.det Seetorch.det()",
        "out_text": "set_rng_state"
    },
    {
        "in_text": "What product of vectors vec1 and vec2 is added to the matrix input?upsample Upsamples the input to either the givensizeor the givenscale_factorcallback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notecondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "out_text": "outer-product"
    },
    {
        "in_text": "Out (Tensor, optional) - what is the name of the output tensor?If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.Tensor.addmm Seetorch.addmm()torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.",
        "out_text": "output tensor"
    },
    {
        "in_text": "What is Tensor.roll Seetorch.roll()?Tensor.ne Seetorch.ne().Tensor.ge Seetorch.ge().Tensor.tanh Seetorch.tanh()",
        "out_text": "Tensor.roll Seetorch.roll()"
    },
    {
        "in_text": "When an exporter adds what to a package, it can optionally scan it for further code dependencies?A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g. exporter.mock(\"**\", exclude=[\"torchvision.**\"])We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specializedWe can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "out_text": "source code"
    },
    {
        "in_text": "What is the name of the exporter that exports a pretrained AlexNet into ONNX?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "ONNX"
    },
    {
        "in_text": "What is the corresponding args for callablemodel?Tensor.ndimension Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factor",
        "out_text": "source(string,optional)"
    },
    {
        "in_text": "A (Tensor) is what to factor of size (,m,n)(*, m, n)(log2 Returns a new tensor with the logarithm to the base 2 of the elements of input.Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingIfkeepdimisTrue, the output tensor is of the same size",
        "out_text": "tensor"
    },
    {
        "in_text": "What are the members of the profiler?Note\u2019fro\u2019 Frobenius norm \u2013Alias fortorch.trunc()",
        "out_text": "CPU CUDA"
    },
    {
        "in_text": "What is at the given URL to a local path?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteTensor.ndim Alias fordim()Tensor.ndimension Alias fordim()",
        "out_text": "Example Download object"
    },
    {
        "in_text": "What is the default torch.Tensor type?Alias fortorch.trunc()Alias fortorch.trunc()Alias fortorch.special.expit().",
        "out_text": "floating point tensor type t"
    },
    {
        "in_text": "What is a member of the profiler that can be taken at the specified intervals?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "CPU CUDA"
    },
    {
        "in_text": "What is the tensor to be added tensor1 (Tensor)?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Notetorch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention.Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "out_text": "input"
    },
    {
        "in_text": "What is the defaultStream for a given device?upsample Upsamples the input to either the givensizeor the givenscale_factordevices(iterable of CUDA IDs) \u2013 CUDA devices for which to forkdevices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "out_text": "defaultStreamfor a given device"
    },
    {
        "in_text": "What portion of the matrix is used by default?Tensor.ndim Alias fordim()callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteAlias fortorch.trunc()",
        "out_text": "upper triangular portion"
    },
    {
        "in_text": "What function returns the local_state_dict for a given rank?upsample Upsamples the input to either the givensizeor the givenscale_factorTensor.ndimension Alias fordim()softshrink Applies the soft shrinkage function element wise",
        "out_text": "insidestep()"
    },
    {
        "in_text": "What are we actively looking for feedback for?Alias fortorch.trunc()Alias fortorch.trunc()soft_margin_loss SeeSoftMarginLossfor details.",
        "out_text": "UI/UX improvements or missing functionalities"
    },
    {
        "in_text": "What is the output of torch.Tensor.coalesce()method?callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteupsample Upsamples the input to either the givensizeor the givenscale_factorcondition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "out_text": "a sparse tensor"
    }
]