[
    {
        "in_text": "What can be called in a traced function?Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "script function"
    },
    {
        "in_text": "What does bool Return?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Graph"
    },
    {
        "in_text": "What does the PyTorch Timer do when necessary?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "synchronize asynchronous CUDA functions"
    },
    {
        "in_text": "If the input argument is a tensor, but ONNX asks for what?Tensor.isreal Seetorch.isreal()is_tensor Returns True if obj is  a PyTorch tensor.Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "out_text": "scalar"
    },
    {
        "in_text": "What is the default mode for ONNX operators?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "operator_export_type mode"
    },
    {
        "in_text": "What is the torch.index_add() called on?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What can't be published models?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a random commit"
    },
    {
        "in_text": "What does TorchScript provide a code pretty-printer for allScriptModuleinstances?verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "out_text": "Python syntax"
    },
    {
        "in_text": "What is a string e.g. \"my_package.my_subpackage\" or list of strings for the names of theConstruct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')",
        "out_text": "include"
    },
    {
        "in_text": "Adds what to each element of the input input and returns a new resulting tensor?Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.Returns a new tensor with a dimension of size one inserted at theMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "out_text": "scalar"
    },
    {
        "in_text": "Activities(iterable) \u2013 what to use in profiling?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "list of activity groups"
    },
    {
        "in_text": "If theFuture's value contains what that reside on GPUs, the callback might be invoked while the async kernelsTensor.split Seetorch.split()Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "out_text": "tensors"
    },
    {
        "in_text": "What is slogdet Alias?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "for torch.linalg.slogdet"
    },
    {
        "in_text": "What should the Inverse short time Fourier Transform return?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "least squares estimation"
    },
    {
        "in_text": "AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes?Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes.lp_pool1d Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "out_text": "nn"
    },
    {
        "in_text": "What does PyTorch preserve across serialization?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "storage sharing"
    },
    {
        "in_text": "What did we try to check with the == equality operator?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "equality of the values of two deep learning models"
    },
    {
        "in_text": "What does Merge extrapolate times tonumber_per_run=1 and not transfer any metadata?Q: Does ONNX support implicit scalar datatype casting?Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "Approximate significant figure estimate"
    },
    {
        "in_text": "What is placed in the.data/?Alias fortorch.linalg.pinv()This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "Framework files"
    },
    {
        "in_text": "What does torch.conj() do?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dThis module is in BETA. New functions are still being added, and someReturns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "out_text": "Computes the element-wise conjugate"
    },
    {
        "in_text": "What creates a criterion that measures the mean squared error between each element in the inputxxxand targetyyy?timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy.optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "out_text": "nn.SmoothL1Loss"
    },
    {
        "in_text": "C++ tensor indexing API looks and behaves the same as what?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "Python API"
    },
    {
        "in_text": "Root can either be an nn.Module instance or a Dict mapping strings to any attribute type?is_available Returns a bool indicating if CUDA is currently available.ignore This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "nn.Module instance or a Dict mapping strings to any attribute type"
    },
    {
        "in_text": "What does local_response_norm apply over an input signal composed of several input planes?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "local response normalization"
    },
    {
        "in_text": "What are computed if the boolean argumenteigenvectors is False?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "eigenvalues"
    },
    {
        "in_text": "Supports broadcasting to a common shape, type promotion, and what other inputs?sub Subtracts other, scaled byalpha, from input.sub Subtracts other, scaled byalpha, from input.sub Subtracts other, scaled byalpha, from input.",
        "out_text": "integer and floating-point inputs"
    },
    {
        "in_text": "What is the name of the extension that is loaded into the process as a plain dynamic library?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "is_standalone"
    },
    {
        "in_text": "Tracker(callable,optional) \u2013 a function for what?You can usetorch.manual_seed()to seed the RNG for all devices (bothTo package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "out_text": "tracing the iteration process"
    },
    {
        "in_text": "What is Seetorch.matmul?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.matmul"
    },
    {
        "in_text": "What is quant_min in torch.fake_quantize_per_tensor_affine?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "the lower bound of quantized domain"
    },
    {
        "in_text": "What is pad?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "Pads tensor"
    },
    {
        "in_text": "What can you use TorchScript's compilation of the code for theforwardmethod?Tensor.arccos_ In-place version ofarccos()Tensor.arccos_ In-place version ofarccos()Tensor.frac_ In-place version offrac()",
        "out_text": "to ensure TorchScript (tracing or scripting) has captured your model code correctly"
    },
    {
        "in_text": "If both elements are what is NaN propagated?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "out_text": NaN
    },
    {
        "in_text": "What only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "the broadcasting logic"
    },
    {
        "in_text": "What operation in kTkHkWkT times kWkTkHkW regions by step sizemultilabel_margin_loss SeeMultiLabelMarginLossfor details.>>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])triplet_margin_loss SeeTripletMarginLossfor details",
        "out_text": "3D average-pooling"
    },
    {
        "in_text": "What is In-place version of leaky_relu()?Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "leaky_relu"
    },
    {
        "in_text": "What default value is used to load the Torch serialized object at the given URL?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Alias for torch.linalg.matrix_power()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "True"
    },
    {
        "in_text": "The tensor autograd APIs and thetorchReturns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "out_text": "dynamic neural networks"
    },
    {
        "in_text": "What does the checkpointed version of the checkpointed version do?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Warning"
    },
    {
        "in_text": "What does root convert into?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Graph representation"
    },
    {
        "in_text": "What action is allowed to stub out a module?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "mock"
    },
    {
        "in_text": "How to use torch.square, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])"
    },
    {
        "in_text": "What do QuantStub and DeQuantStub modules specify?xlogy Computesinput*log(other)with the following cases.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()",
        "out_text": "where activations are quantized and de-quantized"
    },
    {
        "in_text": "OneCycleLR Sets the learning rate of each parameter group according to what policy?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "1cycle learning rate policy"
    },
    {
        "in_text": "In-place version of elu(). Applies what?Tensor.isneginf Seetorch.isneginf()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')",
        "out_text": "element-wise"
    },
    {
        "in_text": "What is implemented using the new parametrization functionality intorch.nn.utils.parameterize.register_parametrization()Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:Generates uniformly distributed random samples from the half-open interval\n[low, high). >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "out_text": "Parametrizations"
    },
    {
        "in_text": "What is an example of a double wildcard?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "out_text": "torch"
    },
    {
        "in_text": "What cannot be guaranteed when multiple callbacks are added to the sameFuture?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "the order in which they will be executed"
    },
    {
        "in_text": "What is the tuple of two output tensors?TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.Tensor.logical_xor Seetorch.logical_xor()ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "out_text": "out"
    },
    {
        "in_text": "Returns what value, ignoring NaN values?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "the median of the values ininput"
    },
    {
        "in_text": "What type of non-linearity does an Elman RNN cell have?Q: Does ONNX support implicit scalar datatype casting?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "tanh or ReLU non-linearity"
    },
    {
        "in_text": "What is the single weight shared among input channels not supported?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "pixel_shuffle pow prelu"
    },
    {
        "in_text": "What performs a batch matrix -matrix  product of matrices stored in batch1 and batch2?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "addbmm"
    },
    {
        "in_text": "What beta is Quantization currently in?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "beta"
    },
    {
        "in_text": "What is the same shape as Tensorinputfilled with random integers generated?Q: Does ONNX support implicit scalar datatype casting?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources of",
        "out_text": "uniformly"
    },
    {
        "in_text": "What does TheKullback-Leibler divergence Loss Function do?2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.gradient This function is analogous to NumPy\u2019s gradient function.Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "out_text": "SeeMarginRankingLossfor details"
    },
    {
        "in_text": "What is preferable to use instead of write the package to the filesystem?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "resource guard syntax"
    },
    {
        "in_text": "sorted_sequence(Tensor) \u2013 N-D or what tensor?Tensor.logical_or_ In-place version oflogical_or()Tensor.arctanh Seetorch.arctanh()Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "1-D"
    },
    {
        "in_text": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes theThis module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.>>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "out_text": "inverse ofrfftn()"
    },
    {
        "in_text": "How to use torch.utils.benchmark.CallgrindStats.as_standardized, give an example?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoullitorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)"
    },
    {
        "in_text": "What does this mean?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it"
    },
    {
        "in_text": "What setting is different from thetorch.backends.cudnn.deterministicsetting discussed below?>>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?multilabel_margin_loss SeeMultiLabelMarginLossfor details.",
        "out_text": "withtorch.backends.cudnn.benchmark=True"
    },
    {
        "in_text": "What can you export the model as in ONNX?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "a custom op"
    },
    {
        "in_text": "Where are many operations for quantized tensors available under the same API as full float version?The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. NoteQ: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "out_text": "torch"
    },
    {
        "in_text": "What does Is_storage Return?Alias for torch.linalg.matrix_power()Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliNo, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "True or False"
    },
    {
        "in_text": "How to use torch.angle, give an example?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?multilabel_margin_loss SeeMultiLabelMarginLossfor details.Alias for torch.linalg.matrix_power()",
        "out_text": ">>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])"
    },
    {
        "in_text": "What is the name of the Sparse grad at V[strided] -> V[strided] torch?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "M[sparse_coo]"
    },
    {
        "in_text": "How to use torch.nn.init.uniform_, give an example?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": ">>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)"
    },
    {
        "in_text": "What is 8-bit integer (signed) torch?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "*.CharTensor"
    },
    {
        "in_text": "If a sequence of importers are passed, what will be constructed out of them?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "an Ordered Importer"
    },
    {
        "in_text": "What is the size ofinput that determines the size of the output tensor?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "input(Tensor)"
    },
    {
        "in_text": "How to use torch.load, give an example?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": ">>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')"
    },
    {
        "in_text": "in torch.utils.model_zoo.load_url where the object should already be  present ,when it's deserialized and returned?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "out_text": "model_dir"
    },
    {
        "in_text": "Which type of dataset can not detect such cases in general?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "PyTorch"
    },
    {
        "in_text": "If \"relaxed\", complex values are considered as NaN  what?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "if either the real or imaginary component is NaN"
    },
    {
        "in_text": "What is the equivalent of Tensor.float self.float()?public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. NoteTensor.double self.double()is equivalent toself.to(torch.float64).Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "self.to(torch.float32)"
    },
    {
        "in_text": "What is the Tensor true if it is a meta tensor?Tensor.multiply_ In-place version ofmultiply().Tensor.xlogy Seetorch.xlogy()Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "meta tensor"
    },
    {
        "in_text": "The resultingouttensor shares its underlying storage with what?timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedReturns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "out_text": "theinputtensor"
    },
    {
        "in_text": "What happens when memory is shared between all processes?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "All changes are written to the file"
    },
    {
        "in_text": "What utation perm could be reconstructed by applying swap(perm[i], perm[pivots[i]Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "perm"
    },
    {
        "in_text": "What is movedim Seetorch.movedim()?Alias fortorch.linalg.pinv()Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Tensor"
    },
    {
        "in_text": "What must be part of graph inputs for ONNX opset version  9?Alias fortorch.linalg.pinv()Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "initializers"
    },
    {
        "in_text": "What does the Global Hooks For Module register for all the modules?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "global forward hook"
    },
    {
        "in_text": "Abstract base class for creation of new what?If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "pruning techniques"
    },
    {
        "in_text": "What does Seetorch.rad2deg do?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "Tensor.rad2deg"
    },
    {
        "in_text": "What should mock be used as?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "last resort"
    },
    {
        "in_text": "Returns the maximum value of all elements in the input tensor. Returns the minimum value of all elements in what tensor?extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.sign Returns a new tensor with the signs of the elements of input.Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Note The domain of the inverse hyperbolic tangent is(-1, 1)and values outside this range\nwill be mapped toNaN, except for the values1and-1for which the output is\nmapped to+/-INFrespectively. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "out_text": "input tensor"
    },
    {
        "in_text": "What can be wrapped in a torch.fx.wrap function?hardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "torch.randn"
    },
    {
        "in_text": "What is a custom version of PyTorch'sLinearmodule called?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Advanced Features"
    },
    {
        "in_text": "What is the tensor to compare other(Tensororfloat)?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "input(Tensor)"
    },
    {
        "in_text": "What does it do when a tensor input is not specified?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "Counts the number of non-zero values in the tensor input along the given dim"
    },
    {
        "in_text": "What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareNtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.You can usetorch.manual_seed()to seed the RNG for all devices (bothThe error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "out_text": "automatic batching"
    },
    {
        "in_text": "What has been initialized?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "PyTorch\u2019s CUDA state"
    },
    {
        "in_text": "What must the imag(Tensor) be?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "same dtype asreal"
    },
    {
        "in_text": "What is returned by setting the default torch.Tensortype to floating point tensor typet?no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() noYes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "the total number of elements in the input tensor"
    },
    {
        "in_text": "Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuuIf a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "out_text": "matrix-free LOBPCG methods"
    },
    {
        "in_text": "What type of initialization of thein_channelsargument of theConvTranspose1d that is inferred from theinBecause FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "lazy"
    },
    {
        "in_text": "What is Tensor.split?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "Seetorch.split"
    },
    {
        "in_text": "Who applies the Sigmoid Linear Unit (SiLU) function, element-wise?Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported.Q: Does ONNX support implicit scalar datatype casting?functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is created with strides matching param (thus matching param\u2019s layout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grads to None before each accumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks.",
        "out_text": "nn.Mish"
    },
    {
        "in_text": "How to use torch.tile, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": ">>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])"
    },
    {
        "in_text": "What is the index inself specified by?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim"
    },
    {
        "in_text": "If win_length is None, it is treated as equal to what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "n_fft"
    },
    {
        "in_text": "Add what to self tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "scalar or tensor"
    },
    {
        "in_text": "What Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "movedim"
    },
    {
        "in_text": "What is the learning rate of each parameter group according to cyclical learning rate policy?std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the condition",
        "out_text": "OneCycleLR"
    },
    {
        "in_text": "How to use torch.addcdiv, give an example?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])"
    },
    {
        "in_text": "What is the name of the matrix with a geometric progression in each row?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Alexandre-Theophile Vandermonde"
    },
    {
        "in_text": "What is the purpose of the current value of the running mean?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "it will be restored when loading a serialized form of the module"
    },
    {
        "in_text": "Adamax Implements is a variant of Adam based on what?Alias for torch.mul().Alias for torch.mul().torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "infinity norm"
    },
    {
        "in_text": "What sometimes don\u2019t contain a__module___?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Methods/properties"
    },
    {
        "in_text": "What is the name of the function that returns a copy of the object in CUDA memory?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "non_blocking(bool)"
    },
    {
        "in_text": "What do you want to generate names from?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "languages"
    },
    {
        "in_text": "What type of description is used to disambiguate measurements?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "description"
    },
    {
        "in_text": "How to use torch.distributed.broadcast_object_list, give an example?Tensor.pow_ In-place version ofpow()Linux (ppc64le) GPU \u2014  \u2014Tensor.neg_ In-place version ofneg()",
        "out_text": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]"
    },
    {
        "in_text": "What type of torch is.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torchnonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examplespivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor. Default: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)). pivots stores all the intermediate transpositions of rows. The final permutation perm could be reconstructed by applying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1, where perm is initially the identity permutation of mmm elements (essentially this is what torch.lu_unpack() is doing).PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:",
        "out_text": "128-bit complex torch"
    },
    {
        "in_text": "What are hybrid tensors?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values"
    },
    {
        "in_text": "When is_storage return True?Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Tensor.logical_xor_ In-place version oflogical_xor()",
        "out_text": "if the  object is a PyTorch storage object."
    },
    {
        "in_text": "What does NOT match what is in VariableType.h?Tensor.arctanh Seetorch.arctanh()Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "Parameter ordering"
    },
    {
        "in_text": "What is symbolic tracing also known as?Tensor.xlogy Seetorch.xlogy()Tensor.multiply_ In-place version ofmultiply().Alias fortorch.linalg.pinv()",
        "out_text": "symbolic execution"
    },
    {
        "in_text": "What do students learn how to generate names from?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "languages"
    },
    {
        "in_text": "Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) NoteTensor.absolute Alias forabs()Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "indexing"
    },
    {
        "in_text": "The last value of a tuple consisting of named parameters and the corresponding inputs are structured as what?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. NoteWithout the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "out_text": "key-value pairs"
    },
    {
        "in_text": "What order are the eigenvalues in?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "ascending"
    },
    {
        "in_text": "What does a new tensor do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.vander Generates a Vandermonde matrix .",
        "out_text": "add"
    },
    {
        "in_text": "What is the tensor to split indices_or_sections?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "n"
    },
    {
        "in_text": "What is placed to prevent this from being an issue?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "constraint"
    },
    {
        "in_text": "A dictionary that maps namespaces that contain what to functions in that namespace that can be overridden?Note Wheninputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically returnindicesfor any of them. input(Tensor) \u2013 the input tensor. k(int) \u2013 k for the k-th smallest element dim(int,optional) \u2013 the dimension to find the kth value along keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor)If you need manual control over .grad\u2019s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True.When called with dims of the list form, the given dimensions will be contracted\nin place of the last ddd of a and the first ddd of bbb. The sizes\nin these dimensions must match, but tensordot() will deal with broadcasted\ndimensions. >>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])",
        "out_text": "overridable functions"
    },
    {
        "in_text": "What applies a multi-layer gated recurrent unit to an input sequence?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "nn.GRU"
    },
    {
        "in_text": "What checks if tensor is in shared memory?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Tensor.is_shared"
    },
    {
        "in_text": "What type of files are placed in the.data/. User files?Alias for torch.linalg.matrix_power()>>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "Framework files"
    },
    {
        "in_text": "What is another name for the @ignore decorator in PyTorch 1.2?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "@torch.jit.ignore"
    },
    {
        "in_text": "What does the//operator and NumPy'snp.floor_divide equivalent to?Alias fortorch.linalg.pinv()Alias for torch.linalg.matrix_power()whereNNNis the full window size.",
        "out_text": "floor division"
    },
    {
        "in_text": "How many op Conv/Batch Norm fusions does replace_pattern() replace?schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. NoteCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "one"
    },
    {
        "in_text": "PruningContainer Container holding a sequence of pruning methods for what?Boolean torch.bool torch.*.BoolTensorPyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "iterative pruning"
    },
    {
        "in_text": "What returns a coalesced copy of self if self is an uncoalesced tensor?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "Tensor.coalesce"
    },
    {
        "in_text": "What does Future.done() return if it has a result or an exception?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "ReturnTrueif thisFutureis done"
    },
    {
        "in_text": "By default,torch.optim.swa_utils.AveragedModelcomputes what of the parameters that you provideYou can usetorch.manual_seed()to seed the RNG for all devices (bothTensor.expm1_ In-place version ofexpm1()PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "out_text": "running equal average"
    },
    {
        "in_text": "What will keep track of the running averages of the parameters of the modelmodel?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "arbitrarytorch.nn.Moduleobject.swa_model"
    },
    {
        "in_text": "The wildcard matches any string, including what?Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "out_text": "empty string"
    },
    {
        "in_text": "How to use torch.package.PackageImporter.id, give an example?Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.nextafter_ In-place version ofnextafter()",
        "out_text": "<torch_package_0>"
    },
    {
        "in_text": "What does lu_solve return?Alias for torch.linalg.matrix_power()Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliThe tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "out_text": "LU solve"
    },
    {
        "in_text": "What does seetorch.i0() do?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Let I_0 be the zeroth order modified Bessel function of the first kind"
    },
    {
        "in_text": "householder_product Computes what of a product of Householder matrices?Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False}) assert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn\u2019t be specialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted into a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph. Warningmvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given byones_like Returns a tensor filled with the scalar value 1  , with the same size as input.",
        "out_text": "firstncolumns"
    },
    {
        "in_text": "What means that all values within a tensor are scaled the same way?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "Per tensor"
    },
    {
        "in_text": "What holds the value held by?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "thisFuture"
    },
    {
        "in_text": "What does cublasHandle_t return?It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "the currently selectedStreamfor a given device"
    },
    {
        "in_text": "What is the initial value of param.grad?Alias fortorch.linalg.pinv()\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "None"
    },
    {
        "in_text": "What is the tuple containing inputs to the function Output of running function on *args?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "args"
    },
    {
        "in_text": "What does nn.LeakyReLU apply?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "element-wise function"
    },
    {
        "in_text": "Returns what with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)>>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)>>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1).",
        "out_text": "a tensor"
    },
    {
        "in_text": "What does lr_scheduler.ReduceLROnPlateau Reduce when a metric has stopped improving?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "learning rate"
    },
    {
        "in_text": "What is ifNone,torch.long?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Default"
    },
    {
        "in_text": "Which package will pickle the object normally when you issue a save_pickle(obj,...)call?dot Computes the dot product of two 1D tensors.Tensor.masked_select Seetorch.masked_select()is_available Returns a bool indicating if CUDA is currently available.",
        "out_text": "Package Exporter"
    },
    {
        "in_text": "What specifies the dynamic axes of provided input?Q: Does ONNX support implicit scalar datatype casting?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "A list of integers"
    },
    {
        "in_text": "What is the learning rate of each parameter group set to?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "the initial lr times a given function"
    },
    {
        "in_text": "What function does hardtanh apply element-wise?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "HardTanh"
    },
    {
        "in_text": "By default,dimis the what dimension of theinputtensor?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "out_text": "last dimension"
    },
    {
        "in_text": "What type of example is a static control flow?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "concrete"
    },
    {
        "in_text": "What do you train/test on the dataset?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "audio classifier network"
    },
    {
        "in_text": "How to use torch.overrides.get_ignored_functions, give an example?Alias for torch.linalg.matrix_power()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.frac_ In-place version offrac()",
        "out_text": ">>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse"
    },
    {
        "in_text": "How can you avoid using nondeterministic algorithms for some operations?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "multiple calls to those operations, given the same inputs, will produce the same result"
    },
    {
        "in_text": "What is used to convert operations that require output requantization to module form?Q: Does ONNX support implicit scalar datatype casting?Tensor.logical_xor_ In-place version oflogical_xor()Tensor.logical_or_ In-place version oflogical_or()",
        "out_text": "torch.nn.ReLU"
    },
    {
        "in_text": "Convenience method that creates what with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension?Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().Q: Does ONNX support implicit scalar datatype casting?Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "setuptools.Extension"
    },
    {
        "in_text": "What is another name for a GraphModule?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Construct a GraphModule"
    },
    {
        "in_text": "What is the padding method used?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "whencenterisTrue"
    },
    {
        "in_text": "What is call_function?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "add_1"
    },
    {
        "in_text": "What is the name of the module that has lazy initialization of thenum_featuresargument of theBatchNorm3d?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "nn.GroupNorm"
    },
    {
        "in_text": "What action stubs out the module?NotewhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "mock"
    },
    {
        "in_text": "How to use In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing., give an example?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "# Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis followed by tensor indexing\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data"
    },
    {
        "in_text": "What model's parameters will use the default learning rate of1e-2?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "model.base"
    },
    {
        "in_text": "By default, the extension will be what to run on all archs of the cards visible during the building process of the extension?Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dictIf keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "out_text": "compiled"
    },
    {
        "in_text": "What is theKullback-Leibler divergence Loss Function?the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Notenn.KLDivLoss The Kullback-Leibler divergence loss measureQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "SeeHingeEmbeddingLossfor details"
    },
    {
        "in_text": "How can we help build a new Graph?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "simply take the Graph we obtain from symbolic tracing and modify it"
    },
    {
        "in_text": "What is equivalent toself.to(torch.bool)?Alias fortorch.linalg.pinv()Alias fortorch.le().In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):",
        "out_text": "self.bool()"
    },
    {
        "in_text": "In the symbolic function, if the operator is already standardized in ONNX, what do we need to do to represent the ONNX operatorWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "create a node"
    },
    {
        "in_text": "What does this function do exactly the same thing as in the forward?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "torch.addmm()"
    },
    {
        "in_text": "What will you need to access.codeon if theScriptModulehas more than one method?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliwhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "the module"
    },
    {
        "in_text": "What does executingfuncand a reference to the value of the result of this execution do?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "Creates an asynchronous task"
    },
    {
        "in_text": "Where is the output tensor of the same size asinput except in the dimensiondim?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "size 1"
    },
    {
        "in_text": "What does Alias fortorch.atan() have?Alias fortorch.linalg.pinv()ger Alias of torch.outer().n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "out_text": "arctangent"
    },
    {
        "in_text": "The output of themodelcallable when called with what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "given*argsand**kwargs"
    },
    {
        "in_text": "What is raised if a dependency on any matching packages is found?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "aPackagingErroris"
    },
    {
        "in_text": "What should the index tensorscrow_indicesandcol_indices have element type?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "eithertorch.int64(default) ortorch.int32"
    },
    {
        "in_text": "Draws binary random numbers (0 or 1) from what distribution?Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. WarningPerforms the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. WarningTensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "out_text": "Bernoulli distribution"
    },
    {
        "in_text": "What is the number of repetitions per dimension?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "reps"
    },
    {
        "in_text": "What does your transform acquire from the torch.nn.Module?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "out_text": "Graph"
    },
    {
        "in_text": "The boolean option sorted if True, will ma what?Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifThe most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "k"
    },
    {
        "in_text": "What does save do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "save Saves an object to a disk file"
    },
    {
        "in_text": "What does nn.Sigmoid apply?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "element-wise function"
    },
    {
        "in_text": "Why might the total self cpu time be artificially increased?NoteTo achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.",
        "out_text": "the shape collection"
    },
    {
        "in_text": "What is the name of the tensor that returns a new Tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.abs"
    },
    {
        "in_text": "What performs the element-wise division of tensor 1 by tensor 2 , multiply the result by the scalarvalueand add it to input?Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:addcmul Performs the element-wise multiplication of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input.nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "out_text": "addcdiv"
    },
    {
        "in_text": "How to use torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently., give an example?Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:>>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])",
        "out_text": ">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)"
    },
    {
        "in_text": "What is the second element of an eigenvalue of input?Q: Does ONNX support implicit scalar datatype casting?Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.heaviside Computes the Heaviside step function for each element in input.",
        "out_text": "imaginary part"
    },
    {
        "in_text": "What does mode(bool) do to enable grad (True) or disable (False)?Q: Does ONNX support implicit scalar datatype casting?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self.relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples",
        "out_text": "Flag"
    },
    {
        "in_text": "What does f * M[strided] + f * mean at M[strided]?>>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteReturns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "out_text": "M[sparse_coo]"
    },
    {
        "in_text": "Is True if the Tensor uses sparse storage layout?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the conditionThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "False"
    },
    {
        "in_text": "If what is true, returns a complex tensor of size?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeMoreover, as forgather(), the values ofindexmust be",
        "out_text": "return_complex"
    },
    {
        "in_text": "comm.scatter Scatters tensor across multiple what?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "GPUs"
    },
    {
        "in_text": "What will includemodulein prevent from saving it in the package?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "dependency discovery"
    },
    {
        "in_text": "What is a limitation of thisFuture?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "aFuturecannot be marked completed twice"
    },
    {
        "in_text": "What is returned by sizeendstartstepleftlceil fractextend - textstartTensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliIfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "1-D tensor"
    },
    {
        "in_text": "In what order are methods called fromforward compiled?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "inforward"
    },
    {
        "in_text": "What does Torch.jit.script now try to do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "recursively compile functions, methods, and classes"
    },
    {
        "in_text": "What does Tensor.eq Seetorch.eq do?Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()Tensor.topk Seetorch.topk()",
        "out_text": "Tensor.eq Seetorch.eq()"
    },
    {
        "in_text": "What is the optional parameter for the non-linear function?A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).Fillsselftensor with elements drawn from the geometric distribution:Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "param"
    },
    {
        "in_text": "What does the network learn to output in this simplified example?Q: Does ONNX support implicit scalar datatype casting?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factor",
        "out_text": "zero"
    },
    {
        "in_text": "What is used to optimize a script?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "just-in-time compilation"
    },
    {
        "in_text": "Window (Tensor) \u2013 the optional window function. Default: None (treated as window of all 111 s)model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: TrueAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "out_text": "optional"
    },
    {
        "in_text": "What indices does theselftensor return when selfis a sparse CSR tensor of layoutsparseschedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. NoteCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "out_text": "column"
    },
    {
        "in_text": "What type annotations will be preserved by symbolic tracing?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Python 3-style type annotations"
    },
    {
        "in_text": "What does Tensor.contiguous return?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "a contiguous in memory tensor"
    },
    {
        "in_text": "What is Tensor.msort?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "Seetorch.msort"
    },
    {
        "in_text": "What action stubs out a module?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "mock"
    },
    {
        "in_text": "Which of the two medians is returned if the input tensors have an even number of elements in the dimension dim?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeFor example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)",
        "out_text": "lower"
    },
    {
        "in_text": "What is mvlgamma version of Tensor?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Tensor.mvlgamma_ In-place version ofmvlgamma"
    },
    {
        "in_text": "What applies weight normalization to a parameter in the given module?Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.nn.Hardswish Applies the hardswish function, element-wise, as described in the paper:This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "out_text": "weight_norm"
    },
    {
        "in_text": "What is Tensor.symeig?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Seetorch.symeig"
    },
    {
        "in_text": "Alias for torch.linalg.matrix_power() Returns the what of a 2-D tensorTensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "out_text": "numerical rank"
    },
    {
        "in_text": "What type of sequences are padded sequences?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "variable length"
    },
    {
        "in_text": "What does ACallgrindStats mirror?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "timeit.Timer.timeit()"
    },
    {
        "in_text": "What is the if-statement if self.do_activation?Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()",
        "out_text": "static"
    },
    {
        "in_text": "Timer\u2013 Callable which returns what?Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_Number \u2013 sum(abs(x)**ord)**(1./ord)Tensor.isneginf Seetorch.isneginf()",
        "out_text": "current time"
    },
    {
        "in_text": "What is model(*args)?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "a valid invocation of the model"
    },
    {
        "in_text": "If the Future is already completed, the given callback will be run what?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "inline"
    },
    {
        "in_text": "UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input channels.The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. WarningWarning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "out_text": "nearest neighbor"
    },
    {
        "in_text": "If unset, all error messages will report as originating from GraphModule. It may be helpful to set this to what?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. NoteQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "root\u2019s original name"
    },
    {
        "in_text": "What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm1d?method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.Tensor.smm Seetorch.smm()Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.",
        "out_text": "nn.LazyBatchNorm2d"
    },
    {
        "in_text": "Performs a matrix-vector product of what?Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.Returns a tensor with the same data and number of elements asinput,Returns a tensor with the same data and number of elements asinput,",
        "out_text": "matrix mat and the vector vec"
    },
    {
        "in_text": "Computes the discrete Fourier Transform sample frequencies for a signal of what?>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuuReturns a tensor with the same data and number of elements asinput,",
        "out_text": "sizen"
    },
    {
        "in_text": "If it\u2019s unset, what will happen to all error messages?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "all error messages will report as originating from GraphModule"
    },
    {
        "in_text": "How to use torch.flip, give an example?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": ">>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])"
    },
    {
        "in_text": "To(int) \u2013 the rank that receives the global states. (default: what?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteWarning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "out_text": "0"
    },
    {
        "in_text": "Tensor.cauchy_ Fills the tensor with numbers drawn from what distribution?Tensor.to_dense Creates a strided copy ofself.The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "out_text": "Cauchy"
    },
    {
        "in_text": "fftshift reorders what dimension of FFT data?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "n-dimensional"
    },
    {
        "in_text": "What do we check to confirm that Elu is standardized in ONNX?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "ONNX operator list"
    },
    {
        "in_text": "What type of matrix does matrix multiplication perform?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "sparse"
    },
    {
        "in_text": "Operator_export_type (enum, default) \u2013 OperatorExportTypes.ONNX: All ops are exported as regularrelu_ In-place version ofrelu().>>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])Tensor.addbmm_ In-place version ofaddbmm()",
        "out_text": "OperatorExportTypes.ONNX"
    },
    {
        "in_text": "What is it called when a script is profiled?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Warning"
    },
    {
        "in_text": "What do I store on aScriptModule?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "attributes"
    },
    {
        "in_text": "What algorithm was heavily inspired?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "L-BFGS algorithm"
    },
    {
        "in_text": "What does the add_embedding method add to the summary?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "embedding projector data"
    },
    {
        "in_text": "Before PyTorch 1.2 what decorator was used to make a function or method callable from code that is exported?Q: Does ONNX support implicit scalar datatype casting?Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().get_arch_list Returns list CUDA architectures this library was compiled for.",
        "out_text": "@ignore"
    },
    {
        "in_text": "What criterion measures the loss given input tensorsx1x_1x1,x2x_2x2atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.xlogy Computesinput*log(other)with the following cases.Tensor.multiply_ In-place version ofmultiply().",
        "out_text": "MultiMarginLoss"
    },
    {
        "in_text": "What else can actual and expected be?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "be Sequence\u2019s or Mapping\u2019s"
    },
    {
        "in_text": "People might need to refactor the model to make it compatible with what?NoteIt is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "FX Graph Mode Quantization"
    },
    {
        "in_text": "Tracing of in-place operations of tensor views (e.g., what on the left-hand side of an assignment)Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:dstack Stack tensors in sequence depthwise (along third axis).Note Wheninputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically returnindicesfor any of them. input(Tensor) \u2013 the input tensor. k(int) \u2013 k for the k-th smallest element dim(int,optional) \u2013 the dimension to find the kth value along keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor)",
        "out_text": "indexing"
    },
    {
        "in_text": "How to use Old API:New API:, give an example?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())"
    },
    {
        "in_text": "What is in-place version ofrenorm?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.renorm_ In-place version ofrenorm()"
    },
    {
        "in_text": "Proxy objects are called what?Alias fortorch.linalg.pinv()softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "arugments"
    },
    {
        "in_text": "How to use torch.distributed.Store.num_keys, give an example?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "out_text": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()"
    },
    {
        "in_text": "What allows you to specialize on the value of b to trace through?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "concrete_args"
    },
    {
        "in_text": "What does shapebut avoid?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "create to intermediate tensors"
    },
    {
        "in_text": "What is used to distinguish sparse instances from other layouts?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "Use torch.Tensor.is_sparse"
    },
    {
        "in_text": "The current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvarsThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details. >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method as Graph.create_node(). Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "current state of convergence criteria"
    },
    {
        "in_text": "If an internmodule glob pattern is added with allow_empty=False, andclose()is called before any modules match thatmethod=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.The hook will be called each time a module matches against an extern() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> NoneBy default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "out_text": "If allow_empty=True"
    },
    {
        "in_text": "What will be copied over from the respective place within root's Module hierarchy into the GraphModule's module hierarchy?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Module-based objects"
    },
    {
        "in_text": "What happens if any of the elements of relevant_args have __torch_function__ implementations?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "if any of the elements of relevant_args have __torch_function__ implementations"
    },
    {
        "in_text": "What is the mode value of each row of the input tensor in the given dimensiondim?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "a named tuple"
    },
    {
        "in_text": "What does Tensor.ndim Alias for dim() Tensor.real Return a new tensor containing?Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. NoteTensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalescedTensor.type_as Returns this tensor cast to the type of the given tensor.",
        "out_text": "real values of the self tensor"
    },
    {
        "in_text": "What does Tensor.logaddexp2 do?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "Tensor.logaddexp2 Seetorch.logaddexp2()"
    },
    {
        "in_text": "Computes the first kind for each element of input. input(Tensor) \u2013 the input tensor. out(Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "out_text": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "in_text": "Proxys allows you to specify your rewrite rules as native what?dot Computes the dot product of two 1D tensors.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "Python code"
    },
    {
        "in_text": "What should be passed as the example output if there is more than one item?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "only one item"
    },
    {
        "in_text": "What is the input tensor expected to be?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "output ofstft()"
    },
    {
        "in_text": "What is the basic usage of the Quantization Invert Transformation?Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.nn.Hardswish Applies the hardswish function, element-wise, as described in the paper:This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "out_text": "Basic usage"
    },
    {
        "in_text": "What is highly recommended to do with the Docstring?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "add a few examples"
    },
    {
        "in_text": "What is an exception to this rule?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "cases in which the last input is also of a dictionary type"
    },
    {
        "in_text": "Annotating a class member asFinal or adding it to a list called what will mark the contained names as constants?Q: Does ONNX support implicit scalar datatype casting?Note Wheninputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically returnindicesfor any of them. input(Tensor) \u2013 the input tensor. k(int) \u2013 k for the k-th smallest element dim(int,optional) \u2013 the dimension to find the kth value along keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor)Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "out_text": "constants"
    },
    {
        "in_text": "What can you do to suit the particular requirements of a part of a model?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "compose tracing and scripting"
    },
    {
        "in_text": "What is an example of a pseudorandom number generator?Tensor.swapaxes Seetorch.swapaxes()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) ReferencesTensor.frexp Seetorch.frexp()",
        "out_text": "Example"
    },
    {
        "in_text": "If False, return the first suitable location that is found.annotate This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.Note LU factorization with pivot = False is not available for CPU, and attempting to do so will throw an error. However, LU factorization with pivot = False is available for CUDA. Note This function does not check if the factorization was successful or not if get_infos is True since the status of the factorization is present in the third element of the return tuple. Note In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support, but only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor. Default: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)). pivots stores all the intermediate transpositions of rows. The final permutation perm could be reconstructed by applying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1, where perm is initially the identity permutation of mmm elements (essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of size (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "out_text": "return the last such index"
    },
    {
        "in_text": "Flip tensor in the left/right direction, returning a new tensor?hstack Stack tensors in sequence horizontally (column wise).hstack Stack tensors in sequence horizontally (column wise).For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "out_text": "Flip tens"
    },
    {
        "in_text": "A dictionary that maps overridable functions to lambda functions that have the same signature as the real function and unconditionally return -1?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "PyTorch API"
    },
    {
        "in_text": "What does lobpcg stand for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "lobpcg"
    },
    {
        "in_text": "What does @torch.jit.ignore and @torch.jit.unused provide?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "details"
    },
    {
        "in_text": "When does a package run locally because it is importing a locally-installed package fail?Q: Does ONNX support implicit scalar datatype casting?With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "when the package is copied to another machine"
    },
    {
        "in_text": "In-place version ofelu()?n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013ger Alias of torch.outer().ger Alias of torch.outer().",
        "out_text": "In-place version ofelu()"
    },
    {
        "in_text": "What returns the product of all elements in the inputtensor?Q: Does ONNX support implicit scalar datatype casting?pow Takes the power of each element in inputwithexponentand returns a tensor with the result.For example, ifinputis of shape",
        "out_text": "Returns the product of all elements in theinputtensor"
    },
    {
        "in_text": "What is function that computes the digamma function on input?Q: Does ONNX support implicit scalar datatype casting?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.Tensor.logical_xor_ In-place version oflogical_xor()",
        "out_text": "polygamma"
    },
    {
        "in_text": "What is program that randomly zeros out entire channels?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "nn.Dropout3d"
    },
    {
        "in_text": "What is incompatible withonesided=True?Alias fortorch.linalg.pinv()IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "return_complex"
    },
    {
        "in_text": "What is executed by timeit.Timer.timeit()?Tensor.arctanh Seetorch.arctanh()Alias for torch.linalg.matrix_power()Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "main statement (stmt)numbertimes"
    },
    {
        "in_text": "What function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputorFillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "out_text": "symeig"
    },
    {
        "in_text": "What can we use concrete_args to do?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "to specialize on the value of b to trace through this"
    },
    {
        "in_text": "What type of tensor does Tensor.coalesce return?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Examplepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "coalesced copy"
    },
    {
        "in_text": "What mode supports three types of quantization?Q: Does ONNX support implicit scalar datatype casting?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "Eager Mode Quantization"
    },
    {
        "in_text": "What is applied to singular input?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "Cholesky"
    },
    {
        "in_text": "The profiler will skip what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "firstskip_firststeps"
    },
    {
        "in_text": "How to use Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model, give an example?Because your script will be profiled, please ensure that it exits in a finite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful. NoteFirst convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?",
        "out_text": "torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)"
    },
    {
        "in_text": "If unspecified, the behavior is chosen automatically as follows.is_storage Returns True if obj is  a PyTorch storage object.The resulting out tensor shares its underlying storage with the\ninput tensor, so changing the content of one would change the content\nof the other. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])input(Tensor) \u2013 the input tensor. Example",
        "out_text": "default None"
    },
    {
        "in_text": "When are both thevaluesandindicestensors the same size as input?For example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensorssubmodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method as Graph.create_node(). Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.",
        "out_text": "IfkeepdimisTrue"
    },
    {
        "in_text": "What is the name of the document that describes how a package is packaged?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "torch.packageFormat Overview"
    },
    {
        "in_text": "What is an example of using?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dAlias fortorch.linalg.pinv()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "out_text": "FlameGraph tool"
    },
    {
        "in_text": "What does sparse.sum return?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "sum of each row of the sparse Tensor inputin the given dimensionsdim"
    },
    {
        "in_text": "What is a tensor gain?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "an optional scaling factor"
    },
    {
        "in_text": "What must you construct to usetorch.optim?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "an optimizer object"
    },
    {
        "in_text": "What does the symbolic tracer perform of the Python code?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "symbolic execution"
    },
    {
        "in_text": "The function can be called once the gradients are computed using what?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "out_text": "e.g.backward()"
    },
    {
        "in_text": "What makes the output tensors of the same size as input except in the dimensiondimwhere they are of size 1?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "IfkeepdimisTrue"
    },
    {
        "in_text": "Returns a coalesced copy of self if self is an what type of tensor?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "uncoalesced"
    },
    {
        "in_text": "What is the name of the function exposed by the public torch API?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "public_api(function)"
    },
    {
        "in_text": "What Applies 3D average-pooling operation inkTkHkWkT times kH timestriplet_margin_loss SeeTripletMarginLossfor detailstriplet_margin_loss SeeTripletMarginLossfor details>>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])",
        "out_text": "avg_pool3d"
    },
    {
        "in_text": "What does ifftn compute?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "N dimensional inverse discrete Fourier transform"
    },
    {
        "in_text": "How to use torch.bitwise_xor, give an example?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])"
    },
    {
        "in_text": "How do we denote a N-dimensional hybrid tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "We use (M + K)-dimensional tensor where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds"
    },
    {
        "in_text": "What does flip tensor in the up/down direction return?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self.relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. ExamplesAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.",
        "out_text": "torch.flipud"
    },
    {
        "in_text": "PyTorch, operation Sparse grad?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "PyTorch"
    },
    {
        "in_text": "What is the name of the module that can be used to package code?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Package a Torch Script module"
    },
    {
        "in_text": "What are the corresponding args for callablemodel?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "*args"
    },
    {
        "in_text": "What effect does argumentsome have whencompute_uvisFalse?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?multilabel_margin_loss SeeMultiLabelMarginLossfor details.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "argumentsomehas no effect whencompute_uvisFalse"
    },
    {
        "in_text": "What is the default setting for input when center is True?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "out_text": "input will be padded on\nboth sides so that the ttt-th frame is centered"
    },
    {
        "in_text": "How to use torch.cos, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": ">>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])"
    },
    {
        "in_text": "How can certain aspects of the interpreter\u2019s execution be overridden?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "method overrides"
    },
    {
        "in_text": "What supports values stored as strided tensors?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "s.values().layout == torch.strided"
    },
    {
        "in_text": "If the operator is an operator of what type, what operator can you find the declaration of the function in torch/csrc/autograd/Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrueIf the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        "out_text": "ATen"
    },
    {
        "in_text": "What does the Future's value contain that reside on GPUs?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "tensors"
    },
    {
        "in_text": "What does usetorch.int32 do?Alias for torch.linalg.matrix_power()If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "MKL-enabled matrix operations"
    },
    {
        "in_text": "What type of pruning method does prune.identity apply pruning reparametrization to the tensor corresponding to the parameter callednameinChecks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. NoteTensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalescedtorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "random_unstructured"
    },
    {
        "in_text": "Parameter ordering does NOT match what?Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifThe most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "VariableType.h"
    },
    {
        "in_text": "What dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.dtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "out_text": "sparse and dense"
    },
    {
        "in_text": "What is Alias fordim?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.ndim"
    },
    {
        "in_text": "What is a Tensor.diagonal?Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in orderAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.Tensor.sigmoid Seetorch.sigmoid()",
        "out_text": "Seetorch"
    },
    {
        "in_text": "In the case that root is a what, the qualified name found in a Node\u2019s target will be looked up directly in the dictcelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Noteconcrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "dict"
    },
    {
        "in_text": "How to use torch.Generator.get_state, give an example?torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensorFunctions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "out_text": ">>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()"
    },
    {
        "in_text": "What performs a matrix -vector product of the matrix  mat and the vector vec?ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTensor.logical_xor Seetorch.logical_xor()TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "addmv"
    },
    {
        "in_text": "What is the name of the function that can be used to force a fresh download of the github repo unconditionally?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "force_reload"
    },
    {
        "in_text": "What is the name of the submodule itself?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "m"
    },
    {
        "in_text": "What is the Torch Hub cache directory used for storing downloaded models & weights?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "out_text": "$XDG_CACHE_HOME/torch/hub"
    },
    {
        "in_text": "What does static quantization require to determine optimal quantization parameters for activations?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "calibration with a representative dataset"
    },
    {
        "in_text": "How long does it take to block the value of thisFuture?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "until the value of thisFutureis ready"
    },
    {
        "in_text": "Where can you find a manual for advanced usage of TensorBoard?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "https://threejs.org/docs/index.html"
    },
    {
        "in_text": "What need to be defined in init function so that inferencing can handle it properly?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Dropout layer"
    },
    {
        "in_text": "What does Tensor.signbit do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Seetorch.signbit()"
    },
    {
        "in_text": "What kind of pooling does adaptive_max_pool3d apply?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "3D"
    },
    {
        "in_text": "What are some examples of tensormethods?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "add/reshape/clone"
    },
    {
        "in_text": "What will be different objects with those before the call?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013Note",
        "out_text": "Parameters of a model after.cuda()"
    },
    {
        "in_text": "What is manager that selects a given stream?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "StreamContext Context-manager"
    },
    {
        "in_text": "What does ortorch.linalg.matrix_norm() do?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "matrix norms"
    },
    {
        "in_text": "What is the result of torch.FloatTensor.abs()?Alias fortorch.linalg.pinv()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Examplepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "a new tensor"
    },
    {
        "in_text": "What type of matrices does the function support?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "batches of matrices"
    },
    {
        "in_text": "For some modules, it may be useful to have state beyond parameters that affects module computation but is what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "not learnable"
    },
    {
        "in_text": "What is provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss?Q: Does ONNX support implicit scalar datatype casting?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "out_text": "Higher-level APIs"
    },
    {
        "in_text": "Returns a tensor filled with the scalar value 1 with what?Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These includeReturns a new tensor with a dimension of size one inserted at theCreate a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "out_text": "the shape defined by the variable argument size"
    },
    {
        "in_text": "What is dim?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "dimension along which to split the tensor"
    },
    {
        "in_text": "What are the keyword arguments represented as?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "key-value pairs"
    },
    {
        "in_text": "What does path(str) do to save stacks file to this location?Alias for torch.linalg.matrix_power()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()",
        "out_text": "path(str) \u2013 save stacks file to this location"
    },
    {
        "in_text": "What do we want to see?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "out_text": "Node\u2019s"
    },
    {
        "in_text": "What is the name of the tensor that prunes tensors corresponding to all parameters inparameters?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "prune.custom_from_mask"
    },
    {
        "in_text": "What holds the data and list ofbatch_sizesof a packed sequence?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "nn.utils.rnn.PackedSequence"
    },
    {
        "in_text": "What does comm.gather gather from multiple GPU devices?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "out_text": "tensors"
    },
    {
        "in_text": "Returns what of the given input tensor along a given dimension?Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.Returns a new tensor with a dimension of size one inserted at theMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "out_text": "k largest elements"
    },
    {
        "in_text": "What does nn.AvgPool3d Applies over an input signal composed of several input planes?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "2D average pooling"
    },
    {
        "in_text": "What does the tensor return with?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "the specified shape"
    },
    {
        "in_text": "What is a method that should only be called after a call to wait() has completed?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteQ: Does ONNX support implicit scalar datatype casting?f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "Obtain the value of an already-completed future"
    },
    {
        "in_text": "What argument does callback(Callable) take as the only argument?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "thisFuture"
    },
    {
        "in_text": "If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dQ: Does ONNX support implicit scalar datatype casting?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "the argument has no effect"
    },
    {
        "in_text": "Input will be padded on both sides so that thettt-th frame is centered at what?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulTensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "out_text": "IfcenterisTrue"
    },
    {
        "in_text": "lu Computes what factorization of a matrix  or batches of matricesA?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "LU"
    },
    {
        "in_text": "Which of the two medians is returned if keepdim is True?Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "out_text": "lower"
    },
    {
        "in_text": "What does is_standalone do if False?Alias for torch.linalg.matrix_power()Alias fortorch.linalg.pinv()Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "build a standalone executable"
    },
    {
        "in_text": "What is loaded into the process as a side effect?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "The shared library"
    },
    {
        "in_text": "This method can be called to clean up an nn.Module without manually calling what on each unused submodule?The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. WarningYes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "delete_submodule"
    },
    {
        "in_text": "What type of storage does Theasyncarg cast to?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "bfloat16"
    },
    {
        "in_text": "Fills the input Tensor with values drawn from what?sub Subtracts other, scaled byalpha, from input.sub Subtracts other, scaled byalpha, from input.sub Subtracts other, scaled byalpha, from input.",
        "out_text": "the uniform distribution"
    },
    {
        "in_text": "How to use torch.normal, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": ">>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])"
    },
    {
        "in_text": "Nuclear norm can only be calculated across exactly how many dimensions?std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.Q: Does ONNX support implicit scalar datatype casting?is_storage Returns True if obj is  a PyTorch storage object.",
        "out_text": "two"
    },
    {
        "in_text": "What program returns a view in constant time?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "NumPy"
    },
    {
        "in_text": "What is the name of a subclass that should overwrite__getitem__()?Tensor.logical_xor_ In-place version oflogical_xor()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "Note"
    },
    {
        "in_text": "what pointer does current_blas_handle return?Q: Does ONNX support implicit scalar datatype casting?Alias for torch.linalg.matrix_power()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "cublasHandle_t"
    },
    {
        "in_text": "What must also be specified if rtol(Optional[float]) is omitted?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "out_text": "specifiedatol"
    },
    {
        "in_text": "How does quantization currently work?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "module by module basis"
    },
    {
        "in_text": "If input is complex and neitherdtypenoroutis specified, the result's data type will be the corresponding floating point type (e.The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifReturns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": "ifinputis complexfloat"
    },
    {
        "in_text": "At what rate are channels in a tensor pruned?Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "out_text": "random"
    },
    {
        "in_text": "What method adds a set of hyperparameters to be compared in TensorBoard?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "add_hparams"
    },
    {
        "in_text": "Thevaluestensor contains the values of the CSR tensor. This is a what?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeDefining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "out_text": "1-D tensor of sizennz"
    },
    {
        "in_text": "What is 8-bit integer (unsigned) torch?Alias for torch.linalg.matrix_power()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()",
        "out_text": "*.ByteTensor"
    },
    {
        "in_text": "The tracer may witness what type of creation on a specific device?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "tensor"
    },
    {
        "in_text": "What does nn.SoftMarginLoss create?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "criterion"
    },
    {
        "in_text": "What returns the unique elements of the input tensor?Q: Does ONNX support implicit scalar datatype casting?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "out_text": "unique"
    },
    {
        "in_text": "What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?Q: Does ONNX support implicit scalar datatype casting?Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported.Tensor.arctanh Seetorch.arctanh()",
        "out_text": "precision"
    },
    {
        "in_text": "References TorchScript is a way to do what from PyTorch code?Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.Tensor.fmod_ In-place version offmod()",
        "out_text": "create serializable and optimizable models"
    },
    {
        "in_text": "What does Tensor.to_mkldnn return?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliAlias for torch.linalg.matrix_power()No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "a copy"
    },
    {
        "in_text": "What type of tuple is out?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "optional output"
    },
    {
        "in_text": "What is the default value of verbose?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "bool, default False"
    },
    {
        "in_text": "What is the name of the package format used by Torch Script?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "torch.packageFormat"
    },
    {
        "in_text": "What is very high and often gives a heavily skewed timeline?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "NVTX overhead"
    },
    {
        "in_text": "Which two packages provide afile_structure()method?You can usetorch.manual_seed()to seed the RNG for all devices (bothinput(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.register_module_forward_hook Registers a global forward hook for all the modules",
        "out_text": "Package Importer and Package Exporter"
    },
    {
        "in_text": "What happens if atol is omitted?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "default values based on the dtype are selected with the below table"
    },
    {
        "in_text": "What does the context-manager do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "sets the anomaly detection for the autograd engine on or off"
    },
    {
        "in_text": "What type of LR reduces learning rate when a metric has stopped improving?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "Cyclic"
    },
    {
        "in_text": "What does matrix _exp compute?Tanh 53\\frac{5}{3}35\u200bTo achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.",
        "out_text": "matrix  exponential"
    },
    {
        "in_text": "Is it okay to load two different branches of the same repo in separate processes?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "totally fine"
    },
    {
        "in_text": "What type of context managers are torch.no_grad(), torch.enable_grad(), and torch.set_grad_enableIf the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')The multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}\u03a3\nor a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121\nor a lower-triangular matrix L\\mathbf{L}L with positive-valued\ndiagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance. >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)",
        "out_text": "thread local"
    },
    {
        "in_text": "What does Useschedule() generate?Q: Does ONNX support implicit scalar datatype casting?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "callable schedule"
    },
    {
        "in_text": "What is the default size of the PyTorch threadpool?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "one"
    },
    {
        "in_text": "Convert a tensor to what format?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "compressed row storage format"
    },
    {
        "in_text": "What API does the user use to combine operations/modules into a single module?This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLossAs you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor",
        "out_text": "torch.quantization.fuse_modules()"
    },
    {
        "in_text": "What is the type of Quantiztion Aware Training?32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensorRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "Static Input/Output Model Type torch"
    },
    {
        "in_text": "If deterministic output compared to non-checkpointed passes is not required, what can be done to omit stashing andReturns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedtimer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "out_text": "supply preserve_rng_state=False to checkpoint or checkpoint_sequential"
    },
    {
        "in_text": "In what platform are the key nn modules Conv2d() and Linear() implemented?Q: Does ONNX support implicit scalar datatype casting?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "FP32"
    },
    {
        "in_text": "What is the name of the window suitable for use in spectral analysis?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "periodic"
    },
    {
        "in_text": "What does move totorch.hub do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "Loads the Torch serialized object at the given URL"
    },
    {
        "in_text": "What Constructs a tensor by repeating the elements of input?ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTensor.logical_xor Seetorch.logical_xor()TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "tile"
    },
    {
        "in_text": "Set param.grad = a zeroed tensor with desired strides before the first backward() and never reset it to whatFor example, ifinputis of shapepow Takes the power of each element in inputwithexponentand returns a tensor with the result.nn.ELU Applies the element-wise function:",
        "out_text": "None"
    },
    {
        "in_text": "What is the default value for displaying a progress bar to stderr?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Default: None"
    },
    {
        "in_text": "What returns cublasHandle_t pointer to current cuBLAS handle?Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Alias for torch.linalg.matrix_power()No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "current_blas_handle"
    },
    {
        "in_text": "When does ReduceLROnPlateau Reduce learning rate?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "when a metric has stopped improving"
    },
    {
        "in_text": "A Module is considered \u201cused\u201d if any one of the following is true: 1. It has what that are used?Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifThe most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "children"
    },
    {
        "in_text": "What can be provided to pass additional arguments to the compilation process?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "extra_cflags or extra_ldflags"
    },
    {
        "in_text": "If normalized is True, the function returns what?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "normalized STFT results"
    },
    {
        "in_text": "What are the inputs to a Graph?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "operations that run inside the method"
    },
    {
        "in_text": "What is True if the Tensor uses sparse storage layout?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "Tensor.is_sparse"
    },
    {
        "in_text": "What does this allow for backends/runtimes that execute graphs?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "better optimizations"
    },
    {
        "in_text": "Default: if what, infers data type fromvalues?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Tensor.isneginf Seetorch.isneginf()Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "None"
    },
    {
        "in_text": "What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "add step"
    },
    {
        "in_text": "What does the export call assume is intended to represent the optional dictionary consisting of named arguments?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Notef = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "the \u2018x\u2019 input"
    },
    {
        "in_text": "What non-blocking behavior is similar to the non-blocking behavior of?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteQ: Does ONNX support implicit scalar datatype casting?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "wait()"
    },
    {
        "in_text": "To add export support, developers need to touch the source code of what?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "PyTorch"
    },
    {
        "in_text": "Does not have any effect if source='local'. Default is False. verbose(bool,optional)Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).do not convert to numpy types: y = x.astype(np.int)cartesian_prod Do cartesian product of the given sequence of tensors.",
        "out_text": "local"
    },
    {
        "in_text": "What is the name of the function that can be called?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "tracker"
    },
    {
        "in_text": "What is a dataset that represents an iterable of data samples called?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "iterable Dataset"
    },
    {
        "in_text": "What is Tensor.sinh?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Seetorch.sinh"
    },
    {
        "in_text": "More than one element of a created tensor may refer to a single what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "memory location"
    },
    {
        "in_text": "If get_infos is what, then the elements in the tuple are Tensor, IntTensor, and InTensor.mvlgamma_ In-place version ofmvlgamma()This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all haveTensor.xlogy Seetorch.xlogy()",
        "out_text": "False"
    },
    {
        "in_text": "What is the value of AdaptiveMaxPool1d?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseNo, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "nn"
    },
    {
        "in_text": "How many times can each dlpack be consumed?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "once"
    },
    {
        "in_text": "What does comm.scatter Scatters tensor do across multiple GPUs?Q: Does ONNX support implicit scalar datatype casting?torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "out_text": "comm.scatter Scatters tensor"
    },
    {
        "in_text": "What allows C-level patching of functions?Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "out_text": "enable_cpatching"
    },
    {
        "in_text": "Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. WhatA (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).Fillsselftensor with elements drawn from the geometric distribution:To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "out_text": "parametrizations.spectral_norm"
    },
    {
        "in_text": "Check for what in the elements of an iterable?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013Note",
        "out_text": "__torch_function__ implementations"
    },
    {
        "in_text": "What function does no M[sparse_coo]@V[strided]->V[strided] usetorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "out_text": "torch.mv()"
    },
    {
        "in_text": "memory_stats Returns a dictionary of what for a given device?Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.neg_ In-place version ofneg()",
        "out_text": "CUDA memory allocator statistics"
    },
    {
        "in_text": "What can you use for ONNX Runtime?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "the backend"
    },
    {
        "in_text": "If map_location is a string containing a device tag, it indicates the location where all tensors should be loaded?Tensor.is_set_to Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.",
        "out_text": "torch.device object or a string containing a device tag"
    },
    {
        "in_text": "What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinputFillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "Alias fortorch.atan"
    },
    {
        "in_text": "What is instantiated by *argsand**kwargsintorch.hub.load()?Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrueAlso known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "model"
    },
    {
        "in_text": "In this case, we have a single placeholder node with a target of what?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "x"
    },
    {
        "in_text": "When does use-1. tracker(callable,optional) \u2013 a function for tracing the iteration process?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteTensor.logical_xor_ In-place version oflogical_xor()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "until convergence criteria is met"
    },
    {
        "in_text": "What is the name of the program that applies a 2D transposed convolution over an input signal?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "nn.ConvTranspose3d"
    },
    {
        "in_text": "What two things do we allow mixing?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "tracing and scripting"
    },
    {
        "in_text": "What does nn.TripletMarginWithDistanceLoss measure?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "triplet loss"
    },
    {
        "in_text": "What is the minimum value for a Seetorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.minimum Seetorch.minimum"
    },
    {
        "in_text": "CustomFromMask prune.identity Applies what to the tensor corresponding to the parameter callednameinmodule?Tensor.to_dense Creates a strided copy ofself.nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.promote_types Returns The torch.dtypewith the smallest size and scalar kind that is not smaller nor of lower kind than eithertype1ortype2.",
        "out_text": "pruning reparametrization"
    },
    {
        "in_text": "What does mock do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "stub out this module"
    },
    {
        "in_text": "What would the new call look like?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "The new call would look like this"
    },
    {
        "in_text": "What is function that computes the element-wise logical AND of the given input tensors?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "logical_and Computes the element-wise logical AND of the given input tensors"
    },
    {
        "in_text": "For what devices can you usetorch.manual_seed() to seed the RNG?Alias for torch.linalg.matrix_power()You can usetorch.manual_seed()to seed the RNG for all devices (bothReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "all devices"
    },
    {
        "in_text": "What supports multiple approaches to quantizing a deep learning model?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "PyTorch"
    },
    {
        "in_text": "What type of element is drawn from the geometric distribution?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Moreover, as forgather(), the values ofindexmust beLUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "out_text": "geometric"
    },
    {
        "in_text": "What provides advanced features that are designed to work with modules?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "PyTorch"
    },
    {
        "in_text": "Returns what of the lower triangular part of a row-by-col matrix in a 2-by-N Tensor?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "the indices"
    },
    {
        "in_text": "What does broadcast_tensors() return of the buckets to which each value in the input belongs?Q: Does ONNX support implicit scalar datatype casting?After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "the indices"
    },
    {
        "in_text": "What should you do instead of splitting up large files with unrelated dependencies?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "define single-purpose modules"
    },
    {
        "in_text": "What version of the function is supported by most optimizers?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "simplified version"
    },
    {
        "in_text": "What should be replaced with L,_=torch.eig(A)?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')>>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1).",
        "out_text": "L,V=torch.eig(A,eigenvectors=True)"
    },
    {
        "in_text": "What is a ZIP archive that uses the.ptextension?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?multilabel_margin_loss SeeMultiLabelMarginLossfor details.>>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])",
        "out_text": "a torch.packagefile"
    },
    {
        "in_text": "What is the name of the tensor cores used in matrix multiplications on Ampere devices?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "SeeTensorFloat-32(TF32)"
    },
    {
        "in_text": "What is returned when a named tuple(values,indices) returns the cumulative minimum of elements of inputin the dimension dimYou can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "out_text": "cumulative product of elements of inputin the dimension dim"
    },
    {
        "in_text": "Why is LU factorization repeated for singular matrices?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeFunction that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "out_text": "LU factorization is repeated for singular matrices"
    },
    {
        "in_text": "What is the layout signature torch.mv() no?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "M[sparse_coo]@V[strided]->V[strided]"
    },
    {
        "in_text": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_mesh, give an example?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "out_text": "from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()"
    },
    {
        "in_text": "Allows the model to jointly attend to what from different representation subspaces?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "information"
    },
    {
        "in_text": "Tensor.view Returns a new tensor with the same data as what?Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "out_text": "the self tensor"
    },
    {
        "in_text": "The mock hook is called each time a module matches against what?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "amock()pattern"
    },
    {
        "in_text": "Returns what tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive)Computes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum andComputes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum andTraining Functions Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX.  It runs a single round of inference and then saves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet).  The keyword argument verbose=True causes the exporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library. You can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime, you will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which were actually run during this run.  This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won\u2019t be accurate.  Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.)  We recommend examining the model trace and making sure the traced operators look reasonable.  If your model contains control flows like for loops and if conditions, trace-based exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run.  If you want to export your model with dynamic control flows, you will need to use the script-based exporter.",
        "out_text": "a tensor"
    },
    {
        "in_text": "After completing this tutorial, you will be familiar with the basic API for what?Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. WarningComputes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dpoisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "creating and using Torch packages"
    },
    {
        "in_text": "What is Seetorch.isneginf function?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.isneginf"
    },
    {
        "in_text": "What setting does the benchmarking feature differ from?Q: Does ONNX support implicit scalar datatype casting?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factor",
        "out_text": "thetorch.backends.cudnn.deterministicsetting"
    },
    {
        "in_text": "What is the name of the function that returns a complex tensor?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "return_complex"
    },
    {
        "in_text": "What is used to apply the custom_from_mask?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "pre-computed mask inmask"
    },
    {
        "in_text": "In what direction do the entries in each row of tensor appear in a different order than before?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "left/right"
    },
    {
        "in_text": "If window is what (default) value, it is treated as if having 111 everywhere in the window?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "None"
    },
    {
        "in_text": "Out (Tensor, optional) \u2013 what is ignored if out = None?Number \u2013 sum(abs(x)**ord)**(1./ord)Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Tensor.double self.double()is equivalent toself.to(torch.float64).",
        "out_text": "the output tensor"
    },
    {
        "in_text": "What is AdaptiveAvgPool3d when attempting to differentiate?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "Tensor.random_ Fillsselftensor with numbers sampled from what kind of distribution?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "uniform distribution"
    },
    {
        "in_text": "What is the new shape?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "shape"
    },
    {
        "in_text": "What happens if PyTorch was built without GPU?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "synchronize CUDA"
    },
    {
        "in_text": "What type of data does TensorBoard finetune?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Image/Video"
    },
    {
        "in_text": "What is the default location for how repo_or_dir is to be interpreted?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "github"
    },
    {
        "in_text": "Option arguments will be used as what in the groups that didn\u2019t override them?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "defaults"
    },
    {
        "in_text": "How can certain aspects of the interpreter's execution be overridden?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "method overrides"
    },
    {
        "in_text": "What is the starting value for the set of points end?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteNumber \u2013 sum(abs(x)**ord)**(1./ord)With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "start"
    },
    {
        "in_text": "What do not convert to numpy tensors?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).whereNNNis the full window size.",
        "out_text": "numpy types"
    },
    {
        "in_text": "A namedtuple of (values, indices, etc.) is returned, where the indices are the indices ofTo compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index.check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0>>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3",
        "out_text": "indices"
    },
    {
        "in_text": "How many conv1,2,3D?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "111"
    },
    {
        "in_text": "What direction does the code in the example above go?Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dcelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "out_text": "off"
    },
    {
        "in_text": "What is the name for the embedding in add_embedding method?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "tag"
    },
    {
        "in_text": "What is Closeness defined as?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "whereinputandotherare finite"
    },
    {
        "in_text": "What does the k-th stride make it possible to do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "perform many tensor operations efficiently"
    },
    {
        "in_text": "What controls whether to computeUandV?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "compute_uv"
    },
    {
        "in_text": "What is the auxiliary section for debuggers?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Available Debuggers"
    },
    {
        "in_text": "What can data (array_like) \u2013 Initial data for the tensor be?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "a list, tuple, NumPy ndarray, scalar, and other types"
    },
    {
        "in_text": "When are the activations dynamically quantized?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "inference"
    },
    {
        "in_text": "Returns a Future object to what?is_tensor Returns True if obj is  a PyTorch tensor.wait Forces completion of a torch.jit.Future[T]asynchronous task, returning the result of the task.wait Forces completion of a torch.jit.Future[T]asynchronous task, returning the result of the task.",
        "out_text": "a list of the passed in Futures"
    },
    {
        "in_text": "What distribution does Tensor.cauchy come from?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "Cauchy"
    },
    {
        "in_text": "The indices of specified elements are collected in indices tensor of size (sparse_dims, dense_Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace fileThis section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "out_text": "nse"
    },
    {
        "in_text": "Matrix multiplies sparse matrix input with what?Q: Does ONNX support implicit scalar datatype casting?clip_grad_norm_ Clips gradient norm of an iterable of parameters.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "dense matrix mat"
    },
    {
        "in_text": "What is used to generate an encoder module?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Q: Does ONNX support implicit scalar datatype casting?Tensor.frexp Seetorch.frexp()",
        "out_text": "tracing"
    },
    {
        "in_text": "What is created of sizestepswhose values are evenly spaced from starttoend inclusive?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).xlogy Computesinput*log(other)with the following cases.",
        "out_text": "one-dimensional tensor"
    },
    {
        "in_text": "What is built with CUDA support?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "PyTorch"
    },
    {
        "in_text": "The input is matrix-multiplied with what?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "theweightparameter"
    },
    {
        "in_text": "What is the name of Tensor.arccosh?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "acosh_()"
    },
    {
        "in_text": "What is Tensor.logical_xor?Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Tensor.logical_xor_ In-place version oflogical_xor()",
        "out_text": "Seetorch"
    },
    {
        "in_text": "What cannot be resized?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Storages in shared memory"
    },
    {
        "in_text": "Returns what tensor of size end start step left l ceil fractextend -Tensor.isneginf Seetorch.isneginf()Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. WarningTensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "out_text": "1-D"
    },
    {
        "in_text": "What is In-place version ofrenorm?Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "Tensor.renorm"
    },
    {
        "in_text": "What does a debugger break on?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "transform_graph(traced)"
    },
    {
        "in_text": "What is rtol?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Relative tolerance"
    },
    {
        "in_text": "Operators can also have what?Alias fortorch.linalg.pinv()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "associatedblocks"
    },
    {
        "in_text": "What type of sine does asinh return?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "hyperbolic"
    },
    {
        "in_text": "Under what conditions might you never need to use in-place operations?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Warning",
        "out_text": "heavy memory pressure"
    },
    {
        "in_text": "What does torch.mv() do?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "no"
    },
    {
        "in_text": "What computes the singular value decomposition of a matrix ?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.Q: Does ONNX support implicit scalar datatype casting?vander Generates a Vandermonde matrix .",
        "out_text": "svd"
    },
    {
        "in_text": "What Applies Batch Normalization over a 5D input as described in the paperBatch Normalization: Accelerating Deep Network Training by ReducThis function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "nn.LazyBatchNorm1d"
    },
    {
        "in_text": "What is used to calculate the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "p-norm"
    },
    {
        "in_text": "Global Hooks For Module Registers a forward pre-hook common to all modules. Registers a what?Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().add_images method  Add batched image data to summary.Note that this requires the pillow package.nn.Softsign Applies the element-wise function:",
        "out_text": "global forward hook"
    },
    {
        "in_text": "What is ordered from left to right according to when each was sampled?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "Indices"
    },
    {
        "in_text": "What do sparse.mm perform a matrix multiplication of?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "out_text": "sparse COO matrix mat1 and a strided matrix mat2"
    },
    {
        "in_text": "What is the Bartlett window function?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "full window size"
    },
    {
        "in_text": "What _ Copies the elements fromsrcintoselftensor and returnsself?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "out_text": "Tensor.copy"
    },
    {
        "in_text": "What is the name of the module you need to package a ResNet fromtorchvision?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "torchvision.models.resnet"
    },
    {
        "in_text": "What is the name of the package that implements various optimization algorithms?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "usetorch.optim"
    },
    {
        "in_text": "What does max_memory_allocated return?Alias for torch.linalg.matrix_power()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "tensors"
    },
    {
        "in_text": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace).This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode.torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. WarningThis is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf.",
        "out_text": "ONNX"
    },
    {
        "in_text": "What is provided as the last input in the tuple args in such cases?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteFillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "empty dictionary"
    },
    {
        "in_text": "What creates a one-dimensional tensor of size steps?vander Generates a Vandermonde matrix .xlogy Computesinput*log(other)with the following cases.Tensor.topk Seetorch.topk()",
        "out_text": "logspace"
    },
    {
        "in_text": "The input must be either a what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "in_text": "What makes it really easy to bind objects and run code at module-level scope?Q: Does ONNX support implicit scalar datatype casting?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources of",
        "out_text": "Python"
    },
    {
        "in_text": "What is this function not defined?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "fortorch.cuda.Tensoryet"
    },
    {
        "in_text": "What computes a solution to the least squares problem of a system of linear equations?Q: Does ONNX support implicit scalar datatype casting?vander Generates a Vandermonde matrix .TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "lstsq"
    },
    {
        "in_text": "The loop within the body ofloop_in_traced_fndepends on what of the inputx?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "shape"
    },
    {
        "in_text": "What is a fractionalmaxPool2d when trying to differentiate?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "a CUDA tensor torch"
    },
    {
        "in_text": "What library does PyTorch use to load and preprocess data from a simple dataset?Q: Does ONNX support implicit scalar datatype casting?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "out_text": "torchaudio library"
    },
    {
        "in_text": "What is an end-to-end example of converting a PyTorch model to TorchScript and running it in C++?Q: Does ONNX support implicit scalar datatype casting?Tensor.bool self.bool()is equivalent toself.to(torch.bool).Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "out_text": "Loading a PyTorch Model in C++tutorial"
    },
    {
        "in_text": "Inputitextinput_iinputi, the first parameter, is what -coordinate?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": "y"
    },
    {
        "in_text": "What dimension is the discrete Fourier transform of realinput?Q: Does ONNX support implicit scalar datatype casting?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "N-dimensional"
    },
    {
        "in_text": "What is the default value of this attribute?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "None by default"
    },
    {
        "in_text": "What type of annealing can you use instead of linear annealing?poisson_nll_loss Poisson negative log likelihood loss.A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):Linux (ppc64le) GPU \u2014  \u2014",
        "out_text": "cosine"
    },
    {
        "in_text": "What is the name of the file that is used by Alias?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias for torch.linalg.matrix_power()",
        "out_text": "Alias of torch.vstack()"
    },
    {
        "in_text": "What is considered non-dispatchable?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "exactTensors andParameters"
    },
    {
        "in_text": "Alias for torch.asinh(). Returns a new tensor with the what of the elements ofinPossible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.Tensor.arccosh acosh() -> Tensorpin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "out_text": "arctangent"
    },
    {
        "in_text": "What allows us to chain together multiple modules?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "theSequentialmodule"
    },
    {
        "in_text": "Sets whether PyTorch operations must use what?Moreover, as forgather(), the values ofindexmust beMoreover, as forgather(), the values ofindexmust beLUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "out_text": "PyTorch operations must use \u201cdeterministic\u201d algorithms"
    },
    {
        "in_text": "What can tracing and scripting be composed to?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "suit the particular requirements of a part of a model"
    },
    {
        "in_text": "What is Tensor.flipud?Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note>>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "out_text": "Seetorch.flipud"
    },
    {
        "in_text": "What does Tensor.values do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.topk Seetorch.topk()",
        "out_text": "Return the values tensor of asparse COO tensor"
    },
    {
        "in_text": "pdist Computes the distance between every pair of row vectors in the input?This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Notenn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "out_text": "p-norm"
    },
    {
        "in_text": "What is reps treated as if input has shape and reps is 2?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteNot providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "out_text": "(1, 1, 2, 2)."
    },
    {
        "in_text": "If win_length is None (default), what value is taken?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Notef = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "n_fft"
    },
    {
        "in_text": "How does Splitsinput create a new tensor?Q: Does ONNX support implicit scalar datatype casting?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "horizontally stacking"
    },
    {
        "in_text": "What does this class store?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "one or more measurements of a given statement"
    },
    {
        "in_text": "How to use torch.std_mean, give an example?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std_mean(a, unbiased=False)\n(tensor(0.4188), tensor(-0.8509))"
    },
    {
        "in_text": "The last dimension of the input tensor is expected to represent what?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeneg Returns a new tensor with the negative of the elements of input.",
        "out_text": "real and imaginary components of complex numbers"
    },
    {
        "in_text": "The torch package contains data structures for what?nn.Threshold Thresholds each element of the input Tensor.slogdet Alias for torch.linalg.slogdet()whereNNNis the full window size.",
        "out_text": "multi-dimensional tensors"
    },
    {
        "in_text": "What is the torch.nn.AdaptiveAvgPool2d when attempting to differentiate?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What operation will act deterministically when mode=True?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseIfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "torch.nn.Conv1d"
    },
    {
        "in_text": "What is a positive Seetorch?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "Tensor"
    },
    {
        "in_text": "What type of torch does atorch.Tensor have?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "double torch"
    },
    {
        "in_text": "The number of bins is one larger than the largest value in input unlessinput is empty, in which case the result is a tenBefore dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect. def forward(self, x):\n    a = x + 1\n    return x + self.attr_1Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements",
        "out_text": "size 1"
    },
    {
        "in_text": "What is represented with no error after quantization?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "zero"
    },
    {
        "in_text": "What does Manual Automatic Support for Customization Limited Support support?Q: Does ONNX support implicit scalar datatype casting?Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b) and vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first parameter, is the y-coordinate.)Support for Customization Limited Support Fully Supported",
        "out_text": "Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization"
    },
    {
        "in_text": "What does emit_nvtx append to ease correlating each backward-pass op with the corresponding forward-pass onn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "sequence number information"
    },
    {
        "in_text": "What is a float32 quantized 4-bit integer stored as?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "8-bit signed integer"
    },
    {
        "in_text": "The default schedule simply records all the events continuously for what?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.Boolean torch.bool torch.*.BoolTensor",
        "out_text": "the duration of the context manager"
    },
    {
        "in_text": "The sparse matrix-vector multiplication is the only math operation supported on what?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "CSR tensors"
    },
    {
        "in_text": "What section in the documentation will we check first?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Limitations of Symbolic Tracing"
    },
    {
        "in_text": "If there is more than one item, it should be passed in tuple format, e.g. what = (x, yallow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument.",
        "out_text": "example_outputs"
    },
    {
        "in_text": "What type of convolution is applied to an input signal composed of several input planes?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "out_text": "1D convolution"
    },
    {
        "in_text": "What does Compilesfn do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Forces completion of atorch.jit.Future[T]asynchronous task"
    },
    {
        "in_text": "What is another name for modules that lazily initialize parameters?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "lazy modules"
    },
    {
        "in_text": "Reorders n-dimensional FFT data, as provided byfftn(), to have what first?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "negative frequency terms"
    },
    {
        "in_text": "What type of distribution does fillselftensor with elements drawn from with torch.Tensor.geometric_?f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Tensor.logical_not_ In-place version oflogical_not()Tensor.logical_xor_ In-place version oflogical_xor()",
        "out_text": "geometric distribution"
    },
    {
        "in_text": "What is the name of the output tensor that has fewer dimension(s) than input?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "orlen(dim)"
    },
    {
        "in_text": "What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "a tensor"
    },
    {
        "in_text": "What is torch.qfint32?Boolean torch.bool torch.*.BoolTensorPyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nametorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "32-bit integer"
    },
    {
        "in_text": "What does Computes the exponential of the elements minus 1 of input?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "in_text": "What criterion uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.removenn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.",
        "out_text": "SmoothL1Loss"
    },
    {
        "in_text": "What happens if atol(Optional[float]) is omitted?Q: Does ONNX support implicit scalar datatype casting?Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning",
        "out_text": "default values based on the dtype are selected"
    },
    {
        "in_text": "What must be passed in for properties?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "their__get__method"
    },
    {
        "in_text": "Insert a Which Node into the Graph?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "call_method"
    },
    {
        "in_text": "What is the default setting for the returned Tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "torch.strided"
    },
    {
        "in_text": "What is a worker_init_fncannot be an unpicklable object?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "lambda function"
    },
    {
        "in_text": "fftn Computes the N dimensional discrete what?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "Fourier transform"
    },
    {
        "in_text": "What does torch.acosh() do?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Adds the scalar other to each element of the input input"
    },
    {
        "in_text": "When import statements are encountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same walking waynn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1LossPadding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2dadd_custom_scalars method  Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once\nfor each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called\nbefore or after the training loop.",
        "out_text": "AST"
    },
    {
        "in_text": "AST parsing has limited support for what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "the__import__(...)syntax"
    },
    {
        "in_text": "What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "64-bit"
    },
    {
        "in_text": "Absolute tolerance must also be specified if what must also be specified?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "specified rtol"
    },
    {
        "in_text": "What is the name of the file you want to debug?Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. WarningTo achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.",
        "out_text": "python -m pdb FILENAME.py"
    },
    {
        "in_text": "When should bool Return the Graph underlying this GraphModule be called?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "after editing the contained graph"
    },
    {
        "in_text": "profileNoteTo compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.Stable",
        "out_text": "CPU-only"
    },
    {
        "in_text": "What is _1 linear?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "call_module linear"
    },
    {
        "in_text": "What does broadcast the given tensors according to?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "Broadcasting semantics"
    },
    {
        "in_text": "What is another name for a tuple of Tensors?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "output"
    },
    {
        "in_text": "How to use To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:, give an example?To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict:The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. WarningAveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running: >>> swa_model = AveragedModel(model)",
        "out_text": "class DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')"
    },
    {
        "in_text": "The prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing what (currently unTensor.asin_ In-place version ofasin()Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:set_device Sets the current device.",
        "out_text": "specifiedamountof"
    },
    {
        "in_text": "What is the Generator state returned as?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "a torch.ByteTensor"
    },
    {
        "in_text": "What does a linear system of equations with a positive semidefinite matrix need to be inverted given?xlogy Computesinput*log(other)with the following cases.Tensor.log10 Seetorch.log10()To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "Cholesky factor matrix uuu"
    },
    {
        "in_text": "What does self do to unused submodules?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Deletes all unused submodules"
    },
    {
        "in_text": "When dim is squeezed, the outputs tensor has how many dimensions less than input?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "1 fewer dimension"
    },
    {
        "in_text": "What does 3D fractional max pooling over an input signal composed of several input planes?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "2D fractional max pooling"
    },
    {
        "in_text": "If actual and expected are  close, they are considered close if both their real and imaginary components are considered close according to the definition above.Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you\u2019re operating under heavy memory pressure, you might never need to use them.This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "complex-valued"
    },
    {
        "in_text": "What type of discussion of PyTorch's advanced features can be found in the links below?pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.Features described in this documentation are classified by release statusFeatures described in this documentation are classified by release status",
        "out_text": "In-depth"
    },
    {
        "in_text": "What is module that implements the versions of the fused operations needed for quantization aware training?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "torch.nn.intrinsic.qat"
    },
    {
        "in_text": "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of whatSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeSeetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining",
        "out_text": "1"
    },
    {
        "in_text": "Where are constants saved?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "the code of the model"
    },
    {
        "in_text": "What does Tensor.logit Seetorch.logit do?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "Tensor.logit Seetorch.logit()"
    },
    {
        "in_text": "What is a name that becomestorch_package_0>.torchvision.models.resnet18?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Alias for torch.linalg.matrix_power()Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State",
        "out_text": "liketorchvision.models.resnet18"
    },
    {
        "in_text": "What did the call to print(traced) show in our transforms?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "an error"
    },
    {
        "in_text": "What will replace some of the ops that have all constant inputs, with pre-computed constant nodes?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "Constant-folding optimization"
    },
    {
        "in_text": "What is the argument diagonal controls?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "If diagonal = 0, it is the main diagonal"
    },
    {
        "in_text": "What is the name of the variable that is used when a sample index is drawn for a row?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Note"
    },
    {
        "in_text": "What is an example of how to train a neural network?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "Recursivelyapply()a function"
    },
    {
        "in_text": "A dictionary that maps what in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally returntorch.autograd a tape-based automatic differentiation library that supports all differentiable Tensor operations in torchExports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "out_text": "overridable functions"
    },
    {
        "in_text": "What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. NoteResult from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "out_text": "Simple Custom Module Modules"
    },
    {
        "in_text": "What is the In-place version of add() Tensor?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "add"
    },
    {
        "in_text": "What is returned in a sparse tensor self?Q: Does ONNX support implicit scalar datatype casting?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "number of dense dimensions"
    },
    {
        "in_text": "What does a Quantized Tensor allow for?Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()Tensor.ravel seetorch.ravel()",
        "out_text": "serialization of data in a quantized format"
    },
    {
        "in_text": "What is the term for a dense layout?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Onlytorch.strided"
    },
    {
        "in_text": "Which criterion optimizes a multi-label one-versus-all loss based on max-entropy?Q: Does ONNX support implicit scalar datatype casting?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "out_text": "nn.CosineEmbeddingLoss"
    },
    {
        "in_text": "For acquiring the COO format data of an what type of tensor, usetorch.Tensor._valuesWarning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.",
        "out_text": "uncoalesced"
    },
    {
        "in_text": "Clips gradient of an iterable of parameters at what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "specified value"
    },
    {
        "in_text": "What would transform an object into a parameter?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "parameterizations"
    },
    {
        "in_text": "What error occurs if corresponding tensors do not have the same dtype?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "If Check_dtype"
    },
    {
        "in_text": "What is another term for a tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "a tuple of tensors"
    },
    {
        "in_text": "What does saving and loading tensors do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Saves an object to a disk file"
    },
    {
        "in_text": "What is a tensor of eigenvalues?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "tensor of eigenvalues"
    },
    {
        "in_text": "Fills the 2-dimensional inputTensorwith what?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "identity matrix"
    },
    {
        "in_text": "What removes the spectral normalization reparameterization from a module?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "remove_spectral_norm"
    },
    {
        "in_text": "What holds the return value of the callback and will be marked as completed when the given callback finishes?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteYou can usetorch.manual_seed()to seed the RNG for all devices (bothQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "A new Future object"
    },
    {
        "in_text": "What does the script assign the output to a (unique) value namedrv.1?Q: Does ONNX support implicit scalar datatype casting?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "%rv.1:Tensormeans"
    },
    {
        "in_text": "Param is what type of parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform distribution?Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "optional"
    },
    {
        "in_text": "what computes the entropy on input  elementwise?Q: Does ONNX support implicit scalar datatype casting?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "out_text": "torch.special.entr"
    },
    {
        "in_text": "Is providing a value forsteps deprecated?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "out_text": "Not providing a value for steps is deprecated"
    },
    {
        "in_text": "What does lerp base its interpolation of two tensors on?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.input(Tensor) \u2013 the input tensor. Exampleis_storage Returns True if obj is  a PyTorch storage object.",
        "out_text": "a scalar or tensor weight"
    },
    {
        "in_text": "What does nn.AvgPool2d Apply over an input signal composed of several input planes?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "2D average pooling"
    },
    {
        "in_text": "Computes the factorization of a matrix or batches of matrices A. Returns the LU solve of the linear system AxComputes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "out_text": "LU"
    },
    {
        "in_text": "What is in-place version ofacos?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.acos"
    },
    {
        "in_text": "What is the default branch of github?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "local"
    },
    {
        "in_text": "What does torch.onnx.export do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "torch.onnx.export"
    },
    {
        "in_text": "What is the returned window if window_lengthis one?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "a single element tensor containing a one"
    },
    {
        "in_text": "What type does the dtype refer to in case actual and expected do not have the same dtype?Q: Does ONNX support implicit scalar datatype casting?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "out_text": "promoted type"
    },
    {
        "in_text": "Returns the indices of what to which each value in the input belongs?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "buckets"
    },
    {
        "in_text": "What element does PyTorch hybrid COO tensor have?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "out_text": "typetorch.int64"
    },
    {
        "in_text": "What is the in-place version of Randomized leaky ReLU?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "rrelu()"
    },
    {
        "in_text": "What is rrelu_ In-place version ofrrelu()?Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *argshardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().",
        "out_text": "rrelu_ In-place version ofrrelu()"
    },
    {
        "in_text": "What are the inputs supported by the LOBPCG method?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Random sampling creation ops are listed underRandom samplingand",
        "out_text": "dense, sparse, and batches of dense matrices"
    },
    {
        "in_text": "What does compiled_with_cxx11_abi return if PyTorch was built with?method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "out_text": "_GLIBCXX_USE_CXX11_ABI=1"
    },
    {
        "in_text": "What is the term for quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "dynamic quantization"
    },
    {
        "in_text": "What is the second input tensor dim(int,optional)?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "other(Tensor)"
    },
    {
        "in_text": "How to use torch.sort, give an example?n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013ger Alias of torch.outer().ger Alias of torch.outer().",
        "out_text": ">>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n>>> x = torch.tensor([0, 1] * 9)\n>>> x.sort()\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n>>> x.sort(stable=True)\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))"
    },
    {
        "in_text": "What does torch.optim implement that updates the parameters?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "a step()method"
    },
    {
        "in_text": "What was the title of the IEEE Trans. ASSP?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "vol.32, no.2, pp.236-243"
    },
    {
        "in_text": "What preserves views for more details?triplet_margin_with_distance_loss SeeTripletMarginWithDistanceLossfor details.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "See Saving and loading tensors"
    },
    {
        "in_text": "What is geqrf?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "low-level function"
    },
    {
        "in_text": "For what dtype is the dtype of output uint8 itself in torch.all?Q: Does ONNX support implicit scalar datatype casting?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "uint8"
    },
    {
        "in_text": "How does this tutorial introduce the fundamental concepts of PyTorch?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "self-contained examples"
    },
    {
        "in_text": "What type of method is supported by dense, sparse, and batches of dense matrices?xlogy Computesinput*log(other)with the following cases.The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in VariableType.h, because dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator. We try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator. We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override; in VariableType.h. This means elu is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We add the following lines to symbolic_opset9.py:The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "out_text": "robust"
    },
    {
        "in_text": "What is the outer product of input andvec2?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "Outer product of input andvec2"
    },
    {
        "in_text": "What are same in torch.baddbmm as the scaling factors used in torch.addbmm()?Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.less_equal Alias for torch.le().Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "out_text": "alpha and beta"
    },
    {
        "in_text": "What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?Q: Does ONNX support implicit scalar datatype casting?Tensor.logical_xor_ In-place version oflogical_xor()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "NHWC"
    },
    {
        "in_text": "What are theksmallest elements returned?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "IflargestisFalsethen"
    },
    {
        "in_text": "Static control flow arises for code making decisions about a model's architecture based on what?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "hyper-parameters"
    },
    {
        "in_text": "What is the size ofinput?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "input(Tensor)"
    },
    {
        "in_text": "Computes the first kind of what function for each element of input?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "in_text": "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionalroot (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModuleOptimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dictCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "out_text": "-1"
    },
    {
        "in_text": "What should you do to your sparse tensors to prevent them from growing too large?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "coalesce"
    },
    {
        "in_text": "What is the default element type for index tensors?Q: Does ONNX support implicit scalar datatype casting?torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.Tensor.logical_xor Seetorch.logical_xor()",
        "out_text": "torch.int64"
    },
    {
        "in_text": "What does this class wrap around an arbitraryoptim.Optimizer?hardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "shards"
    },
    {
        "in_text": "What is the In-place version ofbitwise_xor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Tensor.bitwise_xor"
    },
    {
        "in_text": "What is the front-end to?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "LOBPCG"
    },
    {
        "in_text": "What is the reality if your script spends most of its time executing on the GPU?NoteIt is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "much more complicated"
    },
    {
        "in_text": "What is the name of the module we define for demonstration purposes?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "MyModule"
    },
    {
        "in_text": "How to use torch.vsplit, give an example?PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameBoolean torch.bool torch.*.BoolTensornn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).",
        "out_text": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))"
    },
    {
        "in_text": "What is an example of a mechanism that can enable or disable gradients locally?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "set_grad_enabled."
    },
    {
        "in_text": "What methods now work on tensors with the same method names?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "var.backward(), var.detach(), var.register_hook()"
    },
    {
        "in_text": "The pivots returned by the function are what?Fillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "1-indexed"
    },
    {
        "in_text": "What does the input tensor ignore?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "the median of the values ininput"
    },
    {
        "in_text": "Any TorchScript program can be saved from what process?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Python"
    },
    {
        "in_text": "How to use torch.fmin, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": ">>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])"
    },
    {
        "in_text": "What is the benefit of adding export support for operators?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "advance usage"
    },
    {
        "in_text": "What is the name of the function that returns the counts for each unique element?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "return_counts"
    },
    {
        "in_text": "What is the default value of progress(bool,optional)?Alias fortorch.linalg.pinv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr"
    },
    {
        "in_text": "How to use torch.unique_consecutive, give an example?Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": ">>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])"
    },
    {
        "in_text": "What is the most likely skew for bottom most events?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013Note",
        "out_text": "negligible"
    },
    {
        "in_text": "What is the name of the plugin that allows profiling with Kineto profiler?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?multilabel_margin_loss SeeMultiLabelMarginLossfor details.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "use_kineto"
    },
    {
        "in_text": "What happens if a module is intern-ed?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "it will be placed into the package"
    },
    {
        "in_text": "What type of data is stored in extern_modules?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "serialized tensor data"
    },
    {
        "in_text": "What blog post provides a more comprehensive overview of the tradeoffs between quantization types?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Pytorch"
    },
    {
        "in_text": "Manipulation is generally done using what class?Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTruea dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation",
        "out_text": "FunctionCounts"
    },
    {
        "in_text": "Converts a float tensor to what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "out_text": "a per-channel quantized tensor"
    },
    {
        "in_text": "What is an example of auxiliary tools to make the user workflow smoother?Q: Does ONNX support implicit scalar datatype casting?If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "tokenizers"
    },
    {
        "in_text": "Randomized leaky ReLU.Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".rrelu Randomized leaky ReLU.Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "out_text": "In-place version ofrrelu()"
    },
    {
        "in_text": "What type of integer is the torch?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "64-bit"
    },
    {
        "in_text": "What is the benefit of per channel quantization?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "lesser error"
    },
    {
        "in_text": "How are the options configured?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "the constructor arguments of aDataLoader"
    },
    {
        "in_text": "What does get_rng_state_all return?Alias for torch.linalg.matrix_power()Alias fortorch.linalg.pinv()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "a list of ByteTensor"
    },
    {
        "in_text": "What does function._ContextMethodMixin.set_ do?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "materialize_grads"
    },
    {
        "in_text": "Indices are ordered from left to right according to when each was sampled.Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "first samples are placed in first column"
    },
    {
        "in_text": "Aboolthat controls whether what is enabled. Returns a bool indicating if CUDNN is currently available.is_available Returns a bool indicating if CUDA is currently available.ignore This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "out_text": "cuDNN"
    },
    {
        "in_text": "Where does PyTorch transfer AlexNet from PyTorch to?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "ONNX"
    },
    {
        "in_text": "What does Seetorch.nanquantile do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.nanquantile"
    },
    {
        "in_text": "Is not providing a value forsteps deprecated?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "Warning Not providing a value for steps is deprecated"
    },
    {
        "in_text": "What are the indices of specified elements collected?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Random sampling creation ops are listed underRandom samplingand",
        "out_text": "inindicestensor"
    },
    {
        "in_text": "What must the rows of inputdo be?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "non-negative, finite and have a non-zero sum"
    },
    {
        "in_text": "What can sometimes be tricky?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Training neural networks"
    },
    {
        "in_text": "What is returned when a library is compiled for CUDA?Alias fortorch.linalg.pinv()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "out_text": "Gets the cuda capability of a device"
    },
    {
        "in_text": "How long is the value held by this Future blocked?NoteTo achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.",
        "out_text": "until the value of this Future is ready"
    },
    {
        "in_text": "What algorithm implements stochastic gradient descent?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "resilient backpropagation algorithm"
    },
    {
        "in_text": "How are sparse COO tensors implemented?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "by simply concatenating the indices and values tensors"
    },
    {
        "in_text": "Is each element ofinput infinite or negative infinity?Tensor.swapaxes Seetorch.swapaxes()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References>>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "out_text": "infinite"
    },
    {
        "in_text": "Operator implementations currently only support what for weights of the conv and linear operators?xlogy Computesinput*log(other)with the following cases.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "per channel quantization"
    },
    {
        "in_text": "Why are many topics in this note elaborated on in other notes or tutorials?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "modules are so fundamental to PyTorch"
    },
    {
        "in_text": "What type of video learn how to augment your network using a visual attention mechanism?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "Image/Video"
    },
    {
        "in_text": "The LU factorization does have backward support, but only for square inputs of what?Q: Does ONNX support implicit scalar datatype casting?nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.",
        "out_text": "full rank"
    },
    {
        "in_text": "Computes the what of a square matrix for an integer n?API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.>>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "out_text": "n-th power"
    },
    {
        "in_text": "Depending on model structure, the numeric differences in PyTorch and ONNX backends may be what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "negligible"
    },
    {
        "in_text": "What is the key to debugging?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "work backwards"
    },
    {
        "in_text": "What is the name of the number of rows in a 2-D tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "n(int)"
    },
    {
        "in_text": "Random sampling creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN modelsOptimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. WarningSupports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. >>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])",
        "out_text": "Random sampling creation ops"
    },
    {
        "in_text": "What type of gradients computed via small finite differences against analytical gradients w.r.t?nn.KLDivLoss The Kullback-Leibler divergence loss measurethe indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. NoteQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "Check gradients"
    },
    {
        "in_text": "What is one way to split up large files with unrelated functionality?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wisepoisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "Split up large files with unrelated functionality into smaller ones"
    },
    {
        "in_text": "What applies Layer Normalization for last certain number of dimensions?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "layer_norm"
    },
    {
        "in_text": "X must be a what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "dense tensor"
    },
    {
        "in_text": "How to use A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Tensor.logical_or_ In-place version oflogical_or()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "out_text": ">>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')"
    },
    {
        "in_text": "What helps you debug by making it easier to see if a package is referring to a package or not?poisson_nll_loss Poisson negative log likelihood loss.A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):Linux (ppc64le) GPU \u2014  \u2014",
        "out_text": "stack traces and print statements"
    },
    {
        "in_text": "What type of hook does torch.Tensor.register_hook register?Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.Alias fortorch.linalg.pinv()",
        "out_text": "backward hook"
    },
    {
        "in_text": "What does torch. *_like tensor do?n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013ger Alias of torch.outer().ger Alias of torch.outer().",
        "out_text": "creation ops"
    },
    {
        "in_text": "Rotate a n-D Rotate a n-D Rotate a n-D Rotate a n-We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNormParameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "out_text": "Rotate a n-D"
    },
    {
        "in_text": "What does running the forward pass with detection enabled do?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "will allow the backward pass to print the traceback of the forward operation that created the failing backward function"
    },
    {
        "in_text": "What is function used by addmv()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Tensor.addmv_ In-place version ofaddmv()"
    },
    {
        "in_text": "Using the profiler\u2019sschedule,on_trace_readyandstepfunctions adds a user defined metadata with whatYou can usetorch.manual_seed()to seed the RNG for all devices (bothrepo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: TrueDoing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.",
        "out_text": "a string key and a string value"
    },
    {
        "in_text": "What is returned when the log of summed exponentials of each row of theinputtensor is returned?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "the mean value of all elements in theinputtensor"
    },
    {
        "in_text": "How to use torch.swapdims, give an example?negative Alias for torch.neg()negative Alias for torch.neg()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])"
    },
    {
        "in_text": "What does M[sparse_coo]@M[strided]->M[sparse_coo]Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo]Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "out_text": "no"
    },
    {
        "in_text": "What does resize self to the desired size and the number of sparse and dense dimensions?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the conditionWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "Removes all specified elements"
    },
    {
        "in_text": "s.values().layout == torch.strided - values are stored as what?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Textnn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1).",
        "out_text": "strided tensors"
    },
    {
        "in_text": "What is Seetorch.lgamma?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.lgamma"
    },
    {
        "in_text": "full_like Returns a what with the same size as input filled with fill_value?The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:ldexp Multiplies input by 2**:attr:other.To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones",
        "out_text": "tensor"
    },
    {
        "in_text": "What is the name of the package that Torch.packagefinds your code's dependencies?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "torch.packageFormat"
    },
    {
        "in_text": "Tensor.is_floating_point Returns what if the data type ofselfis a floating point data type?Tensor.is_floating_point Returns True if the data type ofselfis a floating point data type.The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator eluis_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.",
        "out_text": "True"
    },
    {
        "in_text": "How to use torch.diagonal, give an example?ger Alias of torch.outer().ger Alias of torch.outer().n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "out_text": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])"
    },
    {
        "in_text": "Tracing of control flow that is dependent on inputs (e.g. what) Tracing of in-place operations of tensAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.Divides each element of the inputinputby the corresponding element ofother. Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:Divides each element of the inputinputby the corresponding element ofother. Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "out_text": "tensor shapes"
    },
    {
        "in_text": "What does.mv() no M[sparse_csr] @ V[strided] -> V[Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces.torch.mv() no M[sparse_coo]@V[strided]->V[strided]Tensor.masked_scatter_ Copies elements fromsourceintoselftensor at positions where themaskis True.",
        "out_text": "M[sparse_coo] @ V[strided] -> V[strided] torch"
    },
    {
        "in_text": "When you calltorch.jit.script, compilation is what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "\u201copt-out\u201d, rather than \u201copt-in\u201d"
    },
    {
        "in_text": "What is the name of the function that computesinput>other text input> text other inputotherWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "Alias for torch.gt()"
    },
    {
        "in_text": "What does torch.as_tensor() use if you want to avoid a copy of data?Alias for torch.linalg.matrix_power()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()",
        "out_text": "NumPy ndarray"
    },
    {
        "in_text": "What does activities(iterable) list of to use in profiling?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "activity groups"
    },
    {
        "in_text": "What is the inverse hyperbolic cosine of the elements of input?increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:IfupperisFalse, the returned matrixLis lower-triangular, andComputes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.",
        "out_text": "arcsine"
    },
    {
        "in_text": "What does the dependency resolver mark allGLOBALops as?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "dependencies of your pickled object"
    },
    {
        "in_text": "What is the default value of force_reload?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Alias for torch.linalg.matrix_power()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "True"
    },
    {
        "in_text": "What is added to the Tensor.absoluteTensor.less lt(other) -> TensorTensor.less lt(other) -> Tensorp(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "out_text": "a scalar or tensor to self tensor"
    },
    {
        "in_text": "What does Tensor.expm1_ In-place version ofexpm1() do?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Tensor.logical_xor_ In-place version oflogical_xor()",
        "out_text": "Tensor.expm1_ In-place version ofexpm1()"
    },
    {
        "in_text": "If the absolute element-wise error falls below delta, what is a squared term?std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "out_text": "if the absolute element-wise error falls below delta"
    },
    {
        "in_text": "What is the name of the tensor that is different from numpy.repeat?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in orderAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.",
        "out_text": "torch.Tensor.repeat()"
    },
    {
        "in_text": "What is Tensor.nansum?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Seetorch.nansum"
    },
    {
        "in_text": "What is DataLoader and other utility function ?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "torch.utils DataLoader"
    },
    {
        "in_text": "Exporter does not support conversion of models with what input?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "String inputs"
    },
    {
        "in_text": "What Examples Implement a function with checks for__torch_function__overrides?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "Dict[Callable, Callable]"
    },
    {
        "in_text": "What specifies what Tensors should be optimized?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "params(iterable)"
    },
    {
        "in_text": "What type of intthat shows the number of plans currently in the cuFFT plan cache?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliwhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "readonly"
    },
    {
        "in_text": "Who returns a new tensor with the inverse hyperbolic sine of the elements of input?Computes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum andComputes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum andTraining Functions Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX.  It runs a single round of inference and then saves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet).  The keyword argument verbose=True causes the exporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library. You can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime, you will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which were actually run during this run.  This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won\u2019t be accurate.  Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.)  We recommend examining the model trace and making sure the traced operators look reasonable.  If your model contains control flows like for loops and if conditions, trace-based exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run.  If you want to export your model with dynamic control flows, you will need to use the script-based exporter.",
        "out_text": "Alias for torch.asin()"
    },
    {
        "in_text": "What Sets the learning rate of each parameter group to the initial tensors?In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function ExamplesTorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.Tensor.logical_xor Seetorch.logical_xor()",
        "out_text": "lr_scheduler.LambdaLR"
    },
    {
        "in_text": "What does an n-dimensionaltorch.Tensor Examples Fill?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "2-dimensional inputTensorwith the identity matrix"
    },
    {
        "in_text": "What do you need to do to avoid a copy of a Tensordata?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "change itsrequires_gradflag, userequires_grad_()ordetach()"
    },
    {
        "in_text": "What is a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConPyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')Tensor.expm1_ In-place version ofexpm1()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "nn.LazyConv2d"
    },
    {
        "in_text": "Training (enum, default) \u2013 TrainingMode.EVAL: export the model in inference mode in what mode?When a subclass is used with DataLoader, each\nitem in the dataset will be yielded from the DataLoader\niterator. When num_workers > 0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers. get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s __iter__() method or the DataLoader \u2018s\nworker_init_fn option to modify each copy\u2019s behavior. >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]smm Performs a matrix  multiplication of the sparse matrix input with the dense matrix mat.Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "out_text": "TrainingMode.EVAL"
    },
    {
        "in_text": "What does Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containingFor an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. NoteTensor.new_zeros Returns a Tensor of  size filled with0.",
        "out_text": "real values of the self tensor"
    },
    {
        "in_text": "What are two examples of current valid scalar and tensor combination?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination arelr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "out_text": "Currently valid scalar and tensor combination"
    },
    {
        "in_text": "What is a built-in method sum...> (relu_1)?Tensor.logical_xor_ In-place version oflogical_xor()Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "call_function sum_1"
    },
    {
        "in_text": "What is the In-place version of Tensor.add?Tensor.logical_xor_ In-place version oflogical_xor()Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "out_text": "add() Tensor"
    },
    {
        "in_text": "What Computes the one dimensional Fourier transform of real-valuedinput?Q: Does ONNX support implicit scalar datatype casting?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "rfft"
    },
    {
        "in_text": "What is added to the self tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "a scalar or tensor to self tensor"
    },
    {
        "in_text": "AvgPool3d when attempting to differentiate what?Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What is onesided(Optional[bool])?Alias fortorch.linalg.pinv()Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "out_text": "Whether the STFT was onesided"
    },
    {
        "in_text": "What is used to visualize data and model training?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "TensorBoard"
    },
    {
        "in_text": "How to use torch.linalg.householder_product, give an example?dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:scatter_add Out-of-place version of torch.Tensor.scatter_add_()nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).",
        "out_text": ">>> a = torch.randn(2, 2)\n>>> h, tau = torch.geqrf(a)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\nTrue\n\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> q\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)"
    },
    {
        "in_text": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the inverse ofrfft(). Complu Computes the LU factorization of a matrix  or batches of matricesA.Tensor.argmin Seetorch.argmin()version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "out_text": "Computes the N dimensional inverse discrete Fourier transform ofinput"
    },
    {
        "in_text": "What product is added to the matrixinput?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "outer-product of vectorsvec1andvec2"
    },
    {
        "in_text": "What is the path to a local folder to save downloaded models & weights?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "d(string)"
    },
    {
        "in_text": "What is another name for a 2D transposed convolution operator over an input image composed of several input planes?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "deconvolution"
    },
    {
        "in_text": "What is the tensor of size end start step+1 left l floor fractext end?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "out_text": "1-D"
    },
    {
        "in_text": "What is an example of a (Tensor, Tensor)?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "out_text": "Examples"
    },
    {
        "in_text": "What Computes a matrix  norm?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "matrix _norm"
    },
    {
        "in_text": "What type of training can be tricky?It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "Neural network training"
    },
    {
        "in_text": "How to use torch.utils.model_zoo.load_url, give an example?torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensorFunctions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "out_text": ">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')"
    },
    {
        "in_text": "What will be run when theFutureis completed?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliIfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "Append the given callback function to thisFuture"
    },
    {
        "in_text": "What does the new usage look like?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "@ignoredcannot be exported;@unusedcan"
    },
    {
        "in_text": "What Alias for abs() Tensor.absolute_ In-place version of absolute()?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "abs() Tensor.absolute"
    },
    {
        "in_text": "For more information on thetorch.dtype,torch.device, and torch.layout attributes of what>>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "out_text": "a torch.Tensor"
    },
    {
        "in_text": "The layout of a torch.packagefile is identical to what?Returns a new tensor with a dimension of size one inserted at theCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "Pythonregular package"
    },
    {
        "in_text": "What does _to_vector Convert parameters to one vector?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "parameters"
    },
    {
        "in_text": "What is an example of a Graph rewrite?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "deleting or appending nodes"
    },
    {
        "in_text": "Along with Eager Mode Quantization, what other mode does PyTorch provide?Q: Does ONNX support implicit scalar datatype casting?timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "out_text": "FX Graph Mode Quantization"
    },
    {
        "in_text": "What does vsplit Split input's tensors vertically vary according to?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "indices_or_sections"
    },
    {
        "in_text": "The future returned by fut.wait() will be marked appropriately with what?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "out_text": "the encountered error"
    },
    {
        "in_text": "What is a handle that can be used to remove the added hook by?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "callinghandle.remove()"
    },
    {
        "in_text": "What can be multiplied byUandV?Alias fortorch.linalg.pinv()IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "arbitrary phase factor"
    },
    {
        "in_text": "What does the checkpointed part do instead of storing all intermediate activations of the entire computation graph for computing backward?Q: Does ONNX support implicit scalar datatype casting?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]",
        "out_text": "the checkpointed part does not save intermediate activations"
    },
    {
        "in_text": "How do you load a model from a github repo?Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.Tensor.frac_ In-place version offrac()hardtanh_ In-place version ofhardtanh().",
        "out_text": "a local directory"
    },
    {
        "in_text": "What type of integer is intTensor?Q: Does ONNX support implicit scalar datatype casting?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "64-bit"
    },
    {
        "in_text": "What is used to save stacks file to this location?torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensorFunctions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "out_text": "path(str)"
    },
    {
        "in_text": "Does the stashing logic have a way to anticipate if the user will move Tensors to a new device within the run_fThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself"
    },
    {
        "in_text": "What is a squeeze operation done only in the given dimension?If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. WarningComputes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "out_text": "Note"
    },
    {
        "in_text": "What is sigmoid?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Alias for torch.special.expit()"
    },
    {
        "in_text": "How do I know if a package is executing?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Test in my source code"
    },
    {
        "in_text": "At what level is distributed data parallelism based?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "module level"
    },
    {
        "in_text": "What is returned with each of the elements of inputconverted from angles in degrees to radians?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "a new tensor"
    },
    {
        "in_text": "How to use torch.diag, give an example?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])"
    },
    {
        "in_text": "What computes the matrix  exponential of a square matrix  or of each square matrix  in a batch?Fillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "out_text": "matrix _exp"
    },
    {
        "in_text": "What is the name for torch.asin()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "Alias"
    },
    {
        "in_text": "What can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensorIn addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagationa dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue",
        "out_text": "A tensor of specific data type"
    },
    {
        "in_text": "What is the name of the technique used in the API?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "static quantization"
    },
    {
        "in_text": "What happens if an external library changes in a backwards-incompatible way?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "your package may fail to load"
    },
    {
        "in_text": "What is the cosine similarity computed along?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "dim"
    },
    {
        "in_text": "What is required when accessing and using the values, as long as one doesn't change streams?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "No further synchronization"
    },
    {
        "in_text": "Debugging withpdbworks except for what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "when we invoke the@torch.jit.scriptfunction"
    },
    {
        "in_text": "What is replaced by each element in the tensor?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "the value returned bycallable"
    },
    {
        "in_text": "Where can verbose print information about dependency resolution?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "stdout"
    },
    {
        "in_text": "BuildExtension is allowed to supply a dictionary for what?PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameBoolean torch.bool torch.*.BoolTensorTensor.xlogy_ In-place version ofxlogy()",
        "out_text": "extra_compile_args"
    },
    {
        "in_text": "What allows one to see which dimensions have been used under the hood and further group by them?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "prof.key_averages"
    },
    {
        "in_text": "What is the default value of dynamic_axes?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "out_text": "empty"
    },
    {
        "in_text": "What function returns a tensor with all the dimensions of inputof size1removed?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "Alias for torch.transpose()"
    },
    {
        "in_text": "What does this function support backward for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "sparse matrix mat1"
    },
    {
        "in_text": "What is the action that removes or changes dependencies in your code that is not technically part oftorch.package?Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column. >>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])",
        "out_text": "Refactoring"
    },
    {
        "in_text": "What is the name of the tensor cores that can be used in cuDNN convolutions on Ampere devices?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "SeeTensorFloat-32(TF32)"
    },
    {
        "in_text": "What is start(float)?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "starting value for the set of points end(float)"
    },
    {
        "in_text": "How to use One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls.We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below., give an example?Supports input of float, double, cfloat and cdouble dtypes.extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)"
    },
    {
        "in_text": "What is another name for nn.Dropout2d?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "nn.Dropout3d"
    },
    {
        "in_text": "What is another name for a sequential container?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "nn.LazyConv1d"
    },
    {
        "in_text": "What can change if you pass a new input tensor to the traced function?poisson_nll_loss Poisson negative log likelihood loss.A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):Linux (ppc64le) GPU \u2014  \u2014",
        "out_text": "x"
    },
    {
        "in_text": "What is the TorchScript conatiner type refinement?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "a pass-through function that returnsvalue"
    },
    {
        "in_text": "What is built-in method topk?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "call_function topk_1"
    },
    {
        "in_text": "Where can you find details on the use of the C++ tensor indexing API?tensorsolve Computes the solutionXto the systemtorch.tensordot(A, X) = B.record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameThe.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "out_text": "pytorch.org/cppdocs/notes/tensor_indexing.html"
    },
    {
        "in_text": "What is the default value of the atorch.Tensor?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "1e-15"
    },
    {
        "in_text": "Parameters are contained within what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "thestate_dict"
    },
    {
        "in_text": "What does the dependency resolver need to do to process unused imports?Q: Does ONNX support implicit scalar datatype casting?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "out_text": "Qualify your imports"
    },
    {
        "in_text": "How to use torch.vdot, give an example?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])"
    },
    {
        "in_text": "What mode is implemented at the moment?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "CPU-only"
    },
    {
        "in_text": "What does from_numpy create a Tensor from?Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "out_text": "a numpy.ndarray"
    },
    {
        "in_text": "What is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule?torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensorFunctions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "out_text": "2.torch.jit.script(nn_module_instance)"
    },
    {
        "in_text": "What is the summation subscript in Einsum astorch.einsum?Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.Tensor.argmax Seetorch.argmax()Tensor.get_device For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.",
        "out_text": "j"
    },
    {
        "in_text": "What does tensor fill the 2-dimensional inputTensor with?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "the identity matrix"
    },
    {
        "in_text": "Tensor.dim returns the number of dimensions of what?Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin).Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor. >>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrueReturns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax).",
        "out_text": "selftensor"
    },
    {
        "in_text": "At what rate do prune.RandomStructured prune entire channels in a tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "random"
    },
    {
        "in_text": "cholesky_ex Computes the Cholesky decomposition of a complex what?Tensor.asinh Seetorch.asinh()Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace fileExample Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "out_text": "Hermitian"
    },
    {
        "in_text": "OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fallWrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor.nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "out_text": "custom ONNX op"
    },
    {
        "in_text": "What does the script run and then saves the resulting traced model to alexnet?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "single round of inference"
    },
    {
        "in_text": "What is Seetorch.cumsum?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.cumsum"
    },
    {
        "in_text": "What type of PyTorch code examples are included in this quickstart guide?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "Bite-size, ready-to-deploy PyTorch code examples"
    },
    {
        "in_text": "The closure that allows them to reevaluate the function multiple times should clear the gradients, what should the closure do?Q: Does ONNX support implicit scalar datatype casting?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "compute the loss"
    },
    {
        "in_text": "What is the absolute ?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Alias for torch.abs()"
    },
    {
        "in_text": "What will enable or disable the autograd anomaly detection based on its argument mode?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "set_detect_anomaly"
    },
    {
        "in_text": "Where are the unpruned units in prune.RandomUnstructured Prune located?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "a tensor at random"
    },
    {
        "in_text": "What is equivalent to calling torch.kaiser_window?Alias for torch.linalg.matrix_power()Alias fortorch.linalg.pinv()If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "out_text": "Calling torch.kaiser_window"
    },
    {
        "in_text": "What is the name of the Alias for?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "torch.special.expm1"
    },
    {
        "in_text": "What type of tensor does ifobj return True?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "PyTorch tensor"
    },
    {
        "in_text": "Sets the gradients of all optimizedtorch.Tensors to zero. Implements Adadelta algorithm. Implements Admemory_summary Returns a human-readable printout of the current memory allocator statistics for a given device.Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.kaiser_window Computes the Kaiser window with window length window_length and shape parameter beta.",
        "out_text": "Optimizer.zero_grad"
    },
    {
        "in_text": "What does register on the exporter?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "an extern hook"
    },
    {
        "in_text": "What is equivalent to torch.full?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.ger Alias of torch.outer().",
        "out_text": "torch.full_like"
    },
    {
        "in_text": "What is the name of the add_1 function?Note\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "call_module linear_1 linear"
    },
    {
        "in_text": "What is NLLLoss a criterion that measures the Binary Cross Entropy between the target and the output?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "negative log likelihood loss"
    },
    {
        "in_text": "What types of models can use dynamic quantization?Q: Does ONNX support implicit scalar datatype casting?Tensor.sign_ In-place version ofsign()While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear()",
        "out_text": "LSTM and Transformer type models with small batch size"
    },
    {
        "in_text": "What would we still like to capture builtin Python functions in?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "symbolic tracing"
    },
    {
        "in_text": "What does current implementation oftorch.Tensor introduce?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "memory overhead"
    },
    {
        "in_text": "What is the current state of the generator?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "a torch.ByteTensor"
    },
    {
        "in_text": "If target is not a valid target, the module will not be deleted?Q: Does ONNX support implicit scalar datatype casting?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "out_text": "if target is not a valid target"
    },
    {
        "in_text": "What does it do when a generator is seeded?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "Sets the Generator state"
    },
    {
        "in_text": "What type of attribute does register_buffer correspond to?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.atan2_ In-place version ofatan2()A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported.",
        "out_text": "typeTensor"
    },
    {
        "in_text": "How to use torch.log10, give an example?negative Alias for torch.neg()negative Alias for torch.neg()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])"
    },
    {
        "in_text": "Which operations are often slower than nondeterministic operations?Q: Does ONNX support implicit scalar datatype casting?poisson_nll_loss Poisson negative log likelihood loss.A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "out_text": "Deterministic operations"
    },
    {
        "in_text": "What does false onesided(bool,optional) avoid for real inputs?Q: Does ONNX support implicit scalar datatype casting?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Tensor.isneginf Seetorch.isneginf()",
        "out_text": "redundancy"
    },
    {
        "in_text": "What does Torch Script do to a package?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Patch code"
    },
    {
        "in_text": "What type of exporter will unroll loops and if conditions?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "trace-based exporter"
    },
    {
        "in_text": "If what must also be specified, default values based on the dtype are selected with the below table. atol(Optional[float]torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.You can usetorch.manual_seed()to seed the RNG for all devices (bothCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "out_text": "specifiedatol"
    },
    {
        "in_text": "If a type cannot be inferred and is not explicitly annotated, it will not be added as what to the resultingScriptModulNote thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft.Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.",
        "out_text": "an attribute"
    },
    {
        "in_text": "What can happen to an otherwise complete batch?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "broken into multiple ones"
    },
    {
        "in_text": "What does torch.Tensor.detach_ Detaches the Tensor from the graph that created it?Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "out_text": "leaf"
    },
    {
        "in_text": "SVD(M[sparse_coo])->M[strided],M[strided],MPython Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo]repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "out_text": "yes"
    },
    {
        "in_text": "The inverse matrix exists if and only ifAAAisinvertible. What is the inverse matrix?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.is_storage Returns True if obj is  a PyTorch storage object.The resulting out tensor shares its underlying storage with the\ninput tensor, so changing the content of one would change the content\nof the other. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])",
        "out_text": "unique"
    },
    {
        "in_text": "What is version ofaddcdiv()?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "Tensor.addcdiv_ In-place"
    },
    {
        "in_text": "What does Stack tensors in sequence depthwise gather?xlogy Computesinput*log(other)with the following cases.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "Gathers values along an axis specified bydim"
    },
    {
        "in_text": "Image/Video Learn to load and preprocess data from a simple dataset with what?torch.mm() no M[sparse_coo]@M[strided]->M[strided]Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "out_text": "PyTorch's torchaudio library"
    },
    {
        "in_text": "What does Atorch.ByteTensor do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Sets the seed for generating random numbers"
    },
    {
        "in_text": "If what is not specified, tolis the threshold below which the singular values (or the eigenvalues whensymmetricisTrue) areHowever, there are some steps you can take to limit the number of sources ofFillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution:The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "out_text": "Iftolis"
    },
    {
        "in_text": "Pytorch Hub is a pre-trained model repository designed to facilitate what?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "research reproducibility"
    },
    {
        "in_text": "Replace some required modules with what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "mock implementation"
    },
    {
        "in_text": "How do you add a symbolic function to a PyTorch Function class?Tensor.frexp Seetorch.frexp()Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Create a symbolic function named symbolic"
    },
    {
        "in_text": "Where are values selected?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "indices"
    },
    {
        "in_text": "What is registered for all modules?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dTensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "global forward hook"
    },
    {
        "in_text": "What type of models are LSTM and dynamic quantization used for?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "Transformer type models with small batch size"
    },
    {
        "in_text": "Who supports specifying per-parameter options?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Optimizers"
    },
    {
        "in_text": "What is conv_transpose3d?Alias for torch.linalg.matrix_power()No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "deconvolution"
    },
    {
        "in_text": "What returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted?The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifReturns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": "LU factorization of A fromtorch.lu()"
    },
    {
        "in_text": "What converts a tensor to compressed row storage format?Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Q: Does ONNX support implicit scalar datatype casting?add_images method  Add batched image data to summary.Note that this requires the pillow package.",
        "out_text": "Tensor._to_sparse_csr"
    },
    {
        "in_text": "What is a file-like object that has to implement read(), readline(), tell(), and seek()?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "f"
    },
    {
        "in_text": "What returns a new tensor which indexes the input tensor along dimension dim using the entries inindex?64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensorTensor.geometric_ Fillsselftensor with elements drawn from the geometric distribution:Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos()",
        "out_text": "index_select"
    },
    {
        "in_text": "What is the lowest unit in the prune tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "L1-norm"
    },
    {
        "in_text": "What is In-place version ofarctanh?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.arctanh"
    },
    {
        "in_text": "What is the scalar multiplier for other?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "the second input tensor alpha"
    },
    {
        "in_text": "What is Tensor.bitwise_xor?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "Seetorch"
    },
    {
        "in_text": "What can symbolic tracing be used in isolation for?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wiseApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "to capture a form of the code for analysis (and not transformation) purposes"
    },
    {
        "in_text": "max Returns the maximum value of all elements in what?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "the input tensor"
    },
    {
        "in_text": "What is the periodic argument intended for?It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "a helpful shorthand"
    },
    {
        "in_text": "What is the name of the parameter which is a dictionary with ThreeJS classes names and configuration?Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()",
        "out_text": "config_dict"
    },
    {
        "in_text": "What supports a limited subset of data manipulation methods of the regular full-precision tensor?Q: Does ONNX support implicit scalar datatype casting?atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "Quantized Tensors"
    },
    {
        "in_text": "What is used to override the default behavior of Pytorch?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "TORCH_CUDA_ARCH_LIST"
    },
    {
        "in_text": "What is the corresponding kwargs for callablemodel?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.triplet_margin_with_distance_loss SeeTripletMarginWithDistanceLossfor details.",
        "out_text": "kwargs"
    },
    {
        "in_text": "for the dim  parameter to torch.trapz- By default, what is the dimension along which to integrate?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "the last dimension"
    },
    {
        "in_text": "The low-rank SVD is useful for what type of matrices that torch.linalg.svd() cannot handle?Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "huge sparse matrices"
    },
    {
        "in_text": "How are hstack Stack tensors sequenced?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "horizontally"
    },
    {
        "in_text": "What does this method return?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "True"
    },
    {
        "in_text": "What should not matter if your bottlenecks result in code much slower than the CUDA startup time?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "if your bottlenecks result in code much slower than the CUDA startup time"
    },
    {
        "in_text": "What does the function do for each element of input?Q: Does ONNX support implicit scalar datatype casting?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind"
    },
    {
        "in_text": "What will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "boolean optionsortedIf True"
    },
    {
        "in_text": "What controls the order of multiplication?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "left(bool)"
    },
    {
        "in_text": "How  Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__., give an example?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": ">>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue"
    },
    {
        "in_text": "What are some of the fused operations implemented by torch.nn.intrinsic.quantized?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?Tensor.topk Seetorch.topk()",
        "out_text": "conv + relu"
    },
    {
        "in_text": "What is the tensor of eigenvalues of size(,k)(*, k)(,k)submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method as Graph.create_node(). Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensorsFor example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)",
        "out_text": "X"
    },
    {
        "in_text": "What is Tensor.trace?Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in orderTensor.flip Seetorch.flip()",
        "out_text": "Seetorch.trace"
    },
    {
        "in_text": "What is version offix()?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.fix_ In-place"
    },
    {
        "in_text": "How to use torch.addcmul, give an example?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])"
    },
    {
        "in_text": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?randint Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "out_text": "exclusive"
    },
    {
        "in_text": "What returns the smallest element of each row of the inputtensor in the given dimensiondim?atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "a namedtuple"
    },
    {
        "in_text": "How is the hardswish function described in the paper?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "element-wise"
    },
    {
        "in_text": "What is a sparse COO tensor a instance of?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "torch.Tensor"
    },
    {
        "in_text": "Neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with?input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None.Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected):",
        "out_text": "None"
    },
    {
        "in_text": "What styleclass annotations are used for empty container types?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "PEP 526"
    },
    {
        "in_text": "What is the consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "out_text": "at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes"
    },
    {
        "in_text": "What does it do to the learning rate of each parameter group by gamma every epoch?use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "Decays the learning rate of each parameter group by gamma every epoch"
    },
    {
        "in_text": "What does nn.TripletMarginLoss create?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "a criterion"
    },
    {
        "in_text": "What does the intermediate representation of the GraphModule constitute?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources ofThe value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "Python-to-Python transformation pipeline"
    },
    {
        "in_text": "What is.abs Seetorch.abs()?Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:Tensor.ger Seetorch.ger()The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "out_text": "Tensor"
    },
    {
        "in_text": "What does solve This function return the solution to?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.",
        "out_text": "the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A"
    },
    {
        "in_text": "What is the name of the variant oftorch.quantile() that \"ignores\"NaNvalues?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "documentation fortorch.quantile()"
    },
    {
        "in_text": "What does nn.PixelUnshuffle do to reverse thePixelShuffleoperation?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "rearranging elements in a tensor of shape"
    },
    {
        "in_text": "When is a backward through output operation stable?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "when all eigenvalues are distinct"
    },
    {
        "in_text": "What is an n-dimensionaltorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor"
    },
    {
        "in_text": "What is the default name for how repo_or_dir is to be interpreted?Tensor.logical_xor_ In-place version oflogical_xor()Tensor.neg_ In-place version ofneg()Tensor.tril_ In-place version oftril()",
        "out_text": "github"
    },
    {
        "in_text": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides?schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. NoteTensor.arctanh Seetorch.arctanh()To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones",
        "out_text": "padded"
    },
    {
        "in_text": "How many tutorials are there?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "three"
    },
    {
        "in_text": "How to remove a tensor dimension?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "use unbind"
    },
    {
        "in_text": "Options as keyword arguments will be used as what?Q: Does ONNX support implicit scalar datatype casting?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "defaults"
    },
    {
        "in_text": "What program uses routinegesddon earlier versions of CUDA?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "MAGMA"
    },
    {
        "in_text": "What does torch.get_default_dtype() use when both start and end are real?Alias for torch.linalg.matrix_power()Q: Does ONNX support implicit scalar datatype casting?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "global default dtype"
    },
    {
        "in_text": "What APIs are crucial for building dynamic neural networks in C++ frontend?Q: Does ONNX support implicit scalar datatype casting?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "tensor autograd"
    },
    {
        "in_text": "Returns what of the values in input , ignoringNaN values?A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works. WarningTests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument.",
        "out_text": "the median"
    },
    {
        "in_text": "What type of pooling does nn.MaxPool3d apply over an input signal composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "out_text": "2D max pooling"
    },
    {
        "in_text": "What is a quickstart guide to building a complete ML workflow with PyTorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "A step-by-step guide to building a complete ML workflow"
    },
    {
        "in_text": "What is layout(torch.layout, optional)?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "currently only supporttorch.strided"
    },
    {
        "in_text": "What is applied to a parameter in the given module?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "spectral normalization"
    },
    {
        "in_text": "cosh Returns a new tensor with what type of cosine?For example, ifinputis of shapepow Takes the power of each element in inputwithexponentand returns a tensor with the result.Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "out_text": "hyperbolic"
    },
    {
        "in_text": "What is the mini-batch of 2D inputs with additional channel dimension?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "4D input"
    },
    {
        "in_text": "What is the tensor with the arctangent of the elements of input?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "inverse hyperbolic tangent"
    },
    {
        "in_text": "Ifinputhas shape andrepsis are treated as what?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Random sampling creation ops are listed underRandom samplingand",
        "out_text": "(1, 1, 2, 2)."
    },
    {
        "in_text": "What is a common error case?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "UnicodeDecodeError"
    },
    {
        "in_text": "What can you depend on without having to package them?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "third-party libraries"
    },
    {
        "in_text": "How many times is an id guaranteed to be given out for a package?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "once"
    },
    {
        "in_text": "What is the optional setup code used to define variables?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "instmt global_setup"
    },
    {
        "in_text": "Whendimis given, what operation is done only in the given dimension?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "squeeze operation"
    },
    {
        "in_text": "What is the name of the function that loads a PyTorch C++ extension just-in-time?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "load_inline()"
    },
    {
        "in_text": "What is the latest version of tensor indexing in PyTorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "opset"
    },
    {
        "in_text": "What is the output tuple of tensors?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "out(tuple,optional)"
    },
    {
        "in_text": "Force_reload(bool,optional) \u2013 whether to force a fresh download of the github repo unconditionally. Whatenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "Does not have any effect if source='local'"
    },
    {
        "in_text": "What can you do to report runtime of PyTorch functions?If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. WarningIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "wrap any code into it"
    },
    {
        "in_text": "What does a Graphmodule contain?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dwhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Warning"
    },
    {
        "in_text": "What is compiled by default?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.negative Alias for torch.neg()",
        "out_text": "module\u2019sforward"
    },
    {
        "in_text": "Where is the file named test.py located?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "line 9"
    },
    {
        "in_text": "What is built with OpenMP support?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "PyTorch"
    },
    {
        "in_text": "Concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized whatTo package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "out_text": "enable_cpatching"
    },
    {
        "in_text": "What does nn.ConvTranspose1d Apply over an input image composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "1D transposed convolution operator"
    },
    {
        "in_text": "The index tensors crow_indices and col_indices should have element type either what?Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrueYou can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "out_text": "torch.int64"
    },
    {
        "in_text": "What is the replace_pattern API used for?If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "Basic usage Quantization Invert Transformation"
    },
    {
        "in_text": "What does this function provide in TorchScript?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "conatiner type refinement"
    },
    {
        "in_text": "nn.ConvTranspose3d Applies what transposed convolution operator over an input image composed of several input planes?optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizerWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "3D"
    },
    {
        "in_text": "What type of reparametrization does prune.customFromMask apply?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "pruning"
    },
    {
        "in_text": "What does Alias fortorch.acosh() perform?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "the element-wise multiplication"
    },
    {
        "in_text": "What must EXACTLY match the names in forward?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "Parameter names"
    },
    {
        "in_text": "What would a case in which dictionary input is the last input of the args tuple cause when a dictionary of named parameters iscallback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NotewhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "a conflict"
    },
    {
        "in_text": "Ops that do synchronize appear to be what under regular CPU-mode profilers?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "Ops that do synchronize appear to be extremely expensive"
    },
    {
        "in_text": "What type of algorithms does _algorithms_enabled() and torch.use_deterministic_algorithdtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:scatter_add Out-of-place version of torch.Tensor.scatter_add_()nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).",
        "out_text": "deterministic"
    },
    {
        "in_text": "What does Tensor.type_as return?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "the type of the given tensor"
    },
    {
        "in_text": "What is the goal of debugging if tracing is working as expected?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "GraphModule transformation"
    },
    {
        "in_text": "What applies a 1D transposed convolution operator over an input image composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.Fillsselftensor with elements drawn from the geometric distribution:",
        "out_text": "nn.ConvTranspose1d"
    },
    {
        "in_text": "What does PerformsLpL_pLp normalization of inputs over specified dimension?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the conditionThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "normalize"
    },
    {
        "in_text": "What is necessary to ensure when preparing a quantized model?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "qconfig and the engine used for quantized computations match the backend on which the model will be executed"
    },
    {
        "in_text": "What are the elements of input converted from?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. NoteComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "angles in degrees to radians"
    },
    {
        "in_text": "What does the user need to do in addition to Specify where activations are quantized and de-quantized?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "static quantization techniques"
    },
    {
        "in_text": "Each entrypoint is defined as what type of function?Q: Does ONNX support implicit scalar datatype casting?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "python"
    },
    {
        "in_text": "What is the name of a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelstorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "out_text": "nn.LazyConvTranspose1d"
    },
    {
        "in_text": "What function saves given tensors for a future call to backward()?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "backward"
    },
    {
        "in_text": "In what program is emit_nvtx used?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "Nvidia Visual Profiler"
    },
    {
        "in_text": "What is torch.nn.NLLLoss when called on?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What is Tensor.atan Seetorch.atan?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Tensor.atan Seetorch.atan"
    },
    {
        "in_text": "What is done only in the given dimension whendimis given?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "out_text": "squeeze operation"
    },
    {
        "in_text": "What defines computation of both eigenvectors and eigenvalues?Q: Does ONNX support implicit scalar datatype casting?TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.Tensor.logical_xor Seetorch.logical_xor()",
        "out_text": "eigenvectors"
    },
    {
        "in_text": "What does flip tensor in the left/right direction return?Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "a new tensor"
    },
    {
        "in_text": "If window is None (default), it is treated as if having what everywhere in the window?windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:cholesky_inverse Computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu: returns matric inv.",
        "out_text": "111"
    },
    {
        "in_text": "To what direction does function._ContextMethodMixin.save_for_backward save given tensorTensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "out_text": "tobackward"
    },
    {
        "in_text": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped. orthomethod=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.Tensor.ceil_ In-place version ofceil()rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "out_text": "force_stop"
    },
    {
        "in_text": "How does the module use self(i.e. thatPackage Importerinstance) to fulfill future import requests?Q: Does ONNX support implicit scalar datatype casting?allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "out_text": "looking in the package"
    },
    {
        "in_text": "What are the elements of in out at the  given?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "indices"
    },
    {
        "in_text": "How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors:do not convert to numpy types:, give an example?niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,opset_version (int, default is 9) \u2013 by default we export the model to the opset version of the onnx submodule. Since ONNX\u2019s latest opset may evolve before next stable release, by default we export to one stable opset version. Right now, supported stable opset version is 9. The opset_version must be _onnx_main_opset or in _onnx_stable_opsets which are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes.",
        "out_text": "y = x.astype(np.int)"
    },
    {
        "in_text": "Where does a function compute the dot product between a vector v and the Jacobian of a given function?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeWarning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "out_text": "the point given by the inputs"
    },
    {
        "in_text": "Atol(float,optional) \u2013 what is input(Tensor) a second tensor to compare?After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "absolute tolerance"
    },
    {
        "in_text": "What type of tensor does quantize_per_tensor convert to?xlogy Computesinput*log(other)with the following cases.f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:Tensor.logical_not_ In-place version oflogical_not()",
        "out_text": "float tensor"
    },
    {
        "in_text": "How many epochs does the model train for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "300"
    },
    {
        "in_text": "What does prune.is_pruned do to a parameter in a given module?Q: Does ONNX support implicit scalar datatype casting?Tensor.logical_xor_ In-place version oflogical_xor()To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "Applies weight normalization"
    },
    {
        "in_text": "What is used to register both CPU and GPU activity?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "emit_nvtx"
    },
    {
        "in_text": "Performs what division of oftensor1bytensor2?relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "element-wise division"
    },
    {
        "in_text": "Computes the what of the gamma function on input?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "logarithmic derivative"
    },
    {
        "in_text": "What is the value that we assign the output to?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "ofTensortype"
    },
    {
        "in_text": "If input is divisible by n along dimension dim, each section will be of equal size, what is the result?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "input.size(dim) / n"
    },
    {
        "in_text": "What is considered equal to each other whenequal_nanis True?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsAlias fortorch.linalg.pinv()",
        "out_text": "NaNs"
    },
    {
        "in_text": "What is the one dimensional Fourier transform of real-valuedinput?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.xlogy Computesinput*log(other)with the following cases.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "N dimensional inverse discrete Fourier transform ofinput"
    },
    {
        "in_text": "What will be forwarded to the given optimizer?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dpoisson_nll_loss Poisson negative log likelihood loss.A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "out_text": "all trailing arguments"
    },
    {
        "in_text": "What function returns the LU solve of the linear system Ax=bAx = b using the partially pivoted LU factorization of AThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details. >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "out_text": "torch.lu()"
    },
    {
        "in_text": "What function provides for conatiner type refinement in TorchScript?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "isinstance"
    },
    {
        "in_text": "What is the default for ONNX's latest opset?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?triplet_margin_with_distance_loss SeeTripletMarginWithDistanceLossfor details.whereNNNis the full window size.",
        "out_text": "one stable opset version"
    },
    {
        "in_text": "What type of wildcard matches against zero or more complete segments?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "double wildcard"
    },
    {
        "in_text": "How to use torch.logical_or, give an example?torch.mm() no M[sparse_coo]@M[strided]->M[strided]Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "out_text": ">>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])"
    },
    {
        "in_text": "What is the second dimension to be transposed?Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "dim1(int)"
    },
    {
        "in_text": "How to use torch.nanquantile, give an example?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": ">>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])"
    },
    {
        "in_text": "What is a callable that returns an iterable of Tensor-likes?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "dispatcher(Callable)"
    },
    {
        "in_text": "What does the Tensor.col_indices return whenselfis a sparse CSR tensor of layoutsparsWarning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Tensor.addmm_ In-place version ofaddmm()A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "out_text": "column indices of theselftensor"
    },
    {
        "in_text": "What can be used to automate the process of rewriting a graph?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "Proxy objects"
    },
    {
        "in_text": "What kind of distribution is the fillsselftensor drawn from?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.topk Seetorch.topk()",
        "out_text": "geometric distribution"
    },
    {
        "in_text": "Out (Tensor, optional) - what is used to calculate the standard deviation of all elements in the input tensor?Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "out_text": "output tensor"
    },
    {
        "in_text": "What is a hard matter to support in autograd?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "in-place operations"
    },
    {
        "in_text": "Register_buffer is equivalent to what type of typeTensor?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "attribute"
    },
    {
        "in_text": "What type of Tensor returns a Tensor?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "size filled"
    },
    {
        "in_text": "What requires gradients to be computed for?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "Tensor"
    },
    {
        "in_text": "What does a first model need to see?NotewhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "what is inside a package"
    },
    {
        "in_text": "Return the tensor of a sparse COO tensor. Tensor.values Return the values tensTensor.uniform_ Fillsselftensor with numbers sampled from the continuous uniform distribution:Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself.Tensor.index_put_ Puts values from the tensorvaluesinto the tensorselfusing the indices specified inindices(which is a tuple of Tensors).",
        "out_text": "indices"
    },
    {
        "in_text": "What does The Folder object do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "The Folder object itself is directly printable and will print out a file tree representation"
    },
    {
        "in_text": "What is rrelu?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Randomized leaky ReLU"
    },
    {
        "in_text": "What program supports both CPU and CUDA?Q: Does ONNX support implicit scalar datatype casting?multilabel_margin_loss SeeMultiLabelMarginLossfor details.1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]",
        "out_text": "FakeQuantize"
    },
    {
        "in_text": "If your use case is always what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "1-D sorted sequence"
    },
    {
        "in_text": "What does the default gain forSELUsacrifice for more stable gradient flow in rectangular layers?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "the default gain forSELUsacrifices the normalisation effect"
    },
    {
        "in_text": "What Computes a partial inverse ofMaxPool3d?Q: Does ONNX support implicit scalar datatype casting?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "nn.MaxUnpool3d"
    },
    {
        "in_text": "When you issue what call,Package Exporter will pickle the object normally?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "a save_pickle"
    },
    {
        "in_text": "What parameter is given by the corresponding element ininputi?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "out_text": "rate parameter given by the corresponding element ininputi"
    },
    {
        "in_text": "What is returned when the log of summed exponentials of each row of theinputtensor in the given dimensiondim is returned?The computation for determinant and inverse of covariance matrix is avoided when\ncov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and\nmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factorReverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2doptimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "out_text": "the mean value of all elements in theinputtensor"
    },
    {
        "in_text": "What do you need to know about the second value returned bytorch.sort()?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "semantics"
    },
    {
        "in_text": "Return the number of sparse dimensions in what?Fillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "a sparse tensorself"
    },
    {
        "in_text": "nvprof basedtorch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Support for Customization Limited Support Fully SupportedElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b) and vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first parameter, is the y-coordinate.)",
        "out_text": "emit_nvtx"
    },
    {
        "in_text": "What is the lowest value of the prune.l1_unstructured Prunes tensor?atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "L1-norm"
    },
    {
        "in_text": "What is the residual tolerance for stopping criterion?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "tol(float,optional)"
    },
    {
        "in_text": "Wheninput is on the CPU, the implementation of torch.sinh may use what library?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulYes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "Sleef library"
    },
    {
        "in_text": "How many epochs does the SWA model train?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "300"
    },
    {
        "in_text": "what function is useful in statistics?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "torch.logaddexp. "
    },
    {
        "in_text": "What specifies an entrypoint forresnet18model if we expand the implementation inpytorch/vision/hubconf.pTensor.addbmm_ In-place version ofaddbmm()Tensor.addmm_ In-place version ofaddmm()Tensor.ldexp_ In-place version ofldexp()",
        "out_text": "snippet"
    },
    {
        "in_text": "What does rearranging elements in a tensor of shape(,C,Hr,Wr)(*,The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (ifReturns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": "Reverses thePixelShuffleoperation"
    },
    {
        "in_text": "What is In-place version of hardtanh()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "hardtanh"
    },
    {
        "in_text": "What is another name for torch.add()?If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "torch.mul()"
    },
    {
        "in_text": "The submodule itself; the actual object we want to install in the current Module which method to return True, each object in the chain denotedIfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Random sampling creation ops are listed underRandom samplingand",
        "out_text": "m"
    },
    {
        "in_text": "What is the only source module that can be intern-ed?Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimension dim.a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "out_text": "Python"
    },
    {
        "in_text": "What is supported in PyTorch?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "static control flow"
    },
    {
        "in_text": "What object is used to seed a Generator?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "torch.Generator"
    },
    {
        "in_text": "What does torch.set_default_tensor_type() use?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "global default"
    },
    {
        "in_text": "What is a simple custom module?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "Module State Module Hooks Advanced Features"
    },
    {
        "in_text": "What is assumed to be if complex or symmetric if real?Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.",
        "out_text": "Hermitian"
    },
    {
        "in_text": "Atleast_3d Returns a 3-dimensional view of each input tensor with what dimensions?atleast_3d Returns a 3-dimensional view of each input tensor with zero dimensions.nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.atleast_2d Returns a 2-dimensional view of each input tensor with zero dimensions.",
        "out_text": "zero"
    },
    {
        "in_text": "What is Seetorch.floor_divide() function?Alias for torch.linalg.matrix_power()Tensor.floor_ In-place version offloor()fftshift Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.",
        "out_text": "Tensor.floor_divide"
    },
    {
        "in_text": "for torch.max What is the maximum value of each row of the inputtensor in the given dimensiondim?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notetorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "a namedtuple"
    },
    {
        "in_text": "What is inferred from theinput.size(1)?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "thein_channelsargument"
    },
    {
        "in_text": "What is the default to return half of results to avoid redundancy for real inputs?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "False onesided"
    },
    {
        "in_text": "What happens to every consecutive group of equivalent elements in the input tensor?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "Eliminates all but the first element"
    },
    {
        "in_text": "What does new_state(torch.ByteTensor) do?Q: Does ONNX support implicit scalar datatype casting?Alias for torch.linalg.matrix_power()xlogy Computesinput*log(other)with the following cases.",
        "out_text": "Sets the random number generator state"
    },
    {
        "in_text": "What is not smart enough to tell that unused imports are indeed unused?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "The dependency resolver"
    },
    {
        "in_text": "The destination GPU id Defaults to what?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "current device"
    },
    {
        "in_text": "Who calls the method when it encounters an instance of the target class?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "the Package Exporter"
    },
    {
        "in_text": "What is Seetorch.multinomial()?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "Tensor.multinomial"
    },
    {
        "in_text": "What might be artificially increased because of shape collection?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "the total self cpu time"
    },
    {
        "in_text": "What algorithm is implemented?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "resilient backpropagation algorithm"
    },
    {
        "in_text": "What is the name of the dict that contains default values of optimization options?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?Tensor.arctanh Seetorch.arctanh()",
        "out_text": "Optimizer.add_param_group"
    },
    {
        "in_text": "What is the power of a square matrix for an integer n?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.Tensor.log10 Seetorch.log10()",
        "out_text": "n-th power"
    },
    {
        "in_text": "Where are these features sometimes found?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "run-time flags"
    },
    {
        "in_text": "Members: CPU CUDA Returns a callable that can be used as what?Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2: >>> python -m torch.distributed.launch --helptensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examplesenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "profiler schedule argument"
    },
    {
        "in_text": "In the left/right direction, return a new tensor. What does Flip tensor do?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "out_text": "Flip tensor"
    },
    {
        "in_text": "What does the tuple contain when using torch.var_mean?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "variance and mean"
    },
    {
        "in_text": "What is the dot product between a given scalar function and a vector vat?A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).Fillsselftensor with elements drawn from the geometric distribution:Tensor.topk Seetorch.topk()",
        "out_text": "the Hessian"
    },
    {
        "in_text": "What happens if you pass a new input tensor to the traced function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "x can change"
    },
    {
        "in_text": "What does iterable oftorch.Tensors ordicts do?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsequal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "out_text": "Specifies what Tensors should be optimized"
    },
    {
        "in_text": "What is setting that sets memory fraction for a process?Random sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor",
        "out_text": "set_per_process_memory_fraction"
    },
    {
        "in_text": "Where can you find the list of external dependencies for a package?Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.Tensor.tanh_ In-place version oftanh()",
        "out_text": "on package_exporter.extern_modules"
    },
    {
        "in_text": "What is the default value for the lowest integer to be drawn from the distribution?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Tensor.arctanh Seetorch.arctanh()Tensor.log10 Seetorch.log10()",
        "out_text": "0. high(int)"
    },
    {
        "in_text": "What is the name of the broadcast we need all the ranks for?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "insidestep()"
    },
    {
        "in_text": "What is the input(TensororScalar) containing the search value(s)?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.",
        "out_text": "N-D tensor or a Scalar"
    },
    {
        "in_text": "The method to compute the matrix rank is done using what by default?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulTensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "out_text": "SVD"
    },
    {
        "in_text": "What permits uncoalescedsparse tensors?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "PyTorch sparse COO tensor format"
    },
    {
        "in_text": "How many items should be passed as the example output?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "only one item should be passed as the example output"
    },
    {
        "in_text": "Set the result for this Future, which will mark this Future as what?Block until the value of thisFutureis ready.eig Computes the eigenvalue decomposition of a square matrix  if it exists.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "completed"
    },
    {
        "in_text": "What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteQ: Does ONNX support implicit scalar datatype casting?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "Avoid having many 0 bits in the seed"
    },
    {
        "in_text": "What can include(Union[List[str],str]] be?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "out_text": "glob-style pattern"
    },
    {
        "in_text": "What will help you decide if your script is CPU-bound?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "CPU-mode autograd profiler"
    },
    {
        "in_text": "What is the lazy initialization of of theConv3d that is inferred from the input.size(1)?See the Automatic Mixed Precision examples for usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "out_text": "thein_channelsargument"
    },
    {
        "in_text": "What are Arbitrary positional arguments originally passed intopublic_api?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "args"
    },
    {
        "in_text": "nn.LPPool1d Applies what power-average pooling over an input signal composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "1D"
    },
    {
        "in_text": "Pads a packed batch of what?acosh Returns a new tensor with the inverse hyperbolic cosine of the elements of input.To export a raw ir.In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note",
        "out_text": "variable length sequences"
    },
    {
        "in_text": "What is the metric to use for self_cpu_time_total or self_cuda_time_total?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "metric"
    },
    {
        "in_text": "Computes a partial inverse of MaxPool2d. Computes a partial inverse of MaxPoolmemory_summary Returns a human-readable printout of the current memory allocator statistics for a given device.Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.",
        "out_text": "Computes a partial inverse of MaxPool2d"
    },
    {
        "in_text": "What is In-place version oftril()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Tensor.tril_ In-place version oftril()"
    },
    {
        "in_text": "What is version ofldexp()?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "Tensor.ldexp_ In-place"
    },
    {
        "in_text": "What can be done with the combined modules conv + relu?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "quantized"
    },
    {
        "in_text": "Where is the dot product computed between the Jacobian of the given function?Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.For complexA, it returns the angle and the natural logarithm of the modulus of theSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "out_text": "the point given by the inputs and a vector v"
    },
    {
        "in_text": "If certain named argument is not present in the dictionary, it is assigned what?Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "out_text": "default value"
    },
    {
        "in_text": "What is the name of the Prunes tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "prune.ln_structured"
    },
    {
        "in_text": "How do you see what is inside a package?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "Packaging your first model"
    },
    {
        "in_text": "Computes the inverse ofrfft2().Computes the complementary error function ofinput.Computes the complementary error function ofinput.Computes the Cholesky decomposition of a symmetric positive-definite",
        "out_text": "2-dimensional discrete Fourier transform of realinput"
    },
    {
        "in_text": "Where can you find information about other libraries that use random number generators?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "the documentation"
    },
    {
        "in_text": "What is a better way to write import foo and later usefoo.bar.baz?Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:xlogy Computesinput*log(other)with the following cases.Alias fortorch.linalg.pinv()",
        "out_text": "writefromfoo.barimportbaz"
    },
    {
        "in_text": "What can also be a glob-style pattern, as described inmock()?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "include"
    },
    {
        "in_text": "What is the boolean mask mask that returns a new 1-D tensor which indexes the input tensorIfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "out_text": "a Bool Tensor"
    },
    {
        "in_text": "What is a built-in debugger?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "graphical wrapper"
    },
    {
        "in_text": "What does the error indicate you are trying to pass a non-quantized Tensor to?Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) ReferencesTensor.swapaxes Seetorch.swapaxes()Tensor.topk Seetorch.topk()",
        "out_text": "a quantized kernel"
    },
    {
        "in_text": "What is the name of Alias for torch.special.erfinv?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Alias for torch.special.erfinv()"
    },
    {
        "in_text": "What does Alias for torch.acosh do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Adds the scalar other to each element of the input input"
    },
    {
        "in_text": "What is used to specify where activations are quantized and de-quantized?Q: Does ONNX support implicit scalar datatype casting?hardswish Applies the hardswish function, element-wise, as described in the paper:std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "out_text": "QuantStub and DeQuantStub modules"
    },
    {
        "in_text": "What does aScriptModule do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Load aScriptModuleorScriptFunction"
    },
    {
        "in_text": "What does the Graph consist of?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a series of Node s"
    },
    {
        "in_text": "What is the last element of Thecrow_indicestensor?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "the number of non-zeros"
    },
    {
        "in_text": "What type of tracing is used?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Symbolic tracing"
    },
    {
        "in_text": "When is quantization in beta?Q: Does ONNX support implicit scalar datatype casting?Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.IfupperisFalse, the returned matrixLis lower-triangular, and",
        "out_text": "in beta"
    },
    {
        "in_text": "How to use Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\nonly:, give an example?With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": ">>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)"
    },
    {
        "in_text": "When does strip_doc_string (bool, default True) strip the field \"doc_string\" from the exported model?Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Notefrom torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})function._ContextMethodMixin.save_for_backward Saves given tensors for a future call tobackward().",
        "out_text": "if True"
    },
    {
        "in_text": "What algorithm does Adamax implement?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Averaged Stochastic Gradient Descent"
    },
    {
        "in_text": "What is a better way to write import foo?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "writefromfoo.barimportbaz"
    },
    {
        "in_text": "Installs empty Modules where none exist yet if they are subpaths of target. what does this do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "Adds the given submodule to self"
    },
    {
        "in_text": "What does Tensor.pow_ In-place version ofpow() do?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: Truetorch.mm() no M[sparse_coo]@M[strided]->M[strided]",
        "out_text": "Tensor.pow_ In-place version ofpow()"
    },
    {
        "in_text": "What does PyTorch provide for working with C++?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "several features"
    },
    {
        "in_text": "What does eq erf exp expand expand expand_as eye flatten floor floor_divide?multilabel_margin_loss SeeMultiLabelMarginLossfor details.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "div dropout"
    },
    {
        "in_text": "What is used to specify how to remap storage locations?32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensorRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "a function or a dict"
    },
    {
        "in_text": "What is rot90 Seetorch.rot90?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Tensor"
    },
    {
        "in_text": "What is handled for elu operator?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "missing symbolic function"
    },
    {
        "in_text": "nn.RNN Applies a multi-layer Elman RNN withtanhtanhtanhorReLUtorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "input sequence"
    },
    {
        "in_text": "What language implements the equivalent of the torch function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "C++"
    },
    {
        "in_text": "How to use In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "out_text": "dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])"
    },
    {
        "in_text": "In most cases importing the right function is sufficient.input(Tensor) \u2013 the input tensor. ExampleThe resulting out tensor shares its underlying storage with the\ninput tensor, so changing the content of one would change the content\nof the other. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])is_storage Returns True if obj is  a PyTorch storage object.",
        "out_text": "inhubconf.py"
    },
    {
        "in_text": "What is Tensor.bincount?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "out_text": "Seetorch.bincount()"
    },
    {
        "in_text": "What did Appendix Migrate to?NoteComputes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "PyTorch 1.2 Recursive Scripting API References"
    },
    {
        "in_text": "What does a dict contain for all overridable functions?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "dummy overrides"
    },
    {
        "in_text": "What is called if the device ordinal is not present?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "aftertorch.cuda.set_device()"
    },
    {
        "in_text": "imag Returns a new tensor containing what?ger Alias of torch.outer().ger Alias of torch.outer().n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "out_text": "imaginary values of the self tensor"
    },
    {
        "in_text": "Which Pads the input tensor boundaries with a constant value?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Examplepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "nn.ConstantPad3d"
    },
    {
        "in_text": "What type of tensor of sizestepswhose values are evenly spaced from start to end, inclusive?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "one-dimensional"
    },
    {
        "in_text": "What is the tangent of the elements of input?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "hyperbolic"
    },
    {
        "in_text": "What does Seetorch.addcdiv() call?get_rng_state_all Returns a list of ByteTensor representing the random number states of all devices.Tensor.logical_xor Seetorch.logical_xor()torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.",
        "out_text": "Tensor.addcdiv"
    },
    {
        "in_text": "What loss is associated with Poisson distribution of target?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Negative log likelihood loss"
    },
    {
        "in_text": "What is a string of entrypoint name defined in repo's hubconf.py force_reload(bool,optional)Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.Tensor.round_ In-place version ofround()Tensor.fmod_ In-place version offmod()",
        "out_text": "model(string)"
    },
    {
        "in_text": "tanh Returns a new tensor with what element?Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in orderAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.Alias fortorch.linalg.pinv()",
        "out_text": "hyperbolic tangent"
    },
    {
        "in_text": "What is the name of the In-place version of add() Tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "add"
    },
    {
        "in_text": "How to use torch.sqrt, give an example?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])"
    },
    {
        "in_text": "What does setting enabled=False make this context manager a no-op?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteTensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "record_shapes"
    },
    {
        "in_text": "If False, what happens to messages about hitting local caches?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "mute messages about hitting local caches"
    },
    {
        "in_text": "If map_location is a what, it indicates the location where all tensors should be loaded?Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. NoteTensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalescedextern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "out_text": "torch.device object or a string containing a device tag"
    },
    {
        "in_text": "Returns what value of each slice of the input tensor in the given dimension(s)dim?Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).log1p Returns a new tensor with the natural logarithm of (1 +input).log1p Returns a new tensor with the natural logarithm of (1 +input).",
        "out_text": "minimum value"
    },
    {
        "in_text": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is what?Returns a tensor with the same data and number of elements asinput,Returns a tensor with the same data and number of elements asinput,Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.",
        "out_text": "LongTensor"
    },
    {
        "in_text": "Which side of the nvtx range may not match the order in which arguments were passed?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "Python"
    },
    {
        "in_text": "Manual_seed_all Sets the seed for generating what on all GPUs?method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dictTakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "out_text": "random numbers"
    },
    {
        "in_text": "What is another name for Gaussian negative log likelihood loss?triu_indices Returns the indices of the upper triangular part of arowbycolmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.Tensor.cummax Seetorch.cummax()Tensor.topk Seetorch.topk()",
        "out_text": "HingeEmbeddingLoss"
    },
    {
        "in_text": "If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off doing what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "specifying them individually"
    },
    {
        "in_text": "If args is a Tensor, this is equivalent to having called it with what?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "1-ary tuple"
    },
    {
        "in_text": "What does FakeQuantize support both CPU and CUDA?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Note"
    },
    {
        "in_text": "If the value contains tensors that reside on GPUs, what will return True even if the asynchronous kernels that are populBecause FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalescedChecks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "out_text": "Future.done()"
    },
    {
        "in_text": "Why do PyTorch warnings only appear once per process?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "excessive warning information"
    },
    {
        "in_text": "How to use torch.special.erfinv, give an example?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dOf course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Warningenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": ">>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])"
    },
    {
        "in_text": "What is Tensor.fmax Seetorch.fmax()?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "out_text": "Tensor.fmax Seetorch.fmax()"
    },
    {
        "in_text": "What does Tensor.subtract_ In-place version ofsubtract() do?The multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}\u03a3\nor a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121\nor a lower-triangular matrix L\\mathbf{L}L with positive-valued\ndiagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance. >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "Tensor.subtract_ In-place version ofsubtract()"
    },
    {
        "in_text": "Einsum Sums the product of the elements of what dimension specified using a notation based on the Einstein summation convention?xlogy Computesinput*log(other)with the following cases.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "input operands along dimensions"
    },
    {
        "in_text": "What is the default is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.>>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "out_text": "force_reload"
    },
    {
        "in_text": "What does TrainingMode.EVAL stand for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "Training"
    },
    {
        "in_text": "Where can you export your own custom ops implementation?You can usetorch.manual_seed()to seed the RNG for all devices (bothverbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.",
        "out_text": "ONNX"
    },
    {
        "in_text": "What is the name of the GraphModule?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "root\u2019s original name or a name that makes sense within the context of your transform"
    },
    {
        "in_text": "What Solves a system of equations with a triangular coefficient matirx AAA ?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Tanh 53\\frac{5}{3}35\u200b",
        "out_text": "triangular_solve"
    },
    {
        "in_text": "What does reset_peak_memory_stats do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "CUDA memory allocator"
    },
    {
        "in_text": "Set export_params to what if you want to export an untrained model?dot Computes the dot product of two 1D tensors.verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "out_text": "False"
    },
    {
        "in_text": "Extra_include_paths - optional list of what to forward to the build?input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). WarningRe-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "out_text": "include directories"
    },
    {
        "in_text": "What does eachPackage Importerinstance create?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).whereNNNis the full window size.",
        "out_text": "Modules in a package can only import other packaged modules, or modules markedextern"
    },
    {
        "in_text": "What does thetorch.packageimport use to import externalmodules?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliAlias for torch.linalg.matrix_power()",
        "out_text": "the loading environment\u2019s system importer"
    },
    {
        "in_text": "What returns theksmallest elements of the given input tensor?torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "IflargestisFalsethen"
    },
    {
        "in_text": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what happens?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "the argument has no effect"
    },
    {
        "in_text": "What type of object does obj have to implement write and flush?Q: Does ONNX support implicit scalar datatype casting?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "a file-like object"
    },
    {
        "in_text": "What does Seetorch.lu_solve() do?Alias for torch.linalg.matrix_power()xlogy Computesinput*log(other)with the following cases.Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "Tensor.lu_solve"
    },
    {
        "in_text": "What is used to construct the input and do preprocessing?Q: Does ONNX support implicit scalar datatype casting?Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.ModuleEager Mode Quantization FX Graph Mode Quantization",
        "out_text": "C++ Tensor API"
    },
    {
        "in_text": "Tests if each element of input is what?Tensor.isneginf Seetorch.isneginf()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "negative infinity"
    },
    {
        "in_text": "What is Tensor.prod?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self.",
        "out_text": "Seetorch.prod"
    },
    {
        "in_text": "What is represented by eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrixinputsubmodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method as Graph.create_node(). Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensorsFor example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)",
        "out_text": "a named tuple"
    },
    {
        "in_text": "What is the name of the matrix where the powers increase from left to right?Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.IfupperisFalse, the returned matrixLis lower-triangular, andincreasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "out_text": "Vandermonde matrix"
    },
    {
        "in_text": "What happens to the activations during inference?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "the activations are dynamically quantized during inference"
    },
    {
        "in_text": "How many different types of quantization can we theoretically have?Q: Does ONNX support implicit scalar datatype casting?Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "out_text": "6"
    },
    {
        "in_text": "Where is the Example Download object located?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "given URL to a local path"
    },
    {
        "in_text": "What can you update if your dataloader has a different structure?You can usetorch.manual_seed()to seed the RNG for all devices (bothThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wise",
        "out_text": "If your dataloader has a different structure"
    },
    {
        "in_text": "What is the term for a Seetorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor"
    },
    {
        "in_text": "Returns a tensor filled with random integers generated how?empty Returns a tensor filled with uninitialized data.record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameReturns a tensor with the same data and number of elements asinput,",
        "out_text": "uniformly"
    },
    {
        "in_text": "Nondeterministic constructors (rand, randn) will have a single what type of value embedded in the trace?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "random"
    },
    {
        "in_text": "Out ((Tensor, Tensor)) \u2013 optional output tuple.Tensor.arctanh Seetorch.arctanh()Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "out_text": "optional"
    },
    {
        "in_text": "What is a string e.g. \"my_package.my_subpackage\"?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "include(Union[List[str],str])"
    },
    {
        "in_text": "Any calls to what function are now invalid?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "afterclose()"
    },
    {
        "in_text": "What support does PyTorch have?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "OpenMP"
    },
    {
        "in_text": "What does %rv.1:Tensormeans assign the output to?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "a (unique) value namedrv.1"
    },
    {
        "in_text": "What are the indices of specified elements collected in COO format?xlogy Computesinput*log(other)with the following cases.32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensorRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "inindicestensor of size(ndim,nse)"
    },
    {
        "in_text": "What is  method  that  stack tensors in sequence depthwise?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.xlogy Computesinput*log(other)with the following cases.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "dstack"
    },
    {
        "in_text": "What \u2013 Provide supplemental information to disambiguate measurements with identical stmt or label?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "sub_label"
    },
    {
        "in_text": "Why is the LU factorization repeated for batches of square matrices with size less than 32 on a CUDA device?vander Generates a Vandermonde matrix .To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources of",
        "out_text": "MAGMA library"
    },
    {
        "in_text": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what effect does the argument have?Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "out_text": "no effect"
    },
    {
        "in_text": "How many competing objectives must the choice of block size balance?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "two"
    },
    {
        "in_text": "What must X be?Note\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "dense tensor"
    },
    {
        "in_text": "What do I want to train a model on?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Note",
        "out_text": "GPU"
    },
    {
        "in_text": "Which version of TorchScript does this section detail the changes to TorchScript in?multilabel_margin_loss SeeMultiLabelMarginLossfor details.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?margin_ranking_loss SeeMarginRankingLossfor details.",
        "out_text": "PyTorch 1.2"
    },
    {
        "in_text": "What is Tensor.ormqr?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.topk Seetorch.topk()",
        "out_text": "Seetorch.ormqr"
    },
    {
        "in_text": "If a key component such as Python was built in separate locations in the two profiles, what can result in something resembling?Q: Does ONNX support implicit scalar datatype casting?To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:However, there are some steps you can take to limit the number of sources of",
        "out_text": "PyTorch"
    },
    {
        "in_text": "For more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torTests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.Set your device to local rank using eitheror >>> with torch.cuda.device(args.local_rank):\n>>>    # your code to runnn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).",
        "out_text": "SeeExtending torch"
    },
    {
        "in_text": "What type of tensor is input(Tensor)?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeConstruct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "float 1D tensor"
    },
    {
        "in_text": "Global Hooks For Module Registers what?32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensorRandom sampling creation ops are listed underRandom samplingandRandom sampling creation ops are listed underRandom samplingand",
        "out_text": "a forward pre-hook common to all modules"
    },
    {
        "in_text": "When will primitive type inputs be added to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "PyTorch 1.9"
    },
    {
        "in_text": "What can you specify when exporting a custom opset?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "custom domain and version"
    },
    {
        "in_text": "What is Tensor.swapdims?multilabel_margin_loss SeeMultiLabelMarginLossfor details.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Seetorch.swapdims"
    },
    {
        "in_text": "Broadcasts input to what?Broadcastsinputto the shapeshape.equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "out_text": "shape shape"
    },
    {
        "in_text": "What rule is used to Estimateydxint y,dxydxalongdim?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "trapezoid rule"
    },
    {
        "in_text": "How to use torch.Generator.initial_seed, give an example?Alias for torch.linalg.matrix_power()You can usetorch.manual_seed()to seed the RNG for all devices (bothtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": ">>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647"
    },
    {
        "in_text": "What distributes gradient between equal values?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "amax/aminevenly"
    },
    {
        "in_text": "What concatenates a sequence of tensors along a new dimension?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "stack"
    },
    {
        "in_text": "Who supports automatically collating individual fetched data samples into batches?This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.Tensor.bool self.bool()is equivalent toself.to(torch.bool).",
        "out_text": "DataLoader"
    },
    {
        "in_text": "rtol(Optional[float]) \u2013 What is rtol?Alias fortorch.linalg.pinv()hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "Relative tolerance"
    },
    {
        "in_text": "What is the term for dense layout?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "torch.strided"
    },
    {
        "in_text": "What is Tensor.dsplit?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in orderAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.",
        "out_text": "Seetorch.dsplit"
    },
    {
        "in_text": "If n is the number of dimensions in what, x.T is equivalent to x.permute(n-1, n-2Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remainingA kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3dSimilar to the function above, but the means are shared among all drawn\nelements. >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])",
        "out_text": "x"
    },
    {
        "in_text": "What should be replaced with torch.linalg.cholesky()?nn.CTCLoss The Connectionist Temporal Classification loss.ger Alias of torch.outer().n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "out_text": "L=torch.cholesky(A)"
    },
    {
        "in_text": "What does with_flops allow one to estimate?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "hardware performance"
    },
    {
        "in_text": "Computes the distance between every pair of row vectors in the input?Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "out_text": "p-norm"
    },
    {
        "in_text": "What is a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsandTensor.xlogy Seetorch.xlogy()Tensor.multiply_ In-place version ofmultiply().Tensor.cosh Seetorch.cosh()",
        "out_text": "polar Constructs"
    },
    {
        "in_text": "What happens if the copy is performed asynchronously with respect to the host?use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "the argument has no effect"
    },
    {
        "in_text": "What does the torch package have?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "CUDA counterpart"
    },
    {
        "in_text": "What must be specified in order to be included in the package?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "modules that should be packaged"
    },
    {
        "in_text": "What is version of atan2()?hardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "Tensor.atan2_ In-place"
    },
    {
        "in_text": "List what available in github hubconf?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoullimultilabel_margin_loss SeeMultiLabelMarginLossfor details.",
        "out_text": "all entrypoints"
    },
    {
        "in_text": "What does torch.Tensor do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "register_hook"
    },
    {
        "in_text": "Where can you find more information about the tensor autograd APIs?The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:For an overview of the PyTorch C++ model authoring and training API, please seeNo, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "https"
    },
    {
        "in_text": "The inputiis NOT a list of what?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "index tuples"
    },
    {
        "in_text": "If hop_length is None (default), what is it treated asThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "equal to\nfloor(n_fft / 4)"
    },
    {
        "in_text": "What does AlphaDropout Applies over the input?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "out_text": "Alpha Dropout"
    },
    {
        "in_text": "What gradient of an iterable of parameters at specified value. Convert parameters to one vector Convert one vector to the parameters prune.BasePThe module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan.nn.Identity A placeholder identity operator that is argument-insensitive. nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b nn.Bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "out_text": "Clips"
    },
    {
        "in_text": "What methods are used to get a new tensor with the data in inputfake quantized per channel?TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.Tensor.logical_xor Seetorch.logical_xor()ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "out_text": "scale,zero_point,quant_minandquant_max"
    },
    {
        "in_text": "If return_complex is True, the return is a input.dim() + what?Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect. def forward(self, x):\n    a = x + 1\n    return x + self.attr_1Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "out_text": "1 dimensional complex tensor"
    },
    {
        "in_text": "What value does an n-dimensionaltorch.Tensor Examples Fill the input Tensor with?64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensorTensor.geometric_ Fillsselftensor with elements drawn from the geometric distribution:Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos()",
        "out_text": "scalar value0"
    },
    {
        "in_text": "What does Tensor.atan2 Seetorch.atan2() do?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "out_text": "Tensor.atan2 Seetorch.atan2()"
    },
    {
        "in_text": "What does torch.movedim() return for a new tensor that is a narrowed version of input tensnn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1).scatter_add Out-of-place version of torch.Tensor.scatter_add_()dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "out_text": "Alias"
    },
    {
        "in_text": "What is method  that sets options for printing?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "set_printoptions"
    },
    {
        "in_text": "What is the name of the function that returns a new tensor with the exponential of the elements of input?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "Alias for torch.special.erfinv()"
    },
    {
        "in_text": "TorchScript has a representation at a lower level than the code pretty- printer, in the form of what?torch.mm() no M[sparse_coo]@M[strided]->M[strided]Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "out_text": "IR graphs"
    },
    {
        "in_text": "What is returned when a named tuple(values,indices) returns the cumulative product of elements of inputin the dimension dimpow Takes the power of each element in inputwithexponentand returns a tensor with the result.For example, ifinputis of shapeIf actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "out_text": "cumulative sum of elements of inputin the dimension dim"
    },
    {
        "in_text": "What is a deprecated function that returns the maximum GPU memory occupied by tensors in bytes for a given devicepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "max_memory_reserved"
    },
    {
        "in_text": "What is the name of a callable (entrypoint) defined in the repo/dir\u2019shubconf.py?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "model(string)"
    },
    {
        "in_text": "What is the default value of the return tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "ifNone"
    },
    {
        "in_text": "What does param_groups contain across distributed data parallel ranks?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "Partitions parameters"
    },
    {
        "in_text": "What is a number that has a good balance of 0 and 1 bits?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "a large seed"
    },
    {
        "in_text": "What does path(str) do to save stacks file to?Q: Does ONNX support implicit scalar datatype casting?Tensor.arctanh Seetorch.arctanh()static quantization (weights quantized, activations quantized, calibration required post training) quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post for a more comprehensive overview of the tradeoffs between these quantization types. This is the simplest to apply form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference. This is used for situations where the model execution time is dominated by loading weights from memory rather than computing the matrix multiplications. This is true for for LSTM and Transformer type models with small batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "out_text": "path(str) \u2013 save stacks file to this location"
    },
    {
        "in_text": "What do provided tensors do?Tensor.absolute Alias forabs()Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "Create a block diagonal matrix"
    },
    {
        "in_text": "Iftrackersetsbvars[\u201cforce_stop?\u201d] = True, the iteration process will be hard-stopped. orthomethod=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.These options are configured by the constructor arguments of a\nDataLoader, which has signature: DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.",
        "out_text": "force_stop"
    },
    {
        "in_text": "What happens to a tensor with three or more dimensions?Tensor.unsqueeze Seetorch.unsqueeze()Tensor.unsqueeze Seetorch.unsqueeze()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "Splits input"
    },
    {
        "in_text": "If you need to move a model to GPU via.cuda(), please do so before constructing what?You can usetorch.manual_seed()to seed the RNG for all devices (bothApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "before constructing optimizers"
    },
    {
        "in_text": "What is the LU factorization of a matrix or batches of matrices A?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "LU factorization of a tensor"
    },
    {
        "in_text": "What is out (Tensor, optional)?Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note>>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "out_text": "output tensor"
    },
    {
        "in_text": "Where is the original object stored?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "CUDA memory"
    },
    {
        "in_text": "What is the definition of Linear / Identity 111?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dTensor.frexp Seetorch.frexp()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "Linear / Identity 111"
    },
    {
        "in_text": "What does the 3D max pooling over an input signal consist of?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "Computes a partial inverse of MaxPool1d"
    },
    {
        "in_text": "What is Tensor.gather Seetorch.gather?synchronize Waits for all kernels in all streams on a CUDA device to complete.Tensor.gather Seetorch.gather()torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2",
        "out_text": "Tensor.gather Seetorch.gather"
    },
    {
        "in_text": "What does this function return with the global dtype default?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "a tensor with dtypetorch.int64"
    },
    {
        "in_text": "What is Seetorch.arcsinh function?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Tensor.arcsinh"
    },
    {
        "in_text": "What is used to multiply the columns of the spanning subspace inUandV?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from the input.size(1).",
        "out_text": "a rotation matrix"
    },
    {
        "in_text": "What does the exampleema_modelcompute?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "an exponential moving average"
    },
    {
        "in_text": "What computes theoutputusinginputvalues and pixel locations from a flow-fieldgrid?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "grid_sample"
    },
    {
        "in_text": "What is Tensor.round?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Seetorch.round"
    },
    {
        "in_text": "What returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizerrandint Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "randint"
    },
    {
        "in_text": "What does this wrap a given function with?Tensor.t_ In-place version oft()trapz Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule.Fillsselftensor with elements drawn from the geometric distribution:",
        "out_text": "Wraps a given function with__torch_function__-related functionality"
    },
    {
        "in_text": "What is the name of the dictionary to specify the input to the corresponding named parameter?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "KEY: str"
    },
    {
        "in_text": "A dictionary that maps namespaces that contain what?nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().",
        "out_text": "overridable functions"
    },
    {
        "in_text": "How to use torch.var_mean, give an example?Consider using torch.linalg.lstsq() if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: torch.linalg.lstsq(A, B).solution == A.pinv() @ BQ: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices? First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "out_text": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))"
    },
    {
        "in_text": "What does check_device(bool) stand for?Alias for torch.linalg.matrix_power()hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "If True(default)"
    },
    {
        "in_text": "What value does Tensor val fill the input Tensor with?Q: Does ONNX support implicit scalar datatype casting?Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "scalar value"
    },
    {
        "in_text": "What must be _onnx_main_opset or _onnx_stable_opsets?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "opset_version"
    },
    {
        "in_text": "The inverse of a symmetric positive-definite matrix AAA is computed using what?Returns True if the data type ofinputis a complex data type i.e.,Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into a torch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size as input.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning from start.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230a stepend\u2212start\u200b\u230b+1with values from starttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from starttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.mvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given by",
        "out_text": "MAGMA routines"
    },
    {
        "in_text": "What is the name of the script that produces the graph?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias for torch.linalg.matrix_power()",
        "out_text": "test.py"
    },
    {
        "in_text": "Holds submodules in a dictionary. Holds parameters in a dictionary. Holds submodules in a dictionary. Holdclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size. nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "out_text": "Holds submodules in a list"
    },
    {
        "in_text": "How many rows of inputdo not need to sum to one?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "rows ofinputdo not need to sum to one"
    },
    {
        "in_text": "Are eigenvalues ordered or ordered?Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) ReferencesTensor.swapaxes Seetorch.swapaxes()Tensor.multiply_ In-place version ofmultiply().",
        "out_text": "not necessarily ordered"
    },
    {
        "in_text": "What is the input tensor of size(optional)?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteConstruct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "X(tensor,optional)"
    },
    {
        "in_text": "What is the name of the loss function that takes the mean element-wise absolute value difference?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].xlogy Computesinput*log(other)with the following cases.Tensor.log10 Seetorch.log10()",
        "out_text": "SeeHingeEmbeddingLoss"
    },
    {
        "in_text": "How long should thisFuture be blocked?Notetorch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "until the value of thisFutureis ready"
    },
    {
        "in_text": "Why is not providing a value for steps deprecated?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "not reflected in the documented function signature"
    },
    {
        "in_text": "Is it okay to install two different branches of the same repo in the same process?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "load them in separate processes"
    },
    {
        "in_text": "What is used to generate result files for TensorBoard?Q: Does ONNX support implicit scalar datatype casting?Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)"
    },
    {
        "in_text": "What returns the matrix product of the NNN 2-D tensors?Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) ReferencesTensor.swapaxes Seetorch.swapaxes()Tensor.topk Seetorch.topk()",
        "out_text": "matrix product of the NNN 2-D tensors"
    },
    {
        "in_text": "Computes the one dimensional Fourier transform of real-valuedinput. Computes what?Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or notlr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch.>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])",
        "out_text": "inverse ofrfft()"
    },
    {
        "in_text": "What should the mock hook have?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "signature"
    },
    {
        "in_text": "What happens if the operator is a non-ATen operator?Q: Does ONNX support implicit scalar datatype casting?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "out_text": "the symbolic function has to be added in the corresponding PyTorch Function class"
    },
    {
        "in_text": "What size is the tensor of sizeendstartstep?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "1-D"
    },
    {
        "in_text": "What does prune.CustomFromMask prune.identity apply to the tensor corresponding to the parameter callednameinmoduleConstruct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.pow Takes the power of each element in inputwithexponentand returns a tensor with the result.For example, ifinputis of shape",
        "out_text": "pruning reparametrization"
    },
    {
        "in_text": "What computes the element-wise maximum of input and other?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "fmax"
    },
    {
        "in_text": "What is placed into the package when a module is interned?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "model code"
    },
    {
        "in_text": "What is the scalar value of the tensor filled with?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Examplepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "the same size as input"
    },
    {
        "in_text": "What does the function usestorch.linalg.svd() ifhermitian= False?Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrueBecause FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.",
        "out_text": "True"
    },
    {
        "in_text": "How many bit integers does torch.int16ortorch.short torch have?This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.Alias for torch.linalg.matrix_power()Boolean torch.bool torch.*.BoolTensor",
        "out_text": "16"
    },
    {
        "in_text": "What does the exporter store some model parameters in?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "external binary files"
    },
    {
        "in_text": "What is the tensor to unbind dim (int) \u2013 dimension to remove?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Tensor.cosh Seetorch.cosh()Tensor.topk Seetorch.topk()",
        "out_text": "input"
    },
    {
        "in_text": "What is the name of the string that specifies how repo_or_dir is to be interpreted?Tensor.logical_xor_ In-place version oflogical_xor()Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "source"
    },
    {
        "in_text": "How do you add a symbolic function in the corresponding Function class?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "Create a symbolic function named symbolic"
    },
    {
        "in_text": "What Applies the Sigmoid Linear Unit (SiLU) function, element-wise?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "silu"
    },
    {
        "in_text": "What is the name of the criterion that optimizes a two-class classification logistic loss between input tensorxxxand targetWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "nn"
    },
    {
        "in_text": "What Returns the number of dense dimensions in a sparse tensor self?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "Tensor.dense_dim"
    },
    {
        "in_text": "What type of operations will throw a RuntimeError when mode=True?You can usetorch.manual_seed()to seed the RNG for all devices (bothregister_module_forward_hook Registers a global forward hook for all the modulesregister_module_forward_hook Registers a global forward hook for all the modules",
        "out_text": "normally-nondeterministic"
    },
    {
        "in_text": "What is the name of the method that determines the padding method used oninputwhencenterisTrue?Alias for torch.linalg.matrix_power()xlogy Computesinput*log(other)with the following cases.Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "Seetorch.nn.functional.pad()"
    },
    {
        "in_text": "If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding what?TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.Whendimis given, a squeeze operation is done only in the givenFunction that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "out_text": "PyTorch Function class"
    },
    {
        "in_text": "Conv_transpose1d Applies a transposed convolution operator over an input signal composed of several input planes?Tensor.unsqueeze_ In-place version ofunsqueeze()Tensor.logical_xor_ In-place version oflogical_xor()Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "out_text": "1D"
    },
    {
        "in_text": "What is the name of the global step value to record parameter in add_images method?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "global_step"
    },
    {
        "in_text": "What program supports both per tensor and per channel asymmetric linear quantization?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "PyTorch"
    },
    {
        "in_text": "What is Seetorch.frac()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.frac"
    },
    {
        "in_text": "What LR Multiply the learning rate of each parameter group by the factor given in the specified function?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "Multiplicative"
    },
    {
        "in_text": "Whether to use Bessel's correction (N=1delta N = 1N=1)?Q: Does ONNX support implicit scalar datatype casting?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.",
        "out_text": "unbiased"
    },
    {
        "in_text": "In what model should function know how to handle the inputs passed as the tuple?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "LSTM"
    },
    {
        "in_text": "What may increase the overhead of nvtx range creation?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "shape recording"
    },
    {
        "in_text": "What can this function be used as?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "a decorator"
    },
    {
        "in_text": "What does torch.repeat_interleave() attempt to differentiate?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What returns a view of iinput as a complex tensor?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "view_as_complex"
    },
    {
        "in_text": "What format does this module convert your model from?You can usetorch.manual_seed()to seed the RNG for all devices (bothtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Alias for torch.linalg.matrix_power()",
        "out_text": "FP32"
    },
    {
        "in_text": "What type of unrelated functionality does yourutilsmodule contain?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "hodge-podge"
    },
    {
        "in_text": "What does the Alias for torch.acosh() do?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dnegative Alias for torch.neg()negative Alias for torch.neg()",
        "out_text": "Adds the scalar other to each element of the input input"
    },
    {
        "in_text": "What should mock be used for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Prototype",
        "out_text": "last resort"
    },
    {
        "in_text": "What should you do if you want long-term reproducibility for your package?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "limit your use of extern"
    },
    {
        "in_text": "How many vectors does prune.BasePruningMethod convert to the parameters?Tensor.log10 Seetorch.log10()Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.",
        "out_text": "one"
    },
    {
        "in_text": "If no suitable index is found, return what value for non-numerical value?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulis_tensor Returns True if obj is  a PyTorch tensor.",
        "out_text": "0"
    },
    {
        "in_text": "What is 111 Tanh?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "111 Tanh"
    },
    {
        "in_text": "What is the name of a model defined in a repo's hubconf.py?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "pytorch/vision[:hub]"
    },
    {
        "in_text": "What is clone Seetorch.clone()?Tensor.clone Seetorch.clone()Tensor.clone Seetorch.clone()This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph. Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are subpaths of target.",
        "out_text": "Tensor"
    },
    {
        "in_text": "Instead of writing import foo and later usingfoo.bar.baz, what would you prefer to do?Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. WarningComputes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dAlias for torch.linalg.matrix_power()",
        "out_text": "writefromfoo.barimportbaz"
    },
    {
        "in_text": "What is the distance between neighboring window frames?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.arctanh Seetorch.arctanh()",
        "out_text": "hop_length"
    },
    {
        "in_text": "What is the term used to describe a device that uses 1 sign, 5 exponent, and 10 significand bits?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteIt is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "out_text": "Brain Floating Point"
    },
    {
        "in_text": "What returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSRTensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place.Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "out_text": "Tensor.crow_indices"
    },
    {
        "in_text": "What does the \"Sparse grad?\" column indicate?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "if the PyTorch operation supports backward with respect to sparse matrix argument"
    },
    {
        "in_text": "What is the learning rate of each parameter group set according to?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.whereNNNis the full window size.",
        "out_text": "1cycle learning rate policy"
    },
    {
        "in_text": "What returns a new Tensor with data as the tensor data?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.Tensor.log10 Seetorch.log10()",
        "out_text": "Tensor"
    },
    {
        "in_text": "What must you specify for each module that the dependency resolver finds?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "an action to take"
    },
    {
        "in_text": "What type of data can be picked up by exporters?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "Python"
    },
    {
        "in_text": "What is Seetorch.copysign function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.copysign"
    },
    {
        "in_text": "What is identical to a Pythonregular package?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "The layout"
    },
    {
        "in_text": "What is pruned by in modules that inherit from theBasePruningMethod?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "looking forforward_pre_hooks"
    },
    {
        "in_text": "If win_length is None, window will be padded on both sides to length n_fft before being applied.Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')Tensor.split Seetorch.split()ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "n_fft"
    },
    {
        "in_text": "What does max_abs_diff_idx represent?Alias for torch.linalg.matrix_power()Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "out_text": "Index of greatest absolute difference"
    },
    {
        "in_text": "What is ready-to-deploy?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "PyTorch code examples"
    },
    {
        "in_text": "If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor contains the orthoSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeTensor.mvlgamma_ In-place version ofmvlgamma()",
        "out_text": "If eigenvectors=False"
    },
    {
        "in_text": "How to use TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations., give an example?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "out_text": "import torch\n\nclass Module(torch.nn.Module):\n    def forward(self, x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x"
    },
    {
        "in_text": "What function computes the dot product between the Jacobian of a given function at the point given by the inputs and a vectorno M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yesfunctional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs.model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "out_text": "functional.jvp"
    },
    {
        "in_text": "What is the name of the element type in the COO format?Tensor.arctanh Seetorch.arctanh()enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "torch"
    },
    {
        "in_text": "What is the name of the code used to define variables in PyTorch?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "in stmt global_setup"
    },
    {
        "in_text": "How many guarantees your layout is preserved as long as create_graph=False?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Alias for torch.linalg.matrix_power()Tensor.cos_ In-place version ofcos()",
        "out_text": "3"
    },
    {
        "in_text": "What part of the function works as a help message?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "Docstring"
    },
    {
        "in_text": "Where is the status of the factorization present in the return tuple?Q: Does ONNX support implicit scalar datatype casting?Tensor.isneginf Seetorch.isneginf()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "out_text": "third element"
    },
    {
        "in_text": "Exports the collected trace in what format?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "out_text": "Chrome JSON"
    },
    {
        "in_text": "Why is it always preferable to usesolve() when possible?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "faster and more numerically stable"
    },
    {
        "in_text": "Where is the status of the factorization present?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "third element of the return tuple"
    },
    {
        "in_text": "What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements ofConstruct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "out_text": "Alias fortorch.atanh()"
    },
    {
        "in_text": "What wraps a given function with?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "Examples Wraps a given function with__torch_function__-related functionality"
    },
    {
        "in_text": "What kind of memory usage might be caused by the current implementation of torch.Tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "unexpectedly high"
    },
    {
        "in_text": "What API has been deprecated?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Variable API"
    },
    {
        "in_text": "What does the exporter do with the ONNX external data format option enabled?Q: Does ONNX support implicit scalar datatype casting?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "the exporter stores some model parameters in external binary files"
    },
    {
        "in_text": "What is the typical use case of a github repo or a local directory?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Load a model"
    },
    {
        "in_text": "What is a values(TensorScalar) containing the search value(s)?xlogy Computesinput*log(other)with the following cases.callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "N-D tensor or a Scalar"
    },
    {
        "in_text": "What does seetorch.jit.trace_module do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "trace a specific method on a module"
    },
    {
        "in_text": "How to use However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:, give an example?Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current ModuleTensor.clamp_ In-place version ofclamp()",
        "out_text": ">>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)"
    },
    {
        "in_text": "What is an example of a model where optimized parameters live in consistent locations when optimizers are constructed and used?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "Example"
    },
    {
        "in_text": "What would be preserved if the corresponding values invalueswere inserted before the indices?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.",
        "out_text": "the order of the correspondinginnermostdimension withinsorted_sequence"
    },
    {
        "in_text": "How to use Creates a half-normal distribution parameterized by scale where:, give an example?Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers.",
        "out_text": "X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)"
    },
    {
        "in_text": "What tensor must contain a monotonically increasing sequence?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "1-D"
    },
    {
        "in_text": "What is the pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?hardswish Applies the hardswish function, element-wise, as described in the paper:Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "out_text": "Utility"
    },
    {
        "in_text": "When enabled, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw what when called?Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensorSets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "out_text": "RuntimeError"
    },
    {
        "in_text": "Returns a tensor filled with the scalar value 0, with what as input?If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.Returns a tensor of the same size asinputwith each elementmatrix _rank Computes the numerical rank of a matrix .",
        "out_text": "same size"
    },
    {
        "in_text": "What generates the forward() function on GraphModules?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "FX"
    },
    {
        "in_text": "What can include(Union[List[str],str]) be?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Alias fortorch.linalg.pinv()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "glob-style pattern"
    },
    {
        "in_text": "How is sub Subtracts other scaled?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "byalpha"
    },
    {
        "in_text": "What is the name of the function that clamps all elements in input into the range?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dcallback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Notepickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "Alias for torch.clamp()"
    },
    {
        "in_text": "How to use torch.mul, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])"
    },
    {
        "in_text": "What type of elements can be stored in a sparse array?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "non-zero"
    },
    {
        "in_text": "What is the default value of offset(int) if not provided?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "out_text": "Default"
    },
    {
        "in_text": "What is theFuturetype primarily used by?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013Note",
        "out_text": "theDistributed RPC Framework"
    },
    {
        "in_text": "What is clone?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.clone Seetorch.clone"
    },
    {
        "in_text": "What happens in the course of authoring transformations?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "our code will not be quite right"
    },
    {
        "in_text": "What does it do when a mock is not matched to a module used by the package being exported?Q: Does ONNX support implicit scalar datatype casting?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dtimer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "out_text": "Registers an extern hook on the exporter"
    },
    {
        "in_text": "What is input(Tensor) used to quantize scales?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "float tensor"
    },
    {
        "in_text": "Since what version is real input deprecated?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "1.8.0"
    },
    {
        "in_text": "How to use torch.eye, give an example?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.negative Alias for torch.neg()",
        "out_text": ">>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])"
    },
    {
        "in_text": "Sets the current stream.This is a wrapper API to what?Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().You can usetorch.manual_seed()to seed the RNG for all devices (bothenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "set the stream"
    },
    {
        "in_text": "What does TorchScript assign the output to?Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.inverse Alias for torch.linalg.inv()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1"
    },
    {
        "in_text": "What is Seetorch.log1p() function?hardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "Tensor.log1p"
    },
    {
        "in_text": "What is in-place version of atanh?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.atanh_ In-place version ofatanh()"
    },
    {
        "in_text": "Where can we look at the code within GraphModule?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "foo/module.py"
    },
    {
        "in_text": "Package Exporter exposes three methods that allow you to save Python objects, text, and binary data to a package.PyTorch supports both per tensor and per channel asymmetric linear quantization. Per tensor means that all the values within the tensor are scaled the same way. Per channel means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are scaled and offset by a different value (effectively the scale and offset become vectors). This allows for lesser error in converting tensors to quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error after quantization, thereby ensuring that operations like padding do not cause additional quantization error. In order to do quantization in PyTorch, we need to be able to represent quantized data in Tensors. A Quantized Tensor allows for storing quantized data (represented as int8/uint8/int32) along with quantization parameters like scale and zero_point. Quantized Tensors allow for many useful operations making quantized arithmetic easy, in addition to allowing for serialization of data in a quantized format.get_device_capability Gets the cuda capability of a device.Tensor.q_zero_point Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().",
        "out_text": "save_pickle,save_textandsave_binary"
    },
    {
        "in_text": "What does the Alias for torch.gt() do?negative Alias for torch.neg()negative Alias for torch.neg()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Computesinput"
    },
    {
        "in_text": "If the wanted operator is what, it should be easy to add support for exporting such operator?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "standardized in ONNX"
    },
    {
        "in_text": "How does the script save the resulting traced model to alexnet?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.You can usetorch.manual_seed()to seed the RNG for all devices (both",
        "out_text": "runs a single round of inference"
    },
    {
        "in_text": "What function does nn.Hardtanh apply element-wise?Q: Does ONNX support implicit scalar datatype casting?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "HardTanh"
    },
    {
        "in_text": "Supports input of what data types?Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise. Supportsbroadcasting to a common shape,Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise. Supportsbroadcasting to a common shape,exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "out_text": "float, double, cfloat and cdouble"
    },
    {
        "in_text": "What are broadcast_tensors?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "shapes"
    },
    {
        "in_text": "Most types can be inferred from what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "value of the member"
    },
    {
        "in_text": "What is the criterion that optimizes a multi-label one-versus-all loss based on max-entropy?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "nn"
    },
    {
        "in_text": "What is the name of the criterion that optimizes a multi-class multi-classification hinge loss?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "nn.SoftMarginLoss"
    },
    {
        "in_text": "If you have a Tensordataand just want to change what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "itsrequires_gradflag, userequires_grad_()ordetach"
    },
    {
        "in_text": "What do function._ContextMethodMixin.mark_non_differentiable mark outputs as?For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable.function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation.",
        "out_text": "non-differentiable"
    },
    {
        "in_text": "What is the principal use of description to signal toComparethe columns of data?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "usingCompare"
    },
    {
        "in_text": "What takes thisFutureas the only argument?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "aCallable"
    },
    {
        "in_text": "If None, the SHA256 downloaded file should start what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "withhash_prefix"
    },
    {
        "in_text": "If inputis a CUDA tensor and there are what, this function may nondeterministically returnindices for any of themTensor.mvlgamma_ In-place version ofmvlgamma()This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all haveTensor.xlogy Seetorch.xlogy()",
        "out_text": "multiple validkth values"
    },
    {
        "in_text": "How many dices_or_sections is an integer n or a zero dimensional long tensor with value n?xlogy Computesinput*log(other)with the following cases.Number \u2013 sum(abs(x)**ord)**(1./ord)out (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)). pivots stores all the intermediate transpositions of rows. The final permutation perm could be reconstructed by applying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1, where perm is initially the identity permutation of mmm elements (essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of size (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "out_text": "n"
    },
    {
        "in_text": "What do you need if an operation does not act correctly according to the documentation?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a deterministic implementation of an operation that does not have one"
    },
    {
        "in_text": "What is the sum of each row of theinputtensor in the given dimensiondim?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "Ifdimis a list of dimensions"
    },
    {
        "in_text": "What is the default value of enabled?Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "True"
    },
    {
        "in_text": "What is requirement on tensor when using fliplr?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "2-D"
    },
    {
        "in_text": "What tag indicates that otherwise identical tasks were run in different environments?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "env"
    },
    {
        "in_text": "What is an example of quantization in Eager Mode Quantization?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "API"
    },
    {
        "in_text": "Export a model into what format?Fillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).hstack Stack tensors in sequence horizontally (column wise).",
        "out_text": "ONNX format"
    },
    {
        "in_text": "What should be the first parameter of a de-packaging function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a Package Importer instance"
    },
    {
        "in_text": "What is the name of the instruction that produces the graph?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "%rv.1:Tensor"
    },
    {
        "in_text": "What does not save intermediate activations of the entire computation graph for computing backward?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "out_text": "the checkpointed part"
    },
    {
        "in_text": "What does it compute of a matrix or batches of matricesA?atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "LU factorization"
    },
    {
        "in_text": "What is the difference between splitsinput and torch.tensor_split(input, indices_or_sectionsA set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. functions \u2013 A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially.Tensor.acos_ In-place version ofacos()",
        "out_text": "ifindices_or_sectionsis an integer it must evenly divide the split dimension"
    },
    {
        "in_text": "What does Alias for torch.mul() do?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note",
        "out_text": "Alias for torch.mul()"
    },
    {
        "in_text": "How to use torch.reshape, give an example?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "out_text": ">>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])"
    },
    {
        "in_text": "If a matrix on the left is multiplyed by a negative power as what?Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence LossWhendimis given, a squeeze operation is done only in the givenTorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "out_text": "ifn> 0"
    },
    {
        "in_text": "How is torch.package accessed?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.le().",
        "out_text": "defining the method__reduce_package__on a class and by defining a corresponding de-packaging function"
    },
    {
        "in_text": "What does the matrix product of two tensors produce?vander Generates a Vandermonde matrix .Q: Does ONNX support implicit scalar datatype casting?1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "out_text": "Matrix product of two tensors"
    },
    {
        "in_text": "Returns what with all the dimensions of inputof size1removed?xlogy Computesinput*log(other)with the following cases.softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "out_text": "a tensor"
    },
    {
        "in_text": "Python makes it really easy to bind objects and run code at what?Q: Does ONNX support implicit scalar datatype casting?For custom operators, you might need to set python seed as wellFor custom operators, you might need to set python seed as well",
        "out_text": "module-level scope"
    },
    {
        "in_text": "What method walks back through your code to show you where this situation happens?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "The traceback"
    },
    {
        "in_text": "What decomposition of a symmetric positive-definite matrix AAA is computed?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "Cholesky"
    },
    {
        "in_text": "What is the mean of the normal distribution?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "mean"
    },
    {
        "in_text": "What is the name of the context manager that makes every autograd operation emit an NVTX range?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "nvprof"
    },
    {
        "in_text": "What is the name of the main statement?Note\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "stmt"
    },
    {
        "in_text": "n_fft (int) \u2013 size of what?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "Fourier transform hop_length"
    },
    {
        "in_text": "What kind of function is the call to geqrf?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "low-level"
    },
    {
        "in_text": "Values(TensororScalar) \u2013 N-D tensor or a Scalar containing what?Tensor.arccosh_ acosh_() -> TensorThis would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "out_text": "search value(s)"
    },
    {
        "in_text": "How often does the ONNX format exporter run a model?You can usetorch.manual_seed()to seed the RNG for all devices (bothA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "runs your model once"
    },
    {
        "in_text": "How can pretrained weights be loaded?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url()"
    },
    {
        "in_text": "What raises if no implementation is found?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "TypeError"
    },
    {
        "in_text": "What does the function do to a matrix or batches of matrices A?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.",
        "out_text": "Computes the LU factorization"
    },
    {
        "in_text": "What does add a scalar or tensor toselftensor do?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "Add a scalar or tensor toselftensor"
    },
    {
        "in_text": "What does tag (string) refer to?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Data identifier values"
    },
    {
        "in_text": "What is another (Tensor)?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "second input tensor"
    },
    {
        "in_text": "What does torch.linalg.inv() compute of a square matrix?Alias fortorch.linalg.pinv()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "inverse"
    },
    {
        "in_text": "What is logarithmic scale used to create a one-dimensional tensor of size steps?prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.Tensor.xlogy Seetorch.xlogy()Tensor.multiply_ In-place version ofmultiply().",
        "out_text": "basebase"
    },
    {
        "in_text": "What are two examples of what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "Tensor.logical_and Seetorch.logical_and"
    },
    {
        "in_text": "What is function used by Tensor.ne?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.ne_ In-place version ofne()"
    },
    {
        "in_text": "What is the output Tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "Tensor"
    },
    {
        "in_text": "Batch1andbatch2must be what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "3-D tensors"
    },
    {
        "in_text": "What is.isfinite?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "Seetorch"
    },
    {
        "in_text": "What does sparse.addmm do exactly the same thing as?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "torch.addmm()"
    },
    {
        "in_text": "a torch.nn.BatchNorm1dmodule with lazy initialization of what?Boolean torch.bool torch.*.BoolTensorPyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_namehardtanh_ In-place version ofhardtanh().",
        "out_text": "thenum_featuresargument of theBatchNorm1d"
    },
    {
        "in_text": "fftn Computes the N dimensional discrete what of input?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the conditionCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "out_text": "Fourier transform"
    },
    {
        "in_text": "What measures the element-wise mean squared error?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "mse_loss"
    },
    {
        "in_text": "What is the lexico of sparse tensors?Tensor.swapaxes Seetorch.swapaxes()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) ReferencesTensor.frexp Seetorch.frexp()",
        "out_text": "lexico"
    },
    {
        "in_text": "What happens to storages when they are deserialized on the CPU and moved to the device they were saved from?Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimension dim.",
        "out_text": "If this fails"
    },
    {
        "in_text": "What is in-place version ofaddr()?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.addr_ In-place version ofaddr()"
    },
    {
        "in_text": "Model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.Tensor.isneginf Seetorch.isneginf()normalize PerformsLpL_pLp\u200bnormalization of inputs over specified dimension.",
        "out_text": "model.classifier"
    },
    {
        "in_text": "What is the default behavior?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "None"
    },
    {
        "in_text": "Supports what?negative Alias for torch.neg()negative Alias for torch.neg()Alias for torch.mul().",
        "out_text": "broadcasting"
    },
    {
        "in_text": "What could cause callingvalue() to fail?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "thisFuturemay not yet hold a value"
    },
    {
        "in_text": "What is applied to prune.custom_from_mask?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliNo, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Alias for torch.linalg.matrix_power()",
        "out_text": "pre-computed mask inmask"
    },
    {
        "in_text": "What is a list or tuple of ints, or a one-dimensional long tensor?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "indices_or_sections"
    },
    {
        "in_text": "What computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs?Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.",
        "out_text": "functional.vjp Function"
    },
    {
        "in_text": "What returns the median of the values in input?Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.interpolate Down/up samples the input to either the givensizeor the givenscale_factorThis installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "nanmedian"
    },
    {
        "in_text": "What is the Frobenius norm?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "fro"
    },
    {
        "in_text": "What is the tensor to factor of size?class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "A"
    },
    {
        "in_text": "What pre-trained model can TensorBoard fine tune?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Mask R-CNN"
    },
    {
        "in_text": "What device will be the CPU for CPU tensor types?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "current CUDA device"
    },
    {
        "in_text": "What interacts withMulti-process data loading?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "howIterableDataset"
    },
    {
        "in_text": "What does the torch.device contains?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "device type"
    },
    {
        "in_text": "What helper function can convert a scalar tensor into a python scalar?Q: Does ONNX support implicit scalar datatype casting?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "_scalar"
    },
    {
        "in_text": "The first method is to pass all inputs in the same order as required by what?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. NoteIn general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "out_text": "the model"
    },
    {
        "in_text": "What do constantPad1d Pads the input tensor boundaries with?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulliwhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "constant value"
    },
    {
        "in_text": "How  For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors., give an example?Tensor.clamp_ In-place version ofclamp()fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current ModuleRecords operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "out_text": ">>> torch.device(1)\ndevice(type='cuda', index=1)"
    },
    {
        "in_text": "What happens to a tensor with one or more dimensions?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dFillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "out_text": "Splits input"
    },
    {
        "in_text": "weight_norm Applies what to a parameter in a given module?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "weight normalization"
    },
    {
        "in_text": "Not a Numbers (NaNs) is treated as what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.mish Applies the Mish function, element-wise.functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is created with strides matching param (thus matching param\u2019s layout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grads to None before each accumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks.",
        "out_text": "zero"
    },
    {
        "in_text": "What is the return value of Model() k?xlogy Computesinput*log(other)with the following cases.The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "out_text": "x m"
    },
    {
        "in_text": "What do we want to do with a submodule?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "delete"
    },
    {
        "in_text": "What do extra_cuda_cflags forward to when building CUDA sources?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "nvcc"
    },
    {
        "in_text": "What is the name of the class that measures the loss given inputsx1x1x1,x2x2x2, and a labelswhereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "MultiLabelMarginLoss"
    },
    {
        "in_text": "What is an example of a useful feature that can be passed as keyword arguments?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wisexlogy Computesinput*log(other)with the following cases.",
        "out_text": "per-layer learning rates"
    },
    {
        "in_text": "What is repo_or_dir(string)?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "repo name"
    },
    {
        "in_text": "What is the function that returns the result of atorch.jit.Future[T]asynchronous task?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "Forces completion of atorch.jit.Future[T]asynchronous task"
    },
    {
        "in_text": "What happens to a sparse tensor?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "resizes self to the desired size and the number of sparse and dense dimensions"
    },
    {
        "in_text": "What is niter?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "maximum number of iterations"
    },
    {
        "in_text": "What type of version of a module can you save for use in a separate process?Q: Does ONNX support implicit scalar datatype casting?You can usetorch.manual_seed()to seed the RNG for all devices (bothtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "offline"
    },
    {
        "in_text": "What is this mode used to do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "export all operators as ATen ops"
    },
    {
        "in_text": "What disables denormal floating numbers on the CPU?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "set_flush_denormal"
    },
    {
        "in_text": "If increasing is True, the columns of the Vandermonde matrix are reversed if what?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "False"
    },
    {
        "in_text": "Who does not support exporting elu operator?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "PyTorch"
    },
    {
        "in_text": "What is the data type ofinputis a complex data type?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "one oftorch.complex64, andtorch.complex128"
    },
    {
        "in_text": "In what order will hooks be called?NoteComputes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013",
        "out_text": "order of registration"
    },
    {
        "in_text": "How to use torch.complex, give an example?n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013ger Alias of torch.outer().ger Alias of torch.outer().",
        "out_text": ">>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64"
    },
    {
        "in_text": "How many channels are currently unpruned?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "specifiedamountof"
    },
    {
        "in_text": "What types of inputs does torch.solve(B, A)support?Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "real-valued and complex-valued inputs"
    },
    {
        "in_text": "Where*is what?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "the optional batch size of input"
    },
    {
        "in_text": "How  Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?xlogy Computesinput*log(other)with the following cases.Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "out_text": ">>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy"
    },
    {
        "in_text": "s.values().layout==torch.strided- values are stored as what?Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.",
        "out_text": "strided tensors"
    },
    {
        "in_text": "What is the input tensor casted to when performing the operation?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "out_text": ":attr:\u2019dtype\u2019"
    },
    {
        "in_text": "How to use torch.nn.init.zeros_, give an example?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "out_text": ">>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)"
    },
    {
        "in_text": "What is an example of a tensor that has fewer dimensions thanrepsspecifies?With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 NoteWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Noteclass_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "out_text": "ifinputhas shape"
    },
    {
        "in_text": "What type of hook is registered for all the modules?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "a global forward hook"
    },
    {
        "in_text": "What can you package?It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcastSplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "Torch Script module"
    },
    {
        "in_text": "What does a Future not have to be done twice?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "a Future cannot be marked completed twice"
    },
    {
        "in_text": "What Sets the seed for generating random numbers return?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self.",
        "out_text": "a torch.Generator object"
    },
    {
        "in_text": "What will the generated code of this GraphModule be if it is not recompiled?Q: Does ONNX support implicit scalar datatype casting?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dpickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "out of date"
    },
    {
        "in_text": "What is enzyme that is responsible for the RNNBase?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "nn.RNNBase"
    },
    {
        "in_text": "nn.ELU Applies what?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Tensor.unsqueeze Seetorch.unsqueeze()",
        "out_text": "element-wise function"
    },
    {
        "in_text": "In order to prevent this from being an issue, a constraint is placed to provide an empty dictionary as the last input in what?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulWithout the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "out_text": "tuple args"
    },
    {
        "in_text": "What does Package Exporter offer that allows one to save arbitrary Python source code to a module of your choosing?median Returns the median of the values in input.Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "asave_source_string()method"
    },
    {
        "in_text": "What is qconfig for quantization aware training?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "torch.quantization.get_default_qat_qconfig('qnnpack')"
    },
    {
        "in_text": "What does a double wildcard match against?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "matchestorchand all its submodules"
    },
    {
        "in_text": "What is returned after each element of the input tensor is multiplied by the scalar alpha?Q: Does ONNX support implicit scalar datatype casting?pow Takes the power of each element in inputwithexponentand returns a tensor with the result.For example, ifinputis of shape",
        "out_text": "The resulting tensor"
    },
    {
        "in_text": "What must happen if bothinputandotherare non-scalars?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "out_text": "the size of their last dimension must match"
    },
    {
        "in_text": "What provides a standard way to read and write ZIP archive contents?mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. WarningIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "out_text": "Python zip file module"
    },
    {
        "in_text": "What is done to help correlating each backward-pass op with the corresponding forward-pass op?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "To ease this"
    },
    {
        "in_text": "What will always be real-valued, even whenAis complex?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "logabsdet"
    },
    {
        "in_text": "What is the name of the program that walks back through your code to show you where this situation happens?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "The traceback"
    },
    {
        "in_text": "What does register_module_backward_hook register?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "backward hook"
    },
    {
        "in_text": "To motivate this, let\u2019s use what?NoteThe.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "out_text": "an example"
    },
    {
        "in_text": "What are the dimensions of the tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "0 and 1"
    },
    {
        "in_text": "Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) isdstack Stack tensors in sequence depthwise (along third axis).Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:Note Wheninputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically returnindicesfor any of them. input(Tensor) \u2013 the input tensor. k(int) \u2013 k for the k-th smallest element dim(int,optional) \u2013 the dimension to find the kth value along keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor)",
        "out_text": "indexing"
    },
    {
        "in_text": "IfupperisTrue, the returned matrixUis what?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsAlias fortorch.linalg.pinv()",
        "out_text": "upper-triangular"
    },
    {
        "in_text": "What depend on pickle and you may need to add an import tosetup for them to transfer properly?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": "nn.Modules"
    },
    {
        "in_text": "Returns a new tensor that indexes the input tensor according to what boolean maskmask?pow Takes the power of each element in inputwithexponentand returns a tensor with the result.For example, ifinputis of shapeMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "out_text": "1-D"
    },
    {
        "in_text": "What does aint that controls the cache capacity of a cuFFT plan do?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "Clears the cuFFT plan cache"
    },
    {
        "in_text": "How can a tensor be constructed from a Pythonlistor sequence?Q: Does ONNX support implicit scalar datatype casting?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources of",
        "out_text": "thetorch.tensor()"
    },
    {
        "in_text": "What happens to the given submodule from self?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Deletes"
    },
    {
        "in_text": "What divides each element of the input input by the corresponding element of the other?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "div"
    },
    {
        "in_text": "TrainingMode.TRAINING: export the model in what mode?hardtanh_ In-place version ofhardtanh().hardtanh_ In-place version ofhardtanh().Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function. function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "out_text": "training friendly mode"
    },
    {
        "in_text": "What language does Tensor.item come from?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dThis would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "out_text": "Python"
    },
    {
        "in_text": "What are NNN tensors?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.topk Seetorch.topk()",
        "out_text": "Take NNN tensors"
    },
    {
        "in_text": "What is the result of Computes the q-th quantiles of each row of theinputtensor along the dimensiondimoptimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizersolve This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a named tuplesolution, LU.>>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])",
        "out_text": "variant oftorch.quantile()that \u201cignores\u201dNaNvalues"
    },
    {
        "in_text": "What models execute a list of modules/functions in order (sequentially)?input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.register_module_forward_hook Registers a global forward hook for all the modulesregister_module_forward_hook Registers a global forward hook for all the modules",
        "out_text": "Sequential models"
    },
    {
        "in_text": "What is dim(int,list of python:int,optional)?Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:Tensor.atan2_ In-place version ofatan2()A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported.",
        "out_text": "the dimension or dimensions to approximate the gradient over"
    },
    {
        "in_text": "nn.ConvTranspose1d Applies a what transposed convolution operator over an input image composed of several input planesnn.Conv1d Applies a 1D convolution over an input signal composed of several input planes.Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalescelp_pool1d Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "out_text": "1D"
    },
    {
        "in_text": "What does [i-1]input[m][n]...[l][x]=boundaries[i]?nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2dFalse boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i]Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "out_text": "False boundaries"
    },
    {
        "in_text": "What does not prune any units but generates the pruning parametrization with a mask of ones?Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected. Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "out_text": "prune.Identity Utility pruning method"
    },
    {
        "in_text": "The returnedsolutionintorch.lstsq()stores the residuals of the solution in the lastm - what in theCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "ncolumns"
    },
    {
        "in_text": "What is gt function?bitwise_or Computes the bitwise OR of input and other.Alias fortorch.abs()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor.gt_ In-place version ofgt()"
    },
    {
        "in_text": "Pytorch Hub supports publishing pre-trained models to a github repository. It can't be a what?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "random commit"
    },
    {
        "in_text": "What is another name for union[List[str],str])?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "include"
    },
    {
        "in_text": "LOBPCG method \u2013 select LOBPCG method. See what?Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "out_text": "description of the function above"
    },
    {
        "in_text": "ivars[\u201cistep\u201d]- what?Alias fortorch.linalg.pinv()\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "current iteration stepX"
    },
    {
        "in_text": "What could be cheaper to handle batching manually in dataset code?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "directly load batched data"
    },
    {
        "in_text": "What  permits uncoalesced sparse tensors?>>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "PyTorch sparse COO tensor format"
    },
    {
        "in_text": "What type of pooling does AdaptiveAvgPool2d apply?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.Random sampling creation ops are listed underRandom samplingand",
        "out_text": "2D"
    },
    {
        "in_text": "To confirm whether the operator is standardized or not, please check what?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Warning",
        "out_text": "ONNX operator list"
    },
    {
        "in_text": "What does Tensor.sign_ In-place version ofsign() do?This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.Tensor.tanh_ In-place version oftanh()Tensor.asin_ In-place version ofasin()",
        "out_text": "Tensor.sign_ In-place version ofsign()"
    },
    {
        "in_text": "What does this function do for calling LAPACK's geqrf directly?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "low-level function"
    },
    {
        "in_text": "What does Tensor.flip?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Seetorch"
    },
    {
        "in_text": "A tensor of specific data type can be constructed by passing a what?This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "out_text": "torch.dtype"
    },
    {
        "in_text": "How to use torch.unsqueeze, give an example?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorswhereNNNis the full window size.",
        "out_text": ">>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])"
    },
    {
        "in_text": "What does marginRankingLoss stand for?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars.  However for cases that it failed to do so, you will need to manually provide the datatype information.  This often happens with scripted models, where the datatypes are not recorded.  We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "margin_ranking_loss"
    },
    {
        "in_text": "When is a combinedFuture completed?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "when all of the sub-futures are completed"
    },
    {
        "in_text": "What might be artificially increased because of the shape collection?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "the total self cpu time"
    },
    {
        "in_text": "Aintthat controls what of the cuFFT plan?torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "out_text": "cache capacity"
    },
    {
        "in_text": "What does torch.linalg.multi_dot() replace?Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()",
        "out_text": "multiple arguments"
    },
    {
        "in_text": "The computation of the rank is done by obtaining what?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).",
        "out_text": "eigenvalues"
    },
    {
        "in_text": "On what system are two types of libtorch binaries provided?To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "out_text": "Linux"
    },
    {
        "in_text": "Functions and methods called what are compiled as they are seen by the compiler?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLossCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "fromforwardare"
    },
    {
        "in_text": "How to use torch.gradient, give an example?Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column. >>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "out_text": ">>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\n>>> torch.gradient(t)\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\n>>> torch.gradient(t, spacing=(coords,))\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])"
    },
    {
        "in_text": "If normalized is True, what is the default?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "If normalized is True"
    },
    {
        "in_text": "What is the name of the group that disabled gradient calculation?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Context-manager"
    },
    {
        "in_text": "In case the model should accept inputs of dynamic shape, what parameter can you utilize in export api?If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dictReturns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "out_text": "dynamic_axes"
    },
    {
        "in_text": "What type of model is most of the model?Notetorch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "feed-forward network"
    },
    {
        "in_text": "Which calls will not affect.grad layouts?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "model.zero_grad() or optimizer.zero_grad()"
    },
    {
        "in_text": "A Module is considered \u201cused\u201d if any of the following is true: 1. It has what that are used?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "out_text": "children"
    },
    {
        "in_text": "If you want to use MKL-enabled matrix operations, what should you use?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.poisson_nll_loss Poisson negative log likelihood loss.",
        "out_text": "torch.int32"
    },
    {
        "in_text": "To find out if atorch.dtypeis a complex data type, what property can be used?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.IfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "propertyis_complex"
    },
    {
        "in_text": "real Returns a new tensor containing real values of what?torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided]real Returns a new tensor containing real values of the self tensor.Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "out_text": "the self tensor"
    },
    {
        "in_text": "What calls do we want to replace torch.add() calls with?Alias for torch.linalg.matrix_power()You can usetorch.manual_seed()to seed the RNG for all devices (bothtorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "torch.mul()"
    },
    {
        "in_text": "What is the method__reduce_package__(?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.You can usetorch.manual_seed()to seed the RNG for all devices (bothTests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "out_text": "self,exporter:Package Exporter"
    },
    {
        "in_text": "What function is deprecated and will be removed in a future PyTorch release?Q: Does ONNX support implicit scalar datatype casting?This function is deprecated and will be removed in a future release because its behavior is inconsistent withWarning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "out_text": "Alias oftorch.outer()"
    },
    {
        "in_text": "What do we try to do with the missing symbolic function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "export the model"
    },
    {
        "in_text": "What type of pattern can include(Union[List[str],str]) be?Tensor.logical_xor_ In-place version oflogical_xor()callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Notef = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "glob-style pattern"
    },
    {
        "in_text": "What is torch.float64 or torch.double torch?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Q: Does ONNX support implicit scalar datatype casting?Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order",
        "out_text": "64-bit floating point"
    },
    {
        "in_text": "How to use First convert your model from GPU to CPU and then save it, like so:, give an example?Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)"
    },
    {
        "in_text": "Checks if something is a Tensor-like, including an exactTensor-like, including an exactTensorAttribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "out_text": "bool"
    },
    {
        "in_text": "Returns the what of (input - other)?If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "out_text": "p-norm"
    },
    {
        "in_text": "In what state is the PyTorch API of sparse tensors?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "beta"
    },
    {
        "in_text": "What does Tensor.ndim Alias fordim() Tensor.real Return a new tensor containing?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalescedChecks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "out_text": "real values of theselftensor"
    },
    {
        "in_text": "sspaddmm matrix  multiplies what tensormat1 with a dense tensormat2?Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses aWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "out_text": "sparse"
    },
    {
        "in_text": "What is Tensor.arccosh?Alias fortorch.linalg.pinv()whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "acosh"
    },
    {
        "in_text": "Worker_init_fncannot be an unpicklable object, e.g., what?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "a lambda function"
    },
    {
        "in_text": "What is the name of the directory in which to save the object map_location(optional)?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "url(string) \u2013 URL of the object to download model_dir(string,optional)"
    },
    {
        "in_text": "What does torch.outer compute the dot product for?xlogy Computesinput*log(other)with the following cases.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factor",
        "out_text": "1D tensors"
    },
    {
        "in_text": "What must be the original positions of the dims to move?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "unique"
    },
    {
        "in_text": "Most operations will work identically given what two types of sparse tensors?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])This function checks if allinputandothersatisfy the condition",
        "out_text": "coalesced or uncoalesced sparse tensor"
    },
    {
        "in_text": "Returns what stream for a given device?enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenienceenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "out_text": "currently selectedStreamfor a given device"
    },
    {
        "in_text": "What is the input tensorAAAof size(,n,n)(*, n, n)(,Number \u2013 sum(abs(x)**ord)**(1./ord)After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "input(Tensor)"
    },
    {
        "in_text": "What would happen if the export function assumed that the x input was intended to represent the optional dictionary consisting of named arguments?Q: Does ONNX support implicit scalar datatype casting?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteTensor.arctanh Seetorch.arctanh()",
        "out_text": "would work as intended"
    },
    {
        "in_text": "What is the scale of a quantized tensor?TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.Tensor.logical_xor Seetorch.logical_xor()ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "out_text": "zero point"
    },
    {
        "in_text": "What is Tensor.not_equal()?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Tensor.logical_not_ In-place version oflogical_not()f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "out_text": "Seetorch"
    },
    {
        "in_text": "What is the left-hand side expression used to indicate to the TorchScript compiler?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "a class instance attribute with type oftype"
    },
    {
        "in_text": "What type of support does PyTorch have?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "OpenMP"
    },
    {
        "in_text": "What is the reverse operation of the manner described ingather()?Alias fortorch.linalg.pinv()\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "Writes all values from the tensorsrcintoselfat the indices specified in theindextensor"
    },
    {
        "in_text": "What is currently unpruned channels along the specifieddimselected at random?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "specifiedamountof"
    },
    {
        "in_text": "What is Seetorch.remainder?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Tensor.remainder"
    },
    {
        "in_text": "What type of data elements does yourcollate_fnreturns a batch that is a custom type?Alias for torch.linalg.matrix_power()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "a custom type"
    },
    {
        "in_text": "What is the summation subscript?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "j"
    },
    {
        "in_text": "How do you save an offline version of this module for use in a separate process?You can usetorch.manual_seed()to seed the RNG for all devices (bothrepo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: TrueDoing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.",
        "out_text": "save"
    },
    {
        "in_text": "What does extern declare this module as?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().Q: Does ONNX support implicit scalar datatype casting?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "external dependency"
    },
    {
        "in_text": "What does this library get?Notetorch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "the cuda capability of a device"
    },
    {
        "in_text": "What is Seetorch.clamp() function?M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() noTensor.log1p Seetorch.log1p()Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "out_text": "Tensor.clamp"
    },
    {
        "in_text": "What method is used to print out a table showing the nodes of the module MyModule?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the code",
        "out_text": "Graph.print_tabular()"
    },
    {
        "in_text": "What provides several methods to adjust the learning rate based on the number of epochs?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3duse_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "out_text": "torch.optim.lr_scheduler"
    },
    {
        "in_text": "What is the exponentially scaled zeroth order modified Bessel function?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "the first kind"
    },
    {
        "in_text": "What do corresponding tensors not have?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.multiply_ In-place version ofmultiply().",
        "out_text": "same shape"
    },
    {
        "in_text": "What does Python not offer clean boundaries between objects defined in a module?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "Python does not offer clean boundaries between objects defined in a module"
    },
    {
        "in_text": "Where are storages first deserialized?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "the CPU"
    },
    {
        "in_text": "What is the CSR sparse tensor?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "1-D tensor of sizesize[0]+1"
    },
    {
        "in_text": "What is the name of the author of Understanding the difficulty of training deep feedforward neural networks?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "X"
    },
    {
        "in_text": "What can you do without having to package them?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "depend on third-party libraries likenumpyandscipy"
    },
    {
        "in_text": "The arctangent of inputi/otheri is considered with consideration of what?Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:This function checks if allinputandothersatisfy the conditionWith respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "out_text": "quadrant"
    },
    {
        "in_text": "FX generates the forward() function on what?isclose Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.xlogy Computes input*log(other) .This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.",
        "out_text": "GraphModules"
    },
    {
        "in_text": "What dimension do channels occupy?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Note",
        "out_text": "second dimension"
    },
    {
        "in_text": "What does nn.AvgPool2d apply over an input signal composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()",
        "out_text": "2D average pooling"
    },
    {
        "in_text": "How many ways to inform the compiler of attributes onScriptModule?timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedReturns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "out_text": "4"
    },
    {
        "in_text": "What does values represent in the given dimensiondim?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "thekth smallest element of each row of theinputtensor"
    },
    {
        "in_text": "What algorithm implements the lazy version of Adam algorithm?softshrink Applies the soft shrinkage function element wiseThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "AdamW"
    },
    {
        "in_text": "What does a TUPLE OF ARGUEMENTS contain?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "A DICTIONARY OF NAMED PARAMETERS"
    },
    {
        "in_text": "What offers asave_source_string() method?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "Package Exporter"
    },
    {
        "in_text": "What does MultiMarginLoss optimize?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.",
        "out_text": "multi-class classification hinge loss"
    },
    {
        "in_text": "What type of indices does theselftensor return when selfis a sparse CSR tensor of layoutsparstatic quantization (weights quantized, activations quantized, calibration required post training) quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post for a more comprehensive overview of the tradeoffs between these quantization types. This is the simplest to apply form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference. This is used for situations where the model execution time is dominated by loading weights from memory rather than computing the matrix multiplications. This is true for for LSTM and Transformer type models with small batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.Tensor.absolute Alias forabs()output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "out_text": "column"
    },
    {
        "in_text": "What is the name of the module to be partially specialized enable_cpatching?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.linalg.pinv()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "concrete_args"
    },
    {
        "in_text": "What is Tensor.log10?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "Seetorch.log10"
    },
    {
        "in_text": "Is a Future able to be completed twice?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a Future cannot be marked completed twice"
    },
    {
        "in_text": "What did you learn after using character-level RNN to classify names?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "leanr how to generate names from languages"
    },
    {
        "in_text": "How to use torch.autograd.profiler.profile, give an example?Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).inverse Alias for torch.linalg.inv()",
        "out_text": ">>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------"
    },
    {
        "in_text": "What is the equivalent totorch.tensor(x)?Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here.Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example. \u2026 return x m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the x input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order",
        "out_text": "tox.clone().detach()"
    },
    {
        "in_text": "How to use torch.pca_lowrank, give an example?Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.Tensor.frac_ In-place version offrac()torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.",
        "out_text": "- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_)."
    },
    {
        "in_text": "How to use torch.eig, give an example?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "L_complex = torch.linalg.eigvals(A)"
    },
    {
        "in_text": "What type of test will slow down your program execution?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Example"
    },
    {
        "in_text": "Prune (currently unpruned) units in a tensor at what?Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()Tensor.topk Seetorch.topk()",
        "out_text": "random"
    },
    {
        "in_text": "What is the In-place version of ofnan_to_num()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "Tensor.nan_to_num"
    },
    {
        "in_text": "What is the default value for window of all111s?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "out_text": "Default:None"
    },
    {
        "in_text": "What is the vector norm 'fro'?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "matrix norm"
    },
    {
        "in_text": "What type of training is supported in Eager Mode Quantization?IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)While disabling CUDA convolution benchmarking (discussed above) ensures thatApplies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "out_text": "quantization aware training"
    },
    {
        "in_text": "What is this useful when one wants to specify?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "per-layer learning rates"
    },
    {
        "in_text": "How to use torch.argsort, give an example?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Alias fortorch.abs()bitwise_or Computes the bitwise OR of input and other.",
        "out_text": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])"
    },
    {
        "in_text": "Default: if None, uses a what default?Alias for torch.linalg.matrix_power()Tensor.arctanh Seetorch.arctanh()torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "out_text": "global"
    },
    {
        "in_text": "IfupperisTrue, andAAAis a batch of symmetric positive-definite matrices?Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note>>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "out_text": "Warning"
    },
    {
        "in_text": "What is another name for linear_1 linear?torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "add_1"
    },
    {
        "in_text": "What is the dot product between a given function and a vector v?Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])IfkeepdimisTrue, the output tensor is of the same sizeComputes the element-wise greatest common divisor (GCD) ofinputandother. Bothinputandothermust have integer types. Note This definesgcd(0,0)=0gcd(0, 0) = 0gcd(0,0)=0. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "out_text": "the Jacobian"
    },
    {
        "in_text": "When does Seetorch.Tensor.view() occur?Tensor.frexp Seetorch.frexp()Tensor.swapaxes Seetorch.swapaxes()Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "out_text": "when it is possible to return a view"
    },
    {
        "in_text": "What operator specifies which values in scope should be passed as inputs?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Examplepickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "aten"
    },
    {
        "in_text": "What enables profiling with Kineto profiler?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "use_kineto"
    },
    {
        "in_text": "torch.max Returns the maximum value of all elements in what?Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf.Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "out_text": "the input tensor"
    },
    {
        "in_text": "Input must be either a 1-D time sequence or a 2-D batch of time sequences?This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeinterpolate Down/up samples the input to either the givensizeor the givenscale_factorWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "in_text": "If check_device(bool) is disabled, what happens to tensors on differentdevice\u2019s?If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. WarningIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warningmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "out_text": "tensors on differentdevice\u2019s are moved to the CPU before being compared"
    },
    {
        "in_text": "What is the last known global optimizer state?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "globalstate_dict"
    },
    {
        "in_text": "What is the name of the function that calculates pointwise logarithm?Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3dIf the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]",
        "out_text": "torch.logaddexp()"
    },
    {
        "in_text": "What is the name of a file that becomestorch_package_0>.torchvision/modules/retorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.Tensor.frac_ In-place version offrac()Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "out_text": "liketorchvision/models/resnet18.py"
    },
    {
        "in_text": "What does nn.ReflectionPad1d Pads the input tensor using?Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoullitorch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.xlogy Computesinput*log(other)with the following cases.",
        "out_text": "reflection of the input boundary"
    },
    {
        "in_text": "How to use A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:, give an example?No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "out_text": ">>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])"
    },
    {
        "in_text": "Most likely the skew will be what for bottom most events?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.Note",
        "out_text": "negligible"
    },
    {
        "in_text": "How to use Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below., give an example?Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "out_text": "RuntimeError:\nTensor (inferred) cannot be used as a tuple:\n  File <filename>\n        def forward(self, x, tup):\n            t0, t1 = tup\n                     ~~~ <--- HERE\n            return t0 + t1 + x"
    },
    {
        "in_text": "What does GroupNorm Applies over a mini-batch of inputs as described in the paperGroup Normalization?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "Group Normalization"
    },
    {
        "in_text": "Computes the entropy on input in what way?Computes the complementary error function ofinput.Computes the complementary error function ofinput.Returns a tensor with the same data and number of elements asinput,",
        "out_text": "elementwise"
    },
    {
        "in_text": "What is reciprocal Seetorch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "Tensor"
    },
    {
        "in_text": "If the size of dataset is not divisible by the batch size, what happens?pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Notecelu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulQ: Does ONNX support implicit scalar datatype casting?",
        "out_text": "the last batch will be smaller"
    },
    {
        "in_text": "If the object is already present, it's deserialized and returned?Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).IfkeepdimisTrue, both thevaluesandindicestensorsThis is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "out_text": "inmodel_dir"
    },
    {
        "in_text": "Where is the entry [5, 6] in a sparse COO tensor?Tensor.t_ In-place version oft()trapz Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule.Q: Does ONNX support implicit scalar datatype casting?",
        "out_text": "(1, 0"
    },
    {
        "in_text": "What should be replaced with L,V=torch.symeig?If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)Alias for torch.linalg.matrix_power()Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "L,_=torch.symeig"
    },
    {
        "in_text": "Where does the SummaryWriter write events and summaries?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "event file"
    },
    {
        "in_text": "How is the addition of sparse COO tensors implemented?xlogy Computesinput*log(other)with the following cases.ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dictTensor.logical_xor Seetorch.logical_xor()",
        "out_text": "concatenating the indices and values tensors"
    },
    {
        "in_text": "What is returned if the output tensors have an even number of elements in the dimension dim?IfkeepdimisTrue, the output tensor is of the same sizeSamples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])This function checks if allinputandothersatisfy the condition",
        "out_text": "lower of the two medians"
    },
    {
        "in_text": "How does reordering the multiplications work?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Tensor.ravel seetorch.ravel()",
        "out_text": "multiplies two or more matrices"
    },
    {
        "in_text": "How to use If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:, give an example?Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.",
        "out_text": ">>> s.is_coalesced()\nFalse"
    },
    {
        "in_text": "What is a suggested workflow to see all available methods?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "dir(model)"
    },
    {
        "in_text": "Creation ops are listed under Random sampling and include: torch.rand() torch.rand() torch.rand_like() torch.r8-bit integer (unsigned) torch.uint8 torch.*.ByteTensorThis feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation. The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph\u2019s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX! Several example transformations can be found at the examples repository.8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor",
        "out_text": "Random sampling"
    },
    {
        "in_text": "What type of tensor is 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensorReturns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor. Default: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)). pivots stores all the intermediate transpositions of rows. The final permutation perm could be reconstructed by applying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1, where perm is initially the identity permutation of mmm elements (essentially this is what torch.lu_unpack() is doing).",
        "out_text": "CPU tensor GPU tensor"
    },
    {
        "in_text": "What does Alias for torch.atanh() do?n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013ger Alias of torch.outer().ger Alias of torch.outer().",
        "out_text": "Computes the bitwise AND of inputandother"
    },
    {
        "in_text": "What computes the dot product between the Jacobian of a given function at the point given by the inputs and a vector vComputes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "out_text": "functional.jvp Function"
    },
    {
        "in_text": "What is Model() k = torch.randn(2, 3) x = torch.tensor(1)TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))Samples are binary (0 or 1). They take the value 1 with probability p\nand 0 with probability 1 - p. >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])",
        "out_text": "m"
    },
    {
        "in_text": "What Computes the 2-dimensional discrete Fourier transform of realinput?Q: Does ONNX support implicit scalar datatype casting?Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "rfft2"
    },
    {
        "in_text": "What do I want to trace?Notetorch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight OnlySplits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "out_text": "module\u2019s method"
    },
    {
        "in_text": "What type of modules are Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully SupportElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b) and vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first parameter, is the y-coordinate.)Support for Customization Limited Support Fully SupportedReturns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "out_text": "Manual Automatic Quantizing Modules"
    },
    {
        "in_text": "Where does the object return a copy of?callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  NoteTensor.arctanh Seetorch.arctanh()ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "out_text": "CUDA memory"
    },
    {
        "in_text": "When does Tensor.coalesce return a coalesced copy of self?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "out_text": "if self is an uncoalesced tensor"
    },
    {
        "in_text": "What does logdet calculate?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "log determinant"
    },
    {
        "in_text": "What is a sequence of pruning methods for?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "iterative pruning"
    },
    {
        "in_text": "What do you want the extension to support?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "CCs"
    },
    {
        "in_text": "What does Atorch.ByteTensor contain.Tensor.arctanh Seetorch.arctanh()Boolean torch.bool torch.*.BoolTensorPyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "out_text": "Tensor all the necessary bits to restore a Generator to a specific point in time"
    },
    {
        "in_text": "There is no way to package \u201cjust\u201d a function or class from what?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "out_text": "module"
    },
    {
        "in_text": "What is Blackman window function?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "evaluates blackman_window"
    },
    {
        "in_text": "What removes all specified elements from a sparse tensor?Q: Does ONNX support implicit scalar datatype casting?Fillsselftensor with elements drawn from the geometric distribution:A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).",
        "out_text": "Tensor.sparse_resize_and_clear"
    },
    {
        "in_text": "Currently, one can acquire the COO format data only when the tensor instance is what?f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. NoteSee the Automatic Mixed Precision examples for usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "out_text": "coalesced"
    },
    {
        "in_text": "If your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the outputFor custom operators, you might need to set python seed as wellFor custom operators, you might need to set python seed as welltimer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "out_text": "CUDA-mode autograd profiler"
    },
    {
        "in_text": "What is returned when a sparse COO tensorthat is coalesced?xlogy Computesinput*log(other)with the following cases.This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "Trueifselfis"
    },
    {
        "in_text": "Sequentialautomatically feeds the output of the firstMyLinearmodule as input into the ReLU, and the output of that as inputcd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes../flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "out_text": "in-order chaining of modules"
    },
    {
        "in_text": "What type of unit is glu?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "gated linear unit"
    },
    {
        "in_text": "What does triplet_margin_with_distance_loss refer to?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "out_text": "triplet_margin_with_distance_loss"
    },
    {
        "in_text": "What default uses the global default dtype when both start and end are real?Q: Does ONNX support implicit scalar datatype casting?celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mulpickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "out_text": "if None"
    },
    {
        "in_text": "Atanh Returns a new tensor with what of the elements of input?If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.isclose Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.neg Returns a new tensor with the negative of the elements of input.",
        "out_text": "inverse hyperbolic tangent"
    },
    {
        "in_text": "What does eq compute?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "element-wise equality"
    },
    {
        "in_text": "What is TorchScript?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a way to create serializable and optimizable models from PyTorch code"
    },
    {
        "in_text": "What does arccosh_ acosh_() -> Tensor?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "Tensor"
    },
    {
        "in_text": "What does tensor.new_empty return a Tensor of size size filled with?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLosstorch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "out_text": "uninitialized data"
    },
    {
        "in_text": "Where can you find information on how to use quantized functions in PyTorch?Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep. Step isDiagram: API Example: To learn more about static quantization, please see the static quantization tutorial.Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "out_text": "theQuantizationdocumentation"
    },
    {
        "in_text": "What is the returned window if window_length is one?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "a single element tensor containing a one"
    },
    {
        "in_text": "What type of feature is tagged as because the API may change based on user feedback?Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.While disabling CUDA convolution benchmarking (discussed above) ensures thatIfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "out_text": "Beta"
    },
    {
        "in_text": "What has limited support for the__import__(...)syntax?pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "out_text": "AST parsing"
    },
    {
        "in_text": "What is also included when printing a?Alias fortorch.linalg.pinv()If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)Alias fortorch.special.expit().",
        "out_text": "Measurement"
    },
    {
        "in_text": "Inputwill be padded on both sides so that thettt-th frame is centered at timethop_lengthtIflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elementsYou can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "out_text": "IfcenterisTrue"
    },
    {
        "in_text": "For what type of tensor does it compute the logical OR?Q: Does ONNX support implicit scalar datatype casting?Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "out_text": "bool tensors"
    },
    {
        "in_text": "How will other PyTorch operations behave?Q: Does ONNX support implicit scalar datatype casting?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "deterministically"
    },
    {
        "in_text": "When attempting to differentiate a CUDA tensor torch, what is FractionalMaxPool3d?The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.However, there are some steps you can take to limit the number of sources ofTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "out_text": "CUDA tensor torch"
    },
    {
        "in_text": "What is a tuple of arguments or torch?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.",
        "out_text": "a dictionary consisting of named arguments"
    },
    {
        "in_text": "What is returned if self is an uncoalesced tensor?IfkeepdimisTrue, both thevaluesandindicestensorsReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "self"
    },
    {
        "in_text": "Where can you find a more comprehensive overview of the tradeoffs between quantization types?Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.To learn more about static quantization, please see the static quantization tutorial.",
        "out_text": "Pytorch"
    },
    {
        "in_text": "Ifdimis a list of dimensions, what does it do?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "out_text": "reduce over all of them"
    },
    {
        "in_text": "Target \u2013 What is the name of the new submodule?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013Note",
        "out_text": "fully-qualified string name of the new submodule"
    },
    {
        "in_text": "What is Alias forclamp?Alias fortorch.le().In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):bitwise_or Computes the bitwise OR of input and other.",
        "out_text": "Tensor.clip_ Alias forclamp_()"
    },
    {
        "in_text": "What is the default value for the global default dtype?Q: Does ONNX support implicit scalar datatype casting?Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "out_text": "if None"
    },
    {
        "in_text": "How to use torch.quantize_per_channel, give an example?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.bitwise_or Computes the bitwise OR of input and other.",
        "out_text": ">>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)"
    },
    {
        "in_text": "What does a small block size result in?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "better statistics"
    },
    {
        "in_text": "What value means that the cycles will continue until the profiling is finished?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "out_text": "the zero value"
    },
    {
        "in_text": "What class's parameters will use a learning rate of 1e-3?\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.whereNNNis the full window size.",
        "out_text": "model.classifier"
    },
    {
        "in_text": "What type of tensor types does autograd only support?Q: Does ONNX support implicit scalar datatype casting?A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: ExampleTensor.ndimension Alias fordim()",
        "out_text": "floating point Tensor types"
    },
    {
        "in_text": "What is the return value of a method specified by a special output node?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "return value"
    },
    {
        "in_text": "What does nonzero alpha not support?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wisewhereNNNis the full window size.",
        "out_text": "add"
    },
    {
        "in_text": "What Applies Instance Normalization over a 5D input?The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.softshrink Applies the soft shrinkage function element wisexlogy Computesinput*log(other)with the following cases.",
        "out_text": "nn.LayerNorm"
    },
    {
        "in_text": "What is Tensor.clamp?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Alias fortorch.linalg.pinv()",
        "out_text": "Seetorch.clamp"
    },
    {
        "in_text": "approximately how much overhead does each tensor operation add?Q: Does ONNX support implicit scalar datatype casting?Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].xlogy Computesinput*log(other)with the following cases.",
        "out_text": "4us"
    },
    {
        "in_text": "How to use torch.distributions.continuous_bernoulli.ContinuousBernoulli, give an example?xlogy Computesinput*log(other)with the following cases.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... WarningTensor.ndimension Alias fordim()",
        "out_text": ">>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])"
    },
    {
        "in_text": "What product of input and vec2?Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\u2019fro\u2019 Frobenius norm \u2013whereNNNis the full window size.",
        "out_text": "Outer product"
    },
    {
        "in_text": "What is In-place version ofunsqueeze()?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "out_text": "Tensor.unsqueeze_ In-place version ofunsqueeze()"
    },
    {
        "in_text": "What does torch.package package?whereNNNis the full window size.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.Alias fortorch.linalg.pinv()",
        "out_text": "Torch Script module"
    },
    {
        "in_text": "What is the best way to manage dependencies?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Note",
        "out_text": "not have dependencies at all"
    },
    {
        "in_text": "IfModelis is instantiated, what will happen?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "out_text": "a compilation error"
    },
    {
        "in_text": "What is nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule?PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_nameBoolean torch.bool torch.*.BoolTensorIfkeepdimisTrue, both thevaluesandindicestensors",
        "out_text": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule"
    },
    {
        "in_text": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing thevaluestensor toCreates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:gradient This function is analogous to NumPy\u2019s gradient function.2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "out_text": "multi-dimensional tensor"
    },
    {
        "in_text": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the iteration process will be what?To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_onesldexp Multiplies input by 2**:attr:other.The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "out_text": "hard-stopped"
    },
    {
        "in_text": "What does nn.LPPool1d Apply over an input signal composed of several input planes?Q: Does ONNX support implicit scalar datatype casting?xlogy Computesinput*log(other)with the following cases.The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "out_text": "1D power-average pooling"
    },
    {
        "in_text": "What domain library will FX Graph Mode Quantization be integrated into?To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.whereNNNis the full window size.pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=.... Warning",
        "out_text": "torchvision"
    },
    {
        "in_text": "Thecrow_indicestensor consists of what?torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training.This installs empty Modules where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while writing out the codeWarning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "out_text": "compressed row indices"
    },
    {
        "in_text": "Where are external binary files stored?Note\u2019fro\u2019 Frobenius norm \u2013Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "out_text": "same location as the ONNX file"
    },
    {
        "in_text": "What is the name of TheKullback-Leibler divergence Loss Function?Alias fortorch.linalg.pinv()torch.all matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "out_text": "SeeMarginRankingLossfor details"
    },
    {
        "in_text": "What does SmoothL1Loss use if the absolute element-wise error falls below beta?Q: Does ONNX support implicit scalar datatype casting?This function checks if allinputandothersatisfy the conditionComputes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "out_text": "a squared term"
    },
    {
        "in_text": "AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of what?timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reducedReturns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "out_text": "several input planes"
    },
    {
        "in_text": "What does torch.outer() compute the dot product for?xlogy Computesinput*log(other)with the following cases.Q: Does ONNX support implicit scalar datatype casting?Alias for torch.linalg.matrix_power()",
        "out_text": "1D tensors"
    },
    {
        "in_text": "In LSTM, if user passes what, the function should use the first input as activation and the second input as hidden?graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect. def forward(self, x):\n    a = x + 1\n    return x + self.attr_1",
        "out_text": "activation"
    }
]